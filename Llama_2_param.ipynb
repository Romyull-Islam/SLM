{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrQkn7VsMOpd"
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HNXlcv30YeJ",
    "outputId": "ac9251bf-c92c-41dc-e557-ff63f6891c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "Successfully installed tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from torch==2.1.2->torchvision) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No matching packages for pattern \"\\*\"\n"
     ]
    }
   ],
   "source": [
    "pip cache remove \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.13.3 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (0.13.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GZdjjGKNMg_1"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HFZHPAaI1WOX"
   },
   "outputs": [],
   "source": [
    "token = \"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\" # paste your token from hf here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeyojIjj01Wm"
   },
   "source": [
    "# Download and load the Llama-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "9f554f746df440afae509b9ba7e24c33",
      "483f53c628924172bedbd6ac66aeaa22",
      "db4849525171480ab0f8bcc35a4c545c",
      "eb9114d8abf84ad883f19ec1715563cd",
      "09966bb4a04e45cbad464238e668772e",
      "7e6d370082fb406d888bd109f07890d9",
      "04d282bd42964dd2a23eaa0959415f23",
      "d8080c35ca5f4490b1624d27698f5dfa",
      "8c6844fbf91648778a1605df5ce9cdd2",
      "5cbdb7ab718e490c984080785f60a9a8",
      "98fc99b7fbf14f98a5d8a32c4af0c981"
     ]
    },
    "id": "_1VRfALo0qqZ",
    "outputId": "a4742af2-69d3-44f0-c369-c08dd04cee02",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370fe35b64cc4c1caf5ed51a70fb29d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdeng2\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bdeng2\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ca670fe6bf4cb19c18f6fd56bc23e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46b8c16ff064db18c5da53d862ec7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982ec56826ff48d8b5e581f195785f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a9d0c6dab145529f655a4598f6e018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707eae89f830435c88b17300021a91e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca3b357814d45dea265dd07d47ad836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c23ecdb7234f1da117984bfe25db0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e80ddf6f24d879a373ac1d0af77e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34545b6e1b484826964e0f14bbe578fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ee7f6b8c6c45b3b798dce71448b6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674f47310f964fb789e8ada05a4095c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\", use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-hf\",\n",
    "                                             use_auth_token=token,  torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in Llama-2-13B: 13015864320\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in Llama-2-13B: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13015864320"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "163_840_000 + 40 * ( 104_857_600 + 212_336_640 + 5_120 * 2) + 5_120 + 163_840_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing libraries for dataframe creation\n",
    "# and graph plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Creating our own dataframe\n",
    "data = {\"Name\": [\"embed_tokens\", \"self_attn\", \"mlp\", \"input_layernorm\", \"post_attn_layernorm\", \"norm\",\"lm_head\"],\n",
    "        \"Marks\": [163_840_000/13015864320, 40*104_857_600/13015864320, 40*212_336_640/13015864320, 40*5_120/13015864320, 40*5_120/13015864320, 5_120/13015864320, 163_840_000/13015864320]}\n",
    " \n",
    "# Now convert this dictionary type data into a pandas dataframe\n",
    "# specifying what are the column names\n",
    "df = pd.DataFrame(data, columns=['Name', 'Marks'])\n",
    "\n",
    "# Defining the plot size\n",
    "plt.figure(figsize=(6,6))\n",
    " \n",
    "# Defining the values for x-axis, y-axis\n",
    "# and from which dataframe the values are to be picked\n",
    "plots = sns.barplot(x=\"Name\", y=\"Marks\", data=df)\n",
    " \n",
    "# Iterating over the bars one-by-one\n",
    "for bar in plots.patches:\n",
    "   \n",
    "  # Using Matplotlib's annotate function and\n",
    "  # passing the coordinates where the annotation shall be done\n",
    "  # x-coordinate: bar.get_x() + bar.get_width() / 2\n",
    "  # y-coordinate: bar.get_height()\n",
    "  # free space to be left to make graph pleasing: (0, 8)\n",
    "  # ha and va stand for the horizontal and vertical alignment\n",
    "    plots.annotate(format(bar.get_height()*100, '.4f')+'%', \n",
    "                   (bar.get_x() + bar.get_width() / 2, \n",
    "                    bar.get_height()), ha='center', va='center',\n",
    "                   size=10, xytext=(0, 8),\n",
    "                   textcoords='offset points')\n",
    " \n",
    "# Setting the label for x-axis\n",
    "plt.xlabel(\"Layer Name\", size=14)\n",
    "\n",
    "plt.xticks(rotation = 50)\n",
    " \n",
    "# Setting the label for y-axis\n",
    "plt.ylabel(\"Parameter Percentage\", size=14)\n",
    " \n",
    "# Setting the title for the graph\n",
    "plt.title(\"Llama-2-13b-hf\")\n",
    " \n",
    "# Finally showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdeng2\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bdeng2\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                             use_auth_token=token,  torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j41SGRZxMGLn",
    "outputId": "4a7469ff-bccc-4fcd-8578-e807808a6913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in Llama-2-7B: 6738415616\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in Llama-2-7B: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5mTAiZtWjGz",
    "outputId": "7b296c42-bef6-40a1-ebf5-ada0bf9407fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6738415616"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "131_072_000 + 32 * ( 67_108_864 + 135_266_304 + 4_096 * 2) + 4_096 + 131_072_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
      "          2.5787e-03, -3.9368e-03],\n",
      "        [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
      "          7.7057e-04, -5.0049e-03],\n",
      "        ...,\n",
      "        [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
      "          9.5825e-03, -1.8005e-03],\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05]], dtype=torch.float16, requires_grad=True) 131072000\n",
      "Parameter containing:\n",
      "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
      "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
      "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
      "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
      "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0162,  0.0079, -0.0013,  ...,  0.0166, -0.0099, -0.0135],\n",
      "        [ 0.0192,  0.0015,  0.0036,  ..., -0.0211,  0.0152,  0.0234],\n",
      "        [-0.0236, -0.0217,  0.0017,  ...,  0.0150, -0.0165, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0128, -0.0007, -0.0008,  ...,  0.0002,  0.0031,  0.0081],\n",
      "        [-0.0056,  0.0173, -0.0032,  ..., -0.0032,  0.0115, -0.0110],\n",
      "        [ 0.0037, -0.0021,  0.0013,  ...,  0.0070, -0.0115,  0.0095]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0008, -0.0006,  0.0019,  ...,  0.0059, -0.0006,  0.0103],\n",
      "        [-0.0069, -0.0005, -0.0077,  ..., -0.0106,  0.0126,  0.0048],\n",
      "        [ 0.0018,  0.0096,  0.0010,  ...,  0.0048, -0.0139, -0.0142],\n",
      "        ...,\n",
      "        [-0.0063, -0.0057,  0.0103,  ...,  0.0031,  0.0040, -0.0022],\n",
      "        [ 0.0031,  0.0048, -0.0010,  ...,  0.0054,  0.0156,  0.0007],\n",
      "        [ 0.0001,  0.0025,  0.0056,  ..., -0.0007, -0.0007,  0.0015]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-1.6212e-05, -1.9226e-03,  4.8828e-03,  ...,  5.9204e-03,\n",
      "          3.4485e-03, -9.5215e-03],\n",
      "        [ 2.7618e-03,  1.8463e-03, -1.2970e-03,  ..., -1.0300e-03,\n",
      "          1.8082e-03,  6.2561e-03],\n",
      "        [ 2.3346e-03, -2.7275e-04,  9.2697e-04,  ..., -1.6556e-03,\n",
      "         -5.7373e-03, -6.3705e-04],\n",
      "        ...,\n",
      "        [ 4.1809e-03, -3.3264e-03,  5.8899e-03,  ...,  1.2131e-03,\n",
      "          2.6093e-03,  4.3030e-03],\n",
      "        [-3.3569e-03, -2.4872e-03, -2.5787e-03,  ...,  6.1951e-03,\n",
      "         -3.4790e-03, -5.1117e-04],\n",
      "        [ 6.1951e-03, -6.5613e-04,  2.6245e-03,  ...,  5.4932e-03,\n",
      "         -7.5989e-03, -6.6833e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 1.5747e-02,  1.7090e-02,  3.1494e-02,  ..., -1.5869e-02,\n",
      "          6.5002e-03,  1.5869e-02],\n",
      "        [-2.1667e-03, -6.0120e-03,  5.6458e-03,  ...,  1.6113e-02,\n",
      "         -8.6670e-03,  9.8877e-03],\n",
      "        [ 6.8359e-03, -2.1606e-02,  2.0508e-02,  ..., -1.3000e-02,\n",
      "          1.8921e-02,  1.9409e-02],\n",
      "        ...,\n",
      "        [ 1.4126e-05, -3.2227e-02,  5.7983e-03,  ..., -8.9111e-03,\n",
      "         -1.3489e-02,  4.0283e-02],\n",
      "        [ 2.6611e-02,  2.0142e-02, -1.7090e-02,  ..., -3.4332e-03,\n",
      "         -6.4087e-03, -1.8921e-02],\n",
      "        [-5.9891e-04, -1.1353e-02, -2.3682e-02,  ...,  1.1063e-03,\n",
      "          5.9204e-03, -2.4780e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027, -0.0145,  0.0083,  ..., -0.0175, -0.0054,  0.0014],\n",
      "        [ 0.0046, -0.0042,  0.0090,  ...,  0.0160, -0.0138,  0.0334],\n",
      "        [ 0.0020,  0.0339, -0.0044,  ..., -0.0146,  0.0220,  0.0167],\n",
      "        ...,\n",
      "        [-0.0089, -0.0114,  0.0052,  ...,  0.0231, -0.0135,  0.0295],\n",
      "        [-0.0177,  0.0374,  0.0090,  ..., -0.0069, -0.0122, -0.0219],\n",
      "        [ 0.0120, -0.0013, -0.0079,  ..., -0.0003, -0.0030, -0.0302]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0003, -0.0292,  0.0148,  ..., -0.0210, -0.0270,  0.0065],\n",
      "        [-0.0111, -0.0312,  0.0128,  ...,  0.0190,  0.0060,  0.0025],\n",
      "        [-0.0059,  0.0149, -0.0084,  ..., -0.0227,  0.0075,  0.0017],\n",
      "        ...,\n",
      "        [-0.0091, -0.0016, -0.0067,  ...,  0.0295, -0.0028,  0.0183],\n",
      "        [-0.0166,  0.0073,  0.0189,  ...,  0.0014, -0.0166,  0.0031],\n",
      "        [ 0.0190,  0.0197, -0.0004,  ...,  0.0118, -0.0143, -0.0388]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.0297, 0.0136, 0.0020,  ..., 0.0103, 0.0110, 0.0061],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.0503, 0.0525, 0.0500,  ..., 0.0525, 0.0535, 0.0491],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0125,  0.0073, -0.0381,  ..., -0.0024, -0.0588,  0.0356],\n",
      "        [-0.0006, -0.0082,  0.0079,  ..., -0.0083, -0.0488,  0.0277],\n",
      "        [ 0.0306,  0.0325,  0.0205,  ..., -0.0001, -0.0747,  0.0229],\n",
      "        ...,\n",
      "        [-0.0002,  0.0018,  0.0036,  ..., -0.0087, -0.0039, -0.0060],\n",
      "        [-0.0021, -0.0038, -0.0042,  ...,  0.0088,  0.0052,  0.0062],\n",
      "        [ 0.0003,  0.0048,  0.0067,  ..., -0.0079, -0.0005, -0.0111]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-2.4780e-02, -2.5024e-03,  3.8330e-02,  ...,  1.7944e-02,\n",
      "          2.0752e-02, -9.6436e-03],\n",
      "        [-2.9541e-02,  4.5776e-03, -1.1353e-02,  ..., -1.5869e-02,\n",
      "          9.3994e-03, -5.8838e-02],\n",
      "        [-2.5024e-02,  2.9419e-02, -6.4941e-02,  ...,  3.4912e-02,\n",
      "          7.8735e-03, -5.6396e-02],\n",
      "        ...,\n",
      "        [-1.1108e-02,  1.8921e-02, -1.5030e-03,  ...,  1.0925e-02,\n",
      "          6.0797e-05,  6.7139e-03],\n",
      "        [ 7.9956e-03, -1.9165e-02,  3.9978e-03,  ..., -1.2146e-02,\n",
      "         -1.4648e-03, -6.5002e-03],\n",
      "        [-7.9956e-03,  1.4709e-02,  7.1716e-04,  ...,  3.9978e-03,\n",
      "         -1.5640e-03,  6.0425e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0068, -0.0084, -0.0041,  ..., -0.0061, -0.0312, -0.0262],\n",
      "        [-0.0036,  0.0024, -0.0024,  ...,  0.0008,  0.0110,  0.0332],\n",
      "        [ 0.0059, -0.0118,  0.0109,  ...,  0.0004, -0.0009, -0.0019],\n",
      "        ...,\n",
      "        [-0.0053,  0.0027,  0.0046,  ..., -0.0017, -0.0010,  0.0015],\n",
      "        [-0.0063, -0.0056,  0.0094,  ..., -0.0081,  0.0030, -0.0010],\n",
      "        [-0.0010,  0.0016,  0.0042,  ...,  0.0019, -0.0058,  0.0087]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048, -0.0194,  0.0142,  ..., -0.0050, -0.0001,  0.0003],\n",
      "        [-0.0030, -0.0054, -0.0132,  ..., -0.0007, -0.0027,  0.0032],\n",
      "        [ 0.0223,  0.0126, -0.0120,  ...,  0.0047,  0.0020, -0.0022],\n",
      "        ...,\n",
      "        [ 0.0033,  0.0069,  0.0051,  ...,  0.0003,  0.0022, -0.0019],\n",
      "        [ 0.0022, -0.0125,  0.0050,  ..., -0.0022,  0.0018,  0.0021],\n",
      "        [-0.0043, -0.0205,  0.0054,  ...,  0.0013, -0.0043,  0.0019]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0292, -0.0095,  0.0374,  ..., -0.0339, -0.0186, -0.0113],\n",
      "        [-0.0281, -0.0018, -0.0315,  ...,  0.0144, -0.0219, -0.0408],\n",
      "        [-0.0061, -0.0289,  0.0031,  ..., -0.0093, -0.0151, -0.0211],\n",
      "        ...,\n",
      "        [-0.0400,  0.0164, -0.0042,  ..., -0.0016,  0.0144, -0.0014],\n",
      "        [-0.0161,  0.0036, -0.0374,  ...,  0.0206,  0.0195, -0.0120],\n",
      "        [-0.0249,  0.0146,  0.0055,  ...,  0.0364,  0.0099, -0.0347]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0079,  0.0097,  0.0559,  ..., -0.0170,  0.0024, -0.0197],\n",
      "        [ 0.0107,  0.0064, -0.0359,  ...,  0.0091, -0.0132,  0.0029],\n",
      "        [-0.0171, -0.0146, -0.0232,  ..., -0.0014, -0.0106,  0.0034],\n",
      "        ...,\n",
      "        [-0.0102, -0.0065, -0.0162,  ...,  0.0430, -0.0063,  0.0074],\n",
      "        [-0.0129,  0.0215,  0.0149,  ..., -0.0299, -0.0189,  0.0233],\n",
      "        [ 0.0244, -0.0038, -0.0165,  ..., -0.0135, -0.0142,  0.0298]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0017,  0.0356, -0.0175,  ...,  0.0116,  0.0302, -0.0177],\n",
      "        [ 0.0085, -0.0023, -0.0134,  ..., -0.0079,  0.0101,  0.0095],\n",
      "        [ 0.0278, -0.0447,  0.0129,  ...,  0.0168, -0.0454,  0.0289],\n",
      "        ...,\n",
      "        [-0.0137, -0.0271,  0.0133,  ...,  0.0256,  0.0107, -0.0251],\n",
      "        [ 0.0349,  0.0015, -0.0167,  ...,  0.0043, -0.0071, -0.0012],\n",
      "        [-0.0159, -0.0182, -0.0098,  ..., -0.0231, -0.0075,  0.0219]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.1138, 0.1099, 0.1006,  ..., 0.0630, 0.0942, 0.0742],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.0996, 0.1006, 0.0962,  ..., 0.1074, 0.0996, 0.1016],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0233, -0.0091,  0.0077,  ..., -0.0102, -0.0187, -0.0078],\n",
      "        [-0.0148,  0.0107, -0.0374,  ..., -0.0364, -0.0178,  0.0271],\n",
      "        [-0.0143,  0.0330, -0.0256,  ..., -0.0053, -0.0201,  0.0211],\n",
      "        ...,\n",
      "        [-0.0476,  0.0135, -0.0226,  ..., -0.0068, -0.0303, -0.0364],\n",
      "        [ 0.0024, -0.0093,  0.0017,  ..., -0.0012, -0.0156,  0.0137],\n",
      "        [-0.0066, -0.0200,  0.0253,  ...,  0.0625, -0.0469,  0.0022]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0011,  0.0087, -0.0071,  ..., -0.0181, -0.0072,  0.0104],\n",
      "        [ 0.0186, -0.0118, -0.0044,  ...,  0.0171, -0.0067, -0.0009],\n",
      "        [ 0.0135,  0.0128,  0.0354,  ..., -0.0135,  0.0107,  0.0052],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0491, -0.0284,  ...,  0.0305, -0.0400, -0.0062],\n",
      "        [-0.0061, -0.0123, -0.0007,  ...,  0.0041,  0.0013, -0.0040],\n",
      "        [ 0.0059, -0.0156, -0.0045,  ..., -0.0830, -0.0278,  0.0864]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0045,  0.0008,  0.0212,  ...,  0.0396, -0.0060,  0.0048],\n",
      "        [ 0.0028,  0.0141, -0.0219,  ..., -0.0093,  0.0053, -0.0159],\n",
      "        [-0.0007,  0.0152, -0.0013,  ..., -0.0162, -0.0293, -0.0298],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0306, -0.0190,  ..., -0.0131, -0.0017,  0.0011],\n",
      "        [-0.0366, -0.0006,  0.0178,  ..., -0.0076, -0.0104, -0.0060],\n",
      "        [-0.0122,  0.0168,  0.0013,  ..., -0.0052,  0.0157, -0.0078]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0018,  0.0153,  0.0092,  ...,  0.0181,  0.0033, -0.0280],\n",
      "        [-0.0010, -0.0184,  0.0059,  ...,  0.0047, -0.0119, -0.0208],\n",
      "        [-0.0204,  0.0032, -0.0145,  ..., -0.0187, -0.0085,  0.0096],\n",
      "        ...,\n",
      "        [-0.0072, -0.0275,  0.0303,  ..., -0.0042, -0.0216,  0.0206],\n",
      "        [ 0.0205, -0.0157,  0.0359,  ...,  0.0166,  0.0019, -0.0226],\n",
      "        [-0.0120,  0.0013,  0.0097,  ..., -0.0086,  0.0108, -0.0146]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0086,  0.0239, -0.0128,  ..., -0.0086,  0.0275,  0.0019],\n",
      "        [-0.0002,  0.0024, -0.0044,  ...,  0.0105, -0.0146, -0.0077],\n",
      "        [ 0.0018, -0.0089,  0.0266,  ..., -0.0121,  0.0182,  0.0073],\n",
      "        ...,\n",
      "        [ 0.0297, -0.0104, -0.0149,  ..., -0.0339, -0.0408, -0.0122],\n",
      "        [-0.0125, -0.0157, -0.0031,  ...,  0.0223, -0.0496,  0.0131],\n",
      "        [-0.0016,  0.0123,  0.0048,  ..., -0.0129, -0.0291,  0.0056]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 9.8267e-03,  1.5259e-02,  3.2959e-02,  ..., -2.1729e-02,\n",
      "         -2.4414e-03, -3.1494e-02],\n",
      "        [ 2.0142e-02, -2.1973e-02, -1.3733e-02,  ...,  1.2756e-02,\n",
      "          2.9907e-03, -2.3804e-02],\n",
      "        [ 3.9062e-02, -1.8677e-02, -3.0518e-03,  ..., -2.8442e-02,\n",
      "          1.8539e-03, -3.3936e-02],\n",
      "        ...,\n",
      "        [-1.1780e-02,  2.8076e-02, -8.3008e-03,  ..., -1.5616e-05,\n",
      "         -3.7689e-03, -1.3000e-02],\n",
      "        [-2.1484e-02,  3.4668e-02,  9.8267e-03,  ...,  1.0742e-02,\n",
      "         -1.4221e-02, -1.0010e-02],\n",
      "        [ 3.5400e-02,  9.2773e-03, -2.2461e-02,  ...,  2.7710e-02,\n",
      "         -4.1992e-02, -7.0496e-03]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0016, -0.0012,  0.0139,  ..., -0.0198,  0.0069, -0.0007],\n",
      "        [ 0.0074, -0.0179, -0.0089,  ...,  0.0035,  0.0118,  0.0090],\n",
      "        [ 0.0272,  0.0162,  0.0286,  ..., -0.0131, -0.0060,  0.0203],\n",
      "        ...,\n",
      "        [-0.0089, -0.0154,  0.0018,  ..., -0.0164, -0.0078,  0.0214],\n",
      "        [ 0.0146, -0.0041,  0.0129,  ..., -0.0020,  0.0057, -0.0260],\n",
      "        [-0.0148, -0.0288, -0.0232,  ..., -0.0058,  0.0009, -0.0072]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.1738, 0.1777, 0.1738,  ..., 0.1768, 0.1709, 0.1748],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.1338, 0.1367, 0.1357,  ..., 0.1357, 0.1387, 0.1357],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0072,  0.0124,  0.0007,  ...,  0.0283,  0.0149,  0.0217],\n",
      "        [-0.0113,  0.0168, -0.0194,  ..., -0.0437,  0.0007, -0.0057],\n",
      "        [ 0.0154,  0.0062, -0.0200,  ...,  0.0106, -0.0198, -0.0344],\n",
      "        ...,\n",
      "        [ 0.0703, -0.0684,  0.0415,  ..., -0.0762, -0.0120,  0.0060],\n",
      "        [-0.0801,  0.0388, -0.0036,  ...,  0.0334,  0.0718,  0.0206],\n",
      "        [-0.0154, -0.0510,  0.0713,  ..., -0.0239, -0.0087, -0.0129]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0096, -0.0084, -0.0079,  ..., -0.0055,  0.0032,  0.0425],\n",
      "        [-0.0122,  0.0121, -0.0237,  ...,  0.0018, -0.0102, -0.0376],\n",
      "        [ 0.0002, -0.0081, -0.0022,  ...,  0.0061, -0.0066, -0.0322],\n",
      "        ...,\n",
      "        [ 0.0894, -0.0850,  0.0037,  ..., -0.0630, -0.0019,  0.0278],\n",
      "        [-0.0864,  0.0277,  0.0435,  ...,  0.0086,  0.0481, -0.0003],\n",
      "        [ 0.0011, -0.0510,  0.0923,  ..., -0.0157,  0.0069, -0.0115]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0014,  0.0118, -0.0049,  ..., -0.0126,  0.0042, -0.0125],\n",
      "        [-0.0101,  0.0251, -0.0173,  ..., -0.0066, -0.0087, -0.0086],\n",
      "        [ 0.0026, -0.0021,  0.0029,  ..., -0.0391, -0.0127, -0.0280],\n",
      "        ...,\n",
      "        [-0.0077, -0.0098,  0.0025,  ...,  0.0018, -0.0023, -0.0074],\n",
      "        [ 0.0036,  0.0001,  0.0068,  ...,  0.0034, -0.0066,  0.0028],\n",
      "        [-0.0091, -0.0064,  0.0103,  ..., -0.0006,  0.0014,  0.0106]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0273,  0.0070, -0.0172,  ...,  0.0052,  0.0069, -0.0029],\n",
      "        [ 0.0270, -0.0281, -0.0084,  ...,  0.0076, -0.0031,  0.0005],\n",
      "        [-0.0013, -0.0106, -0.0010,  ...,  0.0005,  0.0029,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0148,  0.0130,  ...,  0.0014,  0.0031,  0.0107],\n",
      "        [ 0.0201,  0.0243, -0.0036,  ...,  0.0005, -0.0017, -0.0013],\n",
      "        [ 0.0090, -0.0036,  0.0055,  ...,  0.0041,  0.0028,  0.0088]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-4.2114e-03, -1.2634e-02, -1.7700e-02,  ..., -2.8687e-02,\n",
      "          4.3701e-02,  1.2817e-02],\n",
      "        [ 1.6113e-02,  3.7956e-04, -1.1475e-02,  ..., -7.1716e-03,\n",
      "         -2.3926e-02, -1.0986e-02],\n",
      "        [-2.7710e-02,  1.4877e-03, -1.2878e-02,  ...,  8.2397e-03,\n",
      "         -4.6692e-03, -3.4424e-02],\n",
      "        ...,\n",
      "        [-1.4038e-03,  3.8086e-02,  3.4790e-03,  ...,  1.7090e-02,\n",
      "         -1.2817e-02, -5.1575e-03],\n",
      "        [-5.4016e-03,  7.1716e-03, -2.4261e-03,  ...,  8.6308e-05,\n",
      "          1.0834e-03, -8.3618e-03],\n",
      "        [-2.1362e-03, -1.5991e-02,  2.0752e-03,  ...,  2.1484e-02,\n",
      "         -2.5146e-02,  1.1108e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0008, -0.0067,  0.0055,  ..., -0.0081,  0.0058,  0.0179],\n",
      "        [ 0.0262,  0.0061,  0.0149,  ..., -0.0160, -0.0166, -0.0109],\n",
      "        [ 0.0132,  0.0131, -0.0173,  ..., -0.0139, -0.0079,  0.0054],\n",
      "        ...,\n",
      "        [ 0.0212, -0.0088, -0.0148,  ...,  0.0009,  0.0157,  0.0173],\n",
      "        [-0.0216,  0.0131, -0.0156,  ..., -0.0016, -0.0162,  0.0134],\n",
      "        [ 0.0080, -0.0156, -0.0264,  ..., -0.0017,  0.0082,  0.0084]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0117,  0.0143,  0.0057,  ..., -0.0026,  0.0160, -0.0109],\n",
      "        [-0.0116, -0.0155,  0.0364,  ...,  0.0188,  0.0159, -0.0066],\n",
      "        [-0.0030, -0.0014, -0.0157,  ...,  0.0013,  0.0054, -0.0123],\n",
      "        ...,\n",
      "        [-0.0073, -0.0052, -0.0024,  ...,  0.0234,  0.0114, -0.0243],\n",
      "        [ 0.0038, -0.0103, -0.0082,  ...,  0.0118, -0.0201, -0.0093],\n",
      "        [ 0.0420,  0.0009, -0.0013,  ...,  0.0454,  0.0067,  0.0024]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.2832, 0.2832, 0.2812,  ..., 0.2793, 0.2891, 0.2910],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.1748, 0.1748, 0.1699,  ..., 0.1738, 0.1709, 0.1748],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0176,  0.0045, -0.0019,  ..., -0.0047, -0.0041,  0.0100],\n",
      "        [ 0.0231, -0.0128, -0.0090,  ...,  0.0089, -0.0253, -0.0028],\n",
      "        [-0.0034, -0.0280,  0.0143,  ...,  0.0008, -0.0121, -0.0303],\n",
      "        ...,\n",
      "        [ 0.0226, -0.0038, -0.0037,  ...,  0.0014,  0.0247, -0.0630],\n",
      "        [-0.0312,  0.0138,  0.0269,  ..., -0.0518,  0.0018,  0.0320],\n",
      "        [-0.0121, -0.0339,  0.0649,  ...,  0.0593,  0.0398, -0.0371]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0085,  0.0123, -0.0120,  ..., -0.0182, -0.0030, -0.0001],\n",
      "        [-0.0295, -0.0049,  0.0010,  ..., -0.0027,  0.0132,  0.0052],\n",
      "        [ 0.0166, -0.0002, -0.0141,  ...,  0.0146,  0.0184,  0.0371],\n",
      "        ...,\n",
      "        [-0.0098,  0.1206,  0.0586,  ...,  0.0283, -0.0197, -0.0114],\n",
      "        [ 0.0337,  0.0500, -0.0183,  ...,  0.0547, -0.0400,  0.0684],\n",
      "        [ 0.0091,  0.0610,  0.0165,  ..., -0.0026,  0.0454, -0.0168]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0006, -0.0016, -0.0080,  ..., -0.0156, -0.0075, -0.0004],\n",
      "        [-0.0034,  0.0249,  0.0068,  ...,  0.0088,  0.0017,  0.0110],\n",
      "        [ 0.0140,  0.0012, -0.0366,  ...,  0.0078, -0.0053, -0.0223],\n",
      "        ...,\n",
      "        [-0.0019, -0.0094, -0.0212,  ..., -0.0130, -0.0074,  0.0022],\n",
      "        [-0.0036, -0.0063, -0.0006,  ...,  0.0022, -0.0053,  0.0081],\n",
      "        [-0.0028, -0.0192, -0.0016,  ...,  0.0073, -0.0026,  0.0150]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 3.4424e-02,  9.8877e-03, -1.9379e-03,  ...,  1.1292e-03,\n",
      "         -2.5146e-02, -3.7384e-03],\n",
      "        [ 4.0283e-03, -1.2517e-06, -6.1035e-03,  ...,  4.5471e-03,\n",
      "         -2.0386e-02,  3.6774e-03],\n",
      "        [-1.3199e-03,  5.5542e-03, -1.0254e-02,  ..., -1.0925e-02,\n",
      "          8.1177e-03, -3.5400e-03],\n",
      "        ...,\n",
      "        [-1.0132e-02,  3.0884e-02, -1.5869e-02,  ..., -2.4780e-02,\n",
      "          3.6011e-03, -1.4343e-02],\n",
      "        [-1.3657e-03, -4.7913e-03, -1.4954e-02,  ..., -6.5613e-03,\n",
      "         -1.2054e-03, -7.3547e-03],\n",
      "        [ 1.1597e-02,  2.2217e-02, -7.1716e-03,  ..., -3.0060e-03,\n",
      "          1.4221e-02,  5.3883e-05]], dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.4109e-03,  8.7280e-03, -1.1414e-02,  ..., -1.5747e-02,\n",
      "         -7.3314e-06, -1.4709e-02],\n",
      "        [-2.4658e-02,  1.2756e-02,  1.1353e-02,  ...,  5.1575e-03,\n",
      "          1.3367e-02, -6.3171e-03],\n",
      "        [-8.1177e-03, -2.1240e-02, -3.8300e-03,  ...,  2.1362e-03,\n",
      "         -3.4027e-03,  2.6123e-02],\n",
      "        ...,\n",
      "        [-1.1230e-02, -3.4180e-03,  1.0803e-02,  ..., -2.7100e-02,\n",
      "          4.3335e-03, -1.9455e-03],\n",
      "        [-3.0151e-02,  1.7456e-02, -2.8442e-02,  ...,  1.6357e-02,\n",
      "          2.4780e-02, -5.0049e-03],\n",
      "        [ 6.9427e-04, -1.0498e-02, -1.0437e-02,  ..., -1.1536e-02,\n",
      "          1.8066e-02,  1.6113e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0233, -0.0024,  0.0260,  ..., -0.0194,  0.0047,  0.0315],\n",
      "        [-0.0064, -0.0449,  0.0403,  ..., -0.0238, -0.0201, -0.0051],\n",
      "        [ 0.0004, -0.0173,  0.0229,  ...,  0.0082, -0.0018,  0.0188],\n",
      "        ...,\n",
      "        [-0.0275, -0.0034, -0.0171,  ...,  0.0216,  0.0016,  0.0139],\n",
      "        [-0.0161,  0.0088, -0.0271,  ...,  0.0143, -0.0344, -0.0060],\n",
      "        [ 0.0178, -0.0286, -0.0151,  ..., -0.0032, -0.0457, -0.0269]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0136, -0.0076, -0.0422,  ...,  0.0114,  0.0045, -0.0026],\n",
      "        [-0.0206, -0.0420,  0.0066,  ..., -0.0103,  0.0115, -0.0250],\n",
      "        [-0.0016,  0.0317,  0.0039,  ..., -0.0364, -0.0150,  0.0045],\n",
      "        ...,\n",
      "        [-0.0069, -0.0315, -0.0165,  ...,  0.0079, -0.0085,  0.0491],\n",
      "        [ 0.0099, -0.0074, -0.0152,  ..., -0.0417, -0.0086,  0.0120],\n",
      "        [ 0.0011,  0.0223,  0.0039,  ...,  0.0009, -0.0008, -0.0032]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.2617, 0.2578, 0.2598,  ..., 0.2559, 0.2637, 0.2715],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.1885, 0.1865, 0.1816,  ..., 0.1885, 0.1865, 0.1855],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0064, -0.0048, -0.0155,  ..., -0.0111, -0.0221,  0.0144],\n",
      "        [-0.0242,  0.0145,  0.0396,  ..., -0.0109, -0.0161, -0.0330],\n",
      "        [ 0.0098,  0.0084, -0.0110,  ...,  0.0253, -0.0098,  0.0084],\n",
      "        ...,\n",
      "        [-0.0109,  0.0062, -0.0049,  ...,  0.0474,  0.0056,  0.0325],\n",
      "        [ 0.0369,  0.0152,  0.0189,  ..., -0.0413, -0.0349, -0.0312],\n",
      "        [ 0.0087,  0.0220,  0.0322,  ..., -0.0312, -0.0166,  0.0009]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0145,  0.0295,  0.0062,  ...,  0.0155, -0.0024, -0.0049],\n",
      "        [ 0.0015, -0.0009, -0.0298,  ...,  0.0187, -0.0081,  0.0344],\n",
      "        [-0.0082, -0.0168,  0.0007,  ...,  0.0046, -0.0104,  0.0037],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0007, -0.0234,  ...,  0.0371,  0.0079, -0.0299],\n",
      "        [ 0.0542,  0.0209, -0.0181,  ..., -0.0014, -0.0156, -0.0361],\n",
      "        [-0.0282, -0.0242,  0.0405,  ..., -0.0176, -0.0332,  0.0099]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0359,  0.0062,  0.0143,  ...,  0.0184,  0.0015,  0.0188],\n",
      "        [ 0.0062,  0.0146, -0.0262,  ..., -0.0153,  0.0033, -0.0092],\n",
      "        [ 0.0177,  0.0038,  0.0007,  ..., -0.0206,  0.0292,  0.0041],\n",
      "        ...,\n",
      "        [ 0.0243,  0.0126,  0.0060,  ...,  0.0361, -0.0084, -0.0013],\n",
      "        [-0.0036,  0.0008, -0.0052,  ...,  0.0004, -0.0012, -0.0026],\n",
      "        [-0.0076, -0.0090,  0.0210,  ..., -0.0070, -0.0042, -0.0206]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0133, -0.0060,  0.0039,  ..., -0.0151, -0.0109,  0.0244],\n",
      "        [ 0.0022,  0.0017, -0.0126,  ..., -0.0177,  0.0021, -0.0007],\n",
      "        [-0.0084, -0.0069,  0.0143,  ...,  0.0078,  0.0040,  0.0222],\n",
      "        ...,\n",
      "        [ 0.0178,  0.0057, -0.0047,  ...,  0.0177,  0.0339, -0.0219],\n",
      "        [ 0.0050, -0.0044, -0.0026,  ..., -0.0152, -0.0082, -0.0037],\n",
      "        [ 0.0059,  0.0025, -0.0125,  ...,  0.0075, -0.0084, -0.0195]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0014,  0.0003, -0.0299,  ..., -0.0003, -0.0052, -0.0159],\n",
      "        [ 0.0210, -0.0150, -0.0116,  ...,  0.0287, -0.0008, -0.0022],\n",
      "        [ 0.0106,  0.0057, -0.0153,  ...,  0.0208,  0.0181, -0.0315],\n",
      "        ...,\n",
      "        [ 0.0041,  0.0068,  0.0415,  ..., -0.0688,  0.0125, -0.0215],\n",
      "        [-0.0297,  0.0034,  0.0121,  ...,  0.0047,  0.0193, -0.0057],\n",
      "        [ 0.0292, -0.0339, -0.0148,  ..., -0.0040, -0.0018, -0.0415]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0054, -0.0286, -0.0033,  ..., -0.0027,  0.0071,  0.0140],\n",
      "        [-0.0160,  0.0087,  0.0227,  ...,  0.0221, -0.0046, -0.0309],\n",
      "        [ 0.0177,  0.0228,  0.0120,  ..., -0.0294, -0.0452,  0.0074],\n",
      "        ...,\n",
      "        [ 0.0087, -0.0242, -0.0019,  ..., -0.0018, -0.0248, -0.0204],\n",
      "        [-0.0084, -0.0369,  0.0154,  ...,  0.0148,  0.0089, -0.0515],\n",
      "        [ 0.0164,  0.0105, -0.0164,  ...,  0.0322,  0.0088, -0.0140]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0041, -0.0155, -0.0013,  ..., -0.0135, -0.0055, -0.0125],\n",
      "        [-0.0344, -0.0035, -0.0006,  ...,  0.0078,  0.0225,  0.0222],\n",
      "        [-0.0001,  0.0112,  0.0062,  ...,  0.0045,  0.0126,  0.0171],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0256, -0.0074,  ..., -0.0137,  0.0074,  0.0291],\n",
      "        [ 0.0461, -0.0247, -0.0239,  ..., -0.0047, -0.0160,  0.0234],\n",
      "        [ 0.0093,  0.0223,  0.0066,  ..., -0.0096, -0.0034, -0.0212]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.2637, 0.2637, 0.2598,  ..., 0.2520, 0.2676, 0.2695],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2041, 0.1934, 0.1895,  ..., 0.2051, 0.1992, 0.2031],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0058, -0.0055,  0.0051,  ..., -0.0325, -0.0019, -0.0033],\n",
      "        [-0.0003, -0.0167,  0.0098,  ...,  0.0123, -0.0030, -0.0289],\n",
      "        [-0.0167,  0.0204, -0.0337,  ...,  0.0259, -0.0432, -0.0072],\n",
      "        ...,\n",
      "        [ 0.0157,  0.0098, -0.0349,  ...,  0.0210,  0.0059, -0.0055],\n",
      "        [ 0.0325,  0.0374,  0.0060,  ..., -0.0104, -0.0913, -0.0193],\n",
      "        [-0.0620, -0.0537, -0.0002,  ...,  0.0168,  0.0021,  0.0118]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 1.4771e-02, -1.3367e-02,  1.9531e-02,  ..., -7.8125e-03,\n",
      "          1.9775e-02,  5.5420e-02],\n",
      "        [-1.6357e-02, -5.2795e-03,  4.5654e-02,  ...,  7.0801e-03,\n",
      "         -1.7578e-02,  1.1658e-02],\n",
      "        [-1.0010e-02, -1.3306e-02, -8.3542e-04,  ..., -2.6245e-03,\n",
      "         -2.6978e-02, -1.2939e-02],\n",
      "        ...,\n",
      "        [ 1.7578e-02, -3.8330e-02, -1.5991e-02,  ...,  5.7861e-02,\n",
      "          5.1498e-05, -4.5654e-02],\n",
      "        [-1.9897e-02,  4.7302e-03, -2.9175e-02,  ..., -4.6875e-02,\n",
      "         -4.1504e-02,  1.2451e-02],\n",
      "        [-4.8340e-02, -2.5757e-02, -3.5156e-02,  ...,  1.6235e-02,\n",
      "         -4.0039e-02, -2.5024e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 1.2512e-02,  3.4668e-02,  6.0120e-03,  ...,  1.2390e-02,\n",
      "          2.5635e-03,  5.0049e-03],\n",
      "        [ 3.6133e-02, -6.9046e-04, -7.0496e-03,  ...,  2.8687e-03,\n",
      "          1.7319e-03,  1.3580e-03],\n",
      "        [-8.7280e-03, -5.5542e-03,  1.0925e-02,  ...,  7.2937e-03,\n",
      "          2.1118e-02,  1.8501e-04],\n",
      "        ...,\n",
      "        [-2.2339e-02, -5.7983e-03,  1.0620e-02,  ..., -1.2054e-03,\n",
      "         -1.6113e-02,  6.4392e-03],\n",
      "        [-2.1240e-02, -9.4604e-03,  4.0588e-03,  ..., -4.0894e-03,\n",
      "         -1.0681e-04,  2.2217e-02],\n",
      "        [ 3.5400e-02,  6.2866e-03,  1.8433e-02,  ..., -2.0874e-02,\n",
      "         -5.5552e-05, -2.4719e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0138, -0.0155, -0.0381,  ..., -0.0105,  0.0033,  0.0093],\n",
      "        [-0.0209,  0.0057,  0.0029,  ..., -0.0187,  0.0038, -0.0036],\n",
      "        [ 0.0018, -0.0040, -0.0040,  ...,  0.0115,  0.0025,  0.0098],\n",
      "        ...,\n",
      "        [-0.0013, -0.0146, -0.0128,  ...,  0.0076, -0.0063,  0.0212],\n",
      "        [-0.0226,  0.0081,  0.0019,  ..., -0.0004,  0.0051, -0.0251],\n",
      "        [-0.0187, -0.0164,  0.0064,  ..., -0.0168,  0.0072,  0.0100]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0211,  0.0251, -0.0148,  ..., -0.0062, -0.0095, -0.0081],\n",
      "        [ 0.0339, -0.0052, -0.0139,  ...,  0.0070,  0.0208,  0.0075],\n",
      "        [-0.0198,  0.0226, -0.0143,  ..., -0.0221, -0.0017,  0.0092],\n",
      "        ...,\n",
      "        [ 0.0074, -0.0194, -0.0026,  ...,  0.0150, -0.0255, -0.0039],\n",
      "        [ 0.0115, -0.0020, -0.0166,  ..., -0.0250, -0.0045, -0.0028],\n",
      "        [-0.0315, -0.0500,  0.0239,  ..., -0.0417,  0.0018,  0.0206]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0089,  0.0183, -0.0028,  ..., -0.0009,  0.0136,  0.0168],\n",
      "        [-0.0153, -0.0151,  0.0088,  ..., -0.0023, -0.0278, -0.0112],\n",
      "        [-0.0015,  0.0167, -0.0127,  ...,  0.0260,  0.0244,  0.0150],\n",
      "        ...,\n",
      "        [ 0.0062, -0.0042,  0.0247,  ...,  0.0311, -0.0302, -0.0044],\n",
      "        [ 0.0203, -0.0048,  0.0090,  ...,  0.0107, -0.0143, -0.0110],\n",
      "        [ 0.0161, -0.0178, -0.0049,  ...,  0.0322, -0.0068,  0.0057]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0082, -0.0091, -0.0137,  ...,  0.0074, -0.0046, -0.0160],\n",
      "        [ 0.0069, -0.0432,  0.0366,  ...,  0.0030,  0.0025, -0.0078],\n",
      "        [-0.0255, -0.0200, -0.0212,  ...,  0.0066,  0.0192,  0.0239],\n",
      "        ...,\n",
      "        [-0.0269,  0.0030,  0.0199,  ...,  0.0175,  0.0014,  0.0425],\n",
      "        [ 0.0232,  0.0151, -0.0012,  ..., -0.0281,  0.0164, -0.0052],\n",
      "        [-0.0201,  0.0223, -0.0225,  ...,  0.0064, -0.0156, -0.0459]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.3184, 0.3555, 0.3281,  ..., 0.3164, 0.3359, 0.3203],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2168, 0.2061, 0.2041,  ..., 0.2178, 0.2109, 0.2129],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 6.8970e-03,  2.3804e-03,  3.3569e-03,  ...,  4.0283e-03,\n",
      "         -1.6479e-02,  3.4094e-05],\n",
      "        [-5.4016e-03,  6.1035e-03,  2.3956e-03,  ...,  1.5747e-02,\n",
      "         -1.0559e-02, -1.2146e-02],\n",
      "        [ 1.0605e-03,  7.9346e-03, -1.9531e-02,  ..., -1.8311e-02,\n",
      "         -1.2054e-03,  9.0942e-03],\n",
      "        ...,\n",
      "        [ 1.4191e-03, -3.7354e-02, -2.9785e-02,  ..., -1.5137e-02,\n",
      "          3.2959e-02,  3.6621e-02],\n",
      "        [ 1.0559e-02,  5.4932e-02, -4.1504e-02,  ..., -1.8555e-02,\n",
      "         -6.6406e-02, -5.2979e-02],\n",
      "        [ 8.0078e-02, -2.3926e-02, -1.3123e-02,  ...,  8.8867e-02,\n",
      "          3.6133e-02, -5.7983e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0029, -0.0063, -0.0132,  ...,  0.0085,  0.0039, -0.0046],\n",
      "        [-0.0042, -0.0155,  0.0042,  ..., -0.0061,  0.0027,  0.0220],\n",
      "        [-0.0055, -0.0066,  0.0183,  ..., -0.0079, -0.0117, -0.0178],\n",
      "        ...,\n",
      "        [ 0.0430,  0.0104,  0.0215,  ...,  0.0369,  0.0386,  0.0276],\n",
      "        [ 0.0425,  0.0540, -0.0020,  ..., -0.0337, -0.0332, -0.0034],\n",
      "        [ 0.0334,  0.0084,  0.0109,  ...,  0.0095,  0.0055, -0.0280]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0150,  0.0020,  0.0162,  ...,  0.0266,  0.0059, -0.0134],\n",
      "        [ 0.0289, -0.0306, -0.0019,  ...,  0.0189, -0.0172, -0.0214],\n",
      "        [-0.0076, -0.0059, -0.0049,  ..., -0.0155,  0.0096,  0.0031],\n",
      "        ...,\n",
      "        [-0.0011, -0.0383,  0.0034,  ...,  0.0042, -0.0049, -0.0094],\n",
      "        [-0.0325,  0.0164, -0.0270,  ..., -0.0128,  0.0135, -0.0001],\n",
      "        [-0.0101, -0.0010, -0.0070,  ...,  0.0247,  0.0078,  0.0131]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0148, -0.0164, -0.0050,  ...,  0.0072, -0.0253,  0.0022],\n",
      "        [-0.0020,  0.0242,  0.0110,  ..., -0.0425, -0.0121, -0.0072],\n",
      "        [ 0.0050,  0.0052, -0.0206,  ..., -0.0102, -0.0111, -0.0081],\n",
      "        ...,\n",
      "        [-0.0172, -0.0272, -0.0061,  ...,  0.0234, -0.0203, -0.0004],\n",
      "        [-0.0197,  0.0081,  0.0116,  ...,  0.0078, -0.0129,  0.0145],\n",
      "        [-0.0004,  0.0042,  0.0016,  ...,  0.0080, -0.0145,  0.0059]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0052, -0.0012, -0.0152,  ..., -0.0053, -0.0311, -0.0062],\n",
      "        [-0.0293,  0.0178, -0.0076,  ..., -0.0014, -0.0101, -0.0065],\n",
      "        [ 0.0172, -0.0304, -0.0077,  ...,  0.0303, -0.0194,  0.0061],\n",
      "        ...,\n",
      "        [-0.0058,  0.0057, -0.0120,  ..., -0.0281, -0.0037, -0.0276],\n",
      "        [-0.0036,  0.0204,  0.0115,  ..., -0.0059,  0.0052, -0.0330],\n",
      "        [ 0.0192, -0.0175, -0.0046,  ...,  0.0161,  0.0194, -0.0007]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-8.5449e-03, -1.2085e-02,  9.9945e-04,  ..., -1.3489e-02,\n",
      "         -8.3008e-03, -2.0630e-02],\n",
      "        [-2.8839e-03, -6.2256e-03,  1.9409e-02,  ..., -4.6082e-03,\n",
      "          5.0354e-03,  1.7700e-03],\n",
      "        [ 1.1841e-02,  1.3672e-02,  3.2715e-02,  ..., -1.6479e-02,\n",
      "         -9.3384e-03, -1.4099e-02],\n",
      "        ...,\n",
      "        [-5.8289e-03, -1.8005e-03, -2.1240e-02,  ...,  1.7578e-02,\n",
      "          1.7090e-03, -2.6733e-02],\n",
      "        [ 4.2969e-02, -1.1108e-02,  3.6316e-03,  ..., -1.8555e-02,\n",
      "          5.7602e-04, -1.3245e-02],\n",
      "        [ 1.5991e-02,  1.8921e-02, -3.3447e-02,  ...,  3.9368e-03,\n",
      "         -2.3804e-02, -2.0862e-05]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0211,  0.0007, -0.0019,  ...,  0.0154, -0.0352,  0.0039],\n",
      "        [-0.0113, -0.0219,  0.0305,  ...,  0.0019, -0.0132,  0.0195],\n",
      "        [ 0.0214, -0.0159,  0.0087,  ..., -0.0114,  0.0190, -0.0303],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0002, -0.0179,  ..., -0.0041, -0.0228,  0.0028],\n",
      "        [ 0.0008,  0.0079,  0.0172,  ..., -0.0127,  0.0184, -0.0012],\n",
      "        [-0.0243, -0.0159,  0.0062,  ...,  0.0178,  0.0106,  0.0049]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.3223, 0.3652, 0.3379,  ..., 0.3242, 0.3574, 0.3301],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2314, 0.2158, 0.2178,  ..., 0.2256, 0.2246, 0.2236],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 7.9956e-03, -1.0925e-02, -2.5024e-02,  ..., -5.2795e-03,\n",
      "          2.9602e-03,  2.9144e-03],\n",
      "        [-1.9775e-02, -1.4954e-02, -3.4180e-02,  ..., -3.2349e-03,\n",
      "         -1.0529e-03, -3.7231e-03],\n",
      "        [-3.1128e-02, -2.3956e-03, -1.0376e-02,  ...,  4.6387e-03,\n",
      "          1.7578e-02, -1.9043e-02],\n",
      "        ...,\n",
      "        [-6.8359e-03,  8.3984e-02,  4.2480e-02,  ..., -2.8564e-02,\n",
      "         -2.7344e-02, -4.5410e-02],\n",
      "        [ 1.5378e-05, -6.2988e-02, -1.6479e-02,  ...,  3.6377e-02,\n",
      "          2.0142e-02,  1.8616e-03],\n",
      "        [-6.2988e-02, -5.7617e-02, -1.3916e-02,  ..., -4.7363e-02,\n",
      "          2.9419e-02, -1.8311e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0064,  0.0045, -0.0060,  ..., -0.0057,  0.0060,  0.0098],\n",
      "        [-0.0068,  0.0056, -0.0070,  ...,  0.0243, -0.0017,  0.0154],\n",
      "        [-0.0113,  0.0023,  0.0052,  ...,  0.0018, -0.0148, -0.0171],\n",
      "        ...,\n",
      "        [-0.0027, -0.0378, -0.0076,  ..., -0.0322, -0.0154,  0.0276],\n",
      "        [ 0.0035,  0.0249,  0.0256,  ..., -0.0376,  0.0444, -0.0576],\n",
      "        [ 0.0154, -0.0047, -0.0330,  ...,  0.0226,  0.0095, -0.0125]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0260, -0.0129,  0.0044,  ..., -0.0159, -0.0114, -0.0079],\n",
      "        [-0.0226, -0.0135,  0.0071,  ...,  0.0270,  0.0047,  0.0198],\n",
      "        [-0.0052,  0.0162,  0.0125,  ..., -0.0034,  0.0074, -0.0208],\n",
      "        ...,\n",
      "        [ 0.0295, -0.0159,  0.0030,  ...,  0.0220, -0.0175,  0.0221],\n",
      "        [ 0.0261, -0.0156,  0.0023,  ...,  0.0079, -0.0080,  0.0007],\n",
      "        [ 0.0165,  0.0001, -0.0055,  ..., -0.0021,  0.0041, -0.0066]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 1.3611e-02,  1.3184e-02, -1.3306e-02,  ..., -1.0498e-02,\n",
      "          1.3046e-03,  1.5747e-02],\n",
      "        [ 1.0071e-02,  4.2725e-03, -3.2471e-02,  ..., -6.6223e-03,\n",
      "         -9.5215e-03, -1.3855e-02],\n",
      "        [ 1.4954e-03,  1.0681e-02, -3.9062e-03,  ...,  1.1292e-03,\n",
      "          1.4771e-02,  2.1118e-02],\n",
      "        ...,\n",
      "        [-1.7090e-02,  2.4048e-02, -6.9885e-03,  ..., -9.3937e-05,\n",
      "         -1.9836e-03, -2.1362e-03],\n",
      "        [ 4.1809e-03, -9.6321e-05, -4.1809e-03,  ...,  1.3199e-03,\n",
      "          7.7515e-03, -2.2461e-02],\n",
      "        [-2.6733e-02,  6.4087e-03, -7.9346e-03,  ...,  2.2217e-02,\n",
      "         -9.7656e-03, -2.6703e-03]], dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0112,  0.0041, -0.0420,  ..., -0.0070, -0.0259, -0.0231],\n",
      "        [-0.0029, -0.0432,  0.0104,  ..., -0.0012, -0.0464,  0.0201],\n",
      "        [ 0.0183,  0.0167,  0.0231,  ...,  0.0247,  0.0222, -0.0117],\n",
      "        ...,\n",
      "        [-0.0131,  0.0030,  0.0073,  ...,  0.0242,  0.0146,  0.0056],\n",
      "        [-0.0129, -0.0012,  0.0070,  ..., -0.0010, -0.0231,  0.0251],\n",
      "        [-0.0248,  0.0056, -0.0048,  ..., -0.0146,  0.0119, -0.0010]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0109, -0.0055,  0.0183,  ...,  0.0096,  0.0140, -0.0236],\n",
      "        [ 0.0142,  0.0132, -0.0208,  ..., -0.0062,  0.0070, -0.0231],\n",
      "        [ 0.0015, -0.0229, -0.0269,  ..., -0.0129,  0.0058, -0.0066],\n",
      "        ...,\n",
      "        [ 0.0150, -0.0210, -0.0359,  ...,  0.0032,  0.0054, -0.0175],\n",
      "        [ 0.0151,  0.0040, -0.0063,  ..., -0.0034, -0.0070, -0.0176],\n",
      "        [-0.0002,  0.0133,  0.0006,  ...,  0.0012,  0.0026,  0.0012]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0103,  0.0176, -0.0146,  ...,  0.0292,  0.0072, -0.0103],\n",
      "        [-0.0496,  0.0175, -0.0420,  ...,  0.0146, -0.0033, -0.0064],\n",
      "        [ 0.0104,  0.0045,  0.0087,  ..., -0.0139, -0.0199,  0.0298],\n",
      "        ...,\n",
      "        [-0.0142,  0.0393, -0.0187,  ...,  0.0145, -0.0090,  0.0094],\n",
      "        [-0.0001, -0.0046, -0.0079,  ..., -0.0161,  0.0204, -0.0153],\n",
      "        [-0.0145,  0.0161, -0.0142,  ..., -0.0155,  0.0015,  0.0033]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.3320, 0.3457, 0.3301,  ..., 0.3203, 0.3438, 0.3223],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2383, 0.2236, 0.2178,  ..., 0.2363, 0.2285, 0.2256],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0194,  0.0025,  0.0168,  ..., -0.0049, -0.0095, -0.0164],\n",
      "        [ 0.0022, -0.0349,  0.0134,  ...,  0.0051, -0.0066,  0.0017],\n",
      "        [ 0.0023, -0.0017, -0.0148,  ..., -0.0223, -0.0299, -0.0223],\n",
      "        ...,\n",
      "        [ 0.0037, -0.0452,  0.0189,  ...,  0.0242,  0.0159, -0.0205],\n",
      "        [ 0.0146, -0.0212,  0.0110,  ...,  0.0099,  0.0139,  0.0097],\n",
      "        [-0.0010,  0.0547,  0.0197,  ..., -0.0557,  0.0488, -0.0043]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0347,  0.0101, -0.0167,  ...,  0.0084, -0.0195, -0.0075],\n",
      "        [-0.0057,  0.0325,  0.0094,  ..., -0.0108,  0.0100,  0.0022],\n",
      "        [-0.0118, -0.0110,  0.0189,  ..., -0.0047, -0.0124, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0114, -0.0173, -0.0068,  ..., -0.0115,  0.0219,  0.0222],\n",
      "        [ 0.0325, -0.0101,  0.0215,  ...,  0.0452, -0.0121,  0.0515],\n",
      "        [ 0.0061, -0.0264,  0.0302,  ...,  0.0095,  0.0256, -0.0223]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0032,  0.0104, -0.0009,  ...,  0.0075, -0.0212, -0.0184],\n",
      "        [-0.0164, -0.0175, -0.0008,  ...,  0.0298, -0.0009, -0.0031],\n",
      "        [ 0.0023, -0.0259,  0.0205,  ..., -0.0261,  0.0068, -0.0112],\n",
      "        ...,\n",
      "        [-0.0069, -0.0131,  0.0044,  ...,  0.0035, -0.0223, -0.0006],\n",
      "        [-0.0149,  0.0032, -0.0104,  ..., -0.0121,  0.0001, -0.0128],\n",
      "        [ 0.0236,  0.0206, -0.0018,  ..., -0.0147, -0.0059, -0.0038]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0096,  0.0106,  0.0052,  ..., -0.0104,  0.0010, -0.0020],\n",
      "        [-0.0051, -0.0149, -0.0023,  ..., -0.0002,  0.0073,  0.0145],\n",
      "        [-0.0337, -0.0315,  0.0118,  ...,  0.0264,  0.0014,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0164, -0.0147,  0.0123,  ...,  0.0077,  0.0068, -0.0090],\n",
      "        [ 0.0012,  0.0179,  0.0023,  ...,  0.0128,  0.0001, -0.0013],\n",
      "        [ 0.0140,  0.0505, -0.0205,  ..., -0.0042,  0.0074,  0.0063]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0079,  0.0264, -0.0068,  ..., -0.0249, -0.0106,  0.0023],\n",
      "        [-0.0248,  0.0275,  0.0291,  ...,  0.0292,  0.0312, -0.0035],\n",
      "        [ 0.0376,  0.0291, -0.0243,  ..., -0.0264,  0.0111, -0.0041],\n",
      "        ...,\n",
      "        [-0.0134, -0.0133, -0.0293,  ..., -0.0011, -0.0403,  0.0045],\n",
      "        [-0.0137, -0.0240,  0.0237,  ..., -0.0147,  0.0021,  0.0135],\n",
      "        [ 0.0118, -0.0176, -0.0099,  ...,  0.0129, -0.0148,  0.0104]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0045,  0.0249, -0.0415,  ...,  0.0004, -0.0145,  0.0103],\n",
      "        [ 0.0253, -0.0215, -0.0184,  ..., -0.0013, -0.0277, -0.0312],\n",
      "        [-0.0017,  0.0040,  0.0015,  ..., -0.0154,  0.0021, -0.0029],\n",
      "        ...,\n",
      "        [-0.0096,  0.0190,  0.0011,  ..., -0.0006, -0.0299,  0.0079],\n",
      "        [-0.0347,  0.0008,  0.0141,  ..., -0.0251, -0.0008, -0.0320],\n",
      "        [ 0.0220,  0.0173,  0.0149,  ...,  0.0435,  0.0166,  0.0181]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0128,  0.0117,  0.0121,  ..., -0.0005, -0.0053,  0.0116],\n",
      "        [ 0.0100, -0.0020,  0.0014,  ...,  0.0146,  0.0178, -0.0204],\n",
      "        [-0.0205, -0.0093, -0.0200,  ..., -0.0038, -0.0099, -0.0031],\n",
      "        ...,\n",
      "        [-0.0037,  0.0098, -0.0131,  ..., -0.0069, -0.0300, -0.0192],\n",
      "        [-0.0004,  0.0176, -0.0315,  ..., -0.0317, -0.0099, -0.0153],\n",
      "        [ 0.0011, -0.0354, -0.0129,  ..., -0.0134,  0.0237, -0.0002]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.3516, 0.3594, 0.3223,  ..., 0.3496, 0.3457, 0.3398],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2422, 0.2305, 0.2217,  ..., 0.2373, 0.2363, 0.2324],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-1.4114e-03,  4.8218e-03, -1.6479e-03,  ...,  1.0193e-02,\n",
      "         -4.2419e-03, -8.3618e-03],\n",
      "        [-4.0588e-03, -7.3853e-03,  1.6602e-02,  ..., -6.0120e-03,\n",
      "          5.9814e-02,  8.7261e-05],\n",
      "        [-9.9487e-03, -8.6060e-03, -5.4016e-03,  ...,  2.0386e-02,\n",
      "         -1.5320e-02, -1.7090e-02],\n",
      "        ...,\n",
      "        [ 3.2959e-02,  6.3965e-02, -1.5198e-02,  ...,  3.0762e-02,\n",
      "          3.6621e-02, -8.3008e-03],\n",
      "        [-5.9082e-02,  5.7617e-02, -2.0409e-04,  ..., -6.8359e-02,\n",
      "         -6.8665e-04, -8.0566e-03],\n",
      "        [-4.9805e-02, -2.1606e-02, -4.9744e-03,  ..., -9.5215e-03,\n",
      "         -3.2471e-02, -1.8311e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-2.2339e-02,  7.4463e-03, -5.5542e-03,  ..., -1.0193e-02,\n",
      "          4.5471e-03, -4.3335e-03],\n",
      "        [-1.0803e-02,  7.2937e-03, -5.9509e-03,  ...,  1.0559e-02,\n",
      "         -1.2024e-02,  7.5684e-03],\n",
      "        [-5.0964e-03,  9.3842e-04,  6.5308e-03,  ..., -1.5991e-02,\n",
      "          2.3071e-02, -4.2915e-06],\n",
      "        ...,\n",
      "        [-4.4922e-02, -5.9326e-02,  7.8125e-03,  ...,  3.1250e-02,\n",
      "         -7.8125e-02,  8.9111e-03],\n",
      "        [-3.7109e-02, -7.2937e-03,  2.9541e-02,  ..., -1.9165e-02,\n",
      "         -4.4189e-02, -2.2217e-02],\n",
      "        [-2.8564e-02,  2.6733e-02, -2.1240e-02,  ...,  1.5625e-02,\n",
      "          4.8340e-02, -9.1553e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0400,  0.0035, -0.0065,  ...,  0.0071,  0.0110,  0.0103],\n",
      "        [-0.0080,  0.0132,  0.0123,  ..., -0.0078, -0.0388,  0.0187],\n",
      "        [ 0.0149, -0.0125,  0.0152,  ...,  0.0155,  0.0065, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0231,  0.0291,  0.0002,  ...,  0.0282, -0.0052, -0.0159],\n",
      "        [ 0.0025,  0.0078,  0.0007,  ...,  0.0018,  0.0117,  0.0001],\n",
      "        [ 0.0143,  0.0087, -0.0153,  ..., -0.0046, -0.0125,  0.0299]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0030,  0.0015, -0.0315,  ..., -0.0005,  0.0104,  0.0082],\n",
      "        [ 0.0010, -0.0121,  0.0126,  ...,  0.0035,  0.0138,  0.0047],\n",
      "        [ 0.0132, -0.0361, -0.0200,  ...,  0.0085, -0.0094, -0.0055],\n",
      "        ...,\n",
      "        [-0.0011,  0.0125, -0.0121,  ..., -0.0049,  0.0037,  0.0217],\n",
      "        [ 0.0054, -0.0123, -0.0220,  ...,  0.0117,  0.0101,  0.0034],\n",
      "        [ 0.0090, -0.0129, -0.0256,  ...,  0.0118, -0.0003,  0.0022]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0317, -0.0420, -0.0264,  ..., -0.0052, -0.0015, -0.0097],\n",
      "        [-0.0173, -0.0147, -0.0137,  ...,  0.0029,  0.0145,  0.0166],\n",
      "        [-0.0157, -0.0212,  0.0092,  ...,  0.0304, -0.0035,  0.0012],\n",
      "        ...,\n",
      "        [-0.0469, -0.0023,  0.0125,  ...,  0.0075, -0.0311,  0.0199],\n",
      "        [ 0.0060,  0.0195,  0.0192,  ...,  0.0047, -0.0115, -0.0032],\n",
      "        [-0.0079, -0.0148, -0.0209,  ..., -0.0121,  0.0034, -0.0157]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-9.7656e-03, -2.7832e-02, -4.0283e-03,  ...,  2.9175e-02,\n",
      "         -1.4771e-02, -1.4544e-05],\n",
      "        [-3.9978e-03,  1.8066e-02, -7.7515e-03,  ..., -8.6060e-03,\n",
      "          1.0803e-02, -6.9427e-04],\n",
      "        [-3.2227e-02, -2.0447e-03,  1.6479e-02,  ..., -2.5024e-02,\n",
      "         -2.5269e-02,  8.1177e-03],\n",
      "        ...,\n",
      "        [ 1.4343e-02, -4.9744e-03,  2.6550e-03,  ...,  1.4954e-02,\n",
      "          1.3306e-02,  7.6599e-03],\n",
      "        [-2.2278e-03,  4.6387e-02, -1.5747e-02,  ...,  1.7700e-02,\n",
      "          3.1494e-02, -3.1982e-02],\n",
      "        [-4.8584e-02, -7.2002e-05, -2.8198e-02,  ..., -1.7822e-02,\n",
      "         -1.4587e-02,  1.0254e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0430, -0.0033, -0.0374,  ...,  0.0109, -0.0082, -0.0209],\n",
      "        [-0.0073,  0.0097,  0.0042,  ...,  0.0234,  0.0366, -0.0137],\n",
      "        [-0.0004, -0.0015,  0.0120,  ...,  0.0243, -0.0330, -0.0023],\n",
      "        ...,\n",
      "        [ 0.0220,  0.0214, -0.0012,  ..., -0.0432, -0.0025, -0.0016],\n",
      "        [-0.0043, -0.0018, -0.0278,  ...,  0.0209, -0.0153,  0.0142],\n",
      "        [-0.0020,  0.0143,  0.0034,  ...,  0.0003,  0.0077,  0.0153]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.3633, 0.3594, 0.3203,  ..., 0.3398, 0.3477, 0.3359],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2461, 0.2324, 0.2236,  ..., 0.2402, 0.2373, 0.2373],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0153,  0.0165, -0.0070,  ...,  0.0217, -0.0050,  0.0003],\n",
      "        [-0.0132,  0.0091,  0.0111,  ...,  0.0142,  0.0142, -0.0004],\n",
      "        [ 0.0009,  0.0063, -0.0240,  ..., -0.0020, -0.0103,  0.0059],\n",
      "        ...,\n",
      "        [ 0.0006, -0.0048,  0.0325,  ...,  0.0344,  0.0405, -0.0302],\n",
      "        [ 0.0447,  0.0179, -0.0120,  ..., -0.0082,  0.0184, -0.0293],\n",
      "        [ 0.0718, -0.0206, -0.0214,  ..., -0.0205,  0.0461,  0.0574]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0050,  0.0187, -0.0009,  ..., -0.0106,  0.0024, -0.0212],\n",
      "        [-0.0003, -0.0034, -0.0254,  ..., -0.0079, -0.0046,  0.0264],\n",
      "        [ 0.0147, -0.0254,  0.0073,  ...,  0.0072,  0.0052,  0.0053],\n",
      "        ...,\n",
      "        [-0.0085,  0.0211,  0.0025,  ..., -0.0190, -0.0120,  0.0305],\n",
      "        [ 0.0645, -0.0107, -0.0118,  ..., -0.0029,  0.0055, -0.0077],\n",
      "        [ 0.0060,  0.0056,  0.0181,  ..., -0.0048,  0.0610,  0.0243]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0050,  0.0046, -0.0101,  ..., -0.0190,  0.0007,  0.0059],\n",
      "        [ 0.0023, -0.0065, -0.0121,  ...,  0.0066,  0.0056, -0.0243],\n",
      "        [ 0.0025,  0.0069, -0.0069,  ...,  0.0147,  0.0046, -0.0225],\n",
      "        ...,\n",
      "        [-0.0154,  0.0063, -0.0141,  ...,  0.0056, -0.0189, -0.0023],\n",
      "        [-0.0083,  0.0150, -0.0103,  ..., -0.0144, -0.0172,  0.0034],\n",
      "        [ 0.0131, -0.0228,  0.0027,  ..., -0.0393, -0.0039, -0.0131]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0183, -0.0008,  0.0052,  ...,  0.0092,  0.0112,  0.0015],\n",
      "        [ 0.0016, -0.0261,  0.0173,  ..., -0.0120,  0.0018, -0.0162],\n",
      "        [-0.0077, -0.0050,  0.0193,  ...,  0.0010, -0.0002,  0.0190],\n",
      "        ...,\n",
      "        [ 0.0038, -0.0052,  0.0232,  ...,  0.0016,  0.0012, -0.0074],\n",
      "        [ 0.0026,  0.0217,  0.0096,  ...,  0.0155,  0.0111, -0.0317],\n",
      "        [-0.0034, -0.0044,  0.0019,  ..., -0.0059,  0.0015, -0.0129]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0447, -0.0100,  0.0017,  ...,  0.0038, -0.0060,  0.0381],\n",
      "        [ 0.0021, -0.0109,  0.0234,  ...,  0.0110, -0.0186, -0.0435],\n",
      "        [-0.0022, -0.0442, -0.0378,  ..., -0.0007, -0.0039,  0.0034],\n",
      "        ...,\n",
      "        [-0.0181, -0.0344, -0.0189,  ..., -0.0041, -0.0282,  0.0013],\n",
      "        [ 0.0025,  0.0062, -0.0259,  ..., -0.0128,  0.0097,  0.0005],\n",
      "        [-0.0077,  0.0101,  0.0120,  ...,  0.0344, -0.0032, -0.0184]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0199,  0.0457,  0.0177,  ...,  0.0194,  0.0145, -0.0043],\n",
      "        [-0.0145, -0.0045, -0.0256,  ..., -0.0312, -0.0093,  0.0155],\n",
      "        [-0.0200, -0.0090,  0.0068,  ...,  0.0015, -0.0187, -0.0063],\n",
      "        ...,\n",
      "        [ 0.0188,  0.0254, -0.0124,  ...,  0.0018,  0.0012,  0.0087],\n",
      "        [-0.0156, -0.0299, -0.0309,  ..., -0.0220, -0.0058,  0.0140],\n",
      "        [ 0.0352,  0.0194,  0.0222,  ...,  0.0038, -0.0125,  0.0405]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0101, -0.0012, -0.0260,  ...,  0.0260,  0.0037,  0.0310],\n",
      "        [ 0.0155,  0.0017, -0.0187,  ...,  0.0096, -0.0410,  0.0038],\n",
      "        [-0.0106, -0.0291, -0.0024,  ..., -0.0089, -0.0159,  0.0166],\n",
      "        ...,\n",
      "        [ 0.0422,  0.0126, -0.0036,  ..., -0.0019,  0.0020,  0.0197],\n",
      "        [-0.0107,  0.0082,  0.0101,  ...,  0.0197,  0.0076,  0.0067],\n",
      "        [ 0.0014, -0.0024, -0.0081,  ..., -0.0143,  0.0058,  0.0265]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.3945, 0.3926, 0.3594,  ..., 0.3828, 0.3770, 0.3672],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2539, 0.2363, 0.2334,  ..., 0.2500, 0.2490, 0.2451],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0045, -0.0197,  0.0095,  ...,  0.0189, -0.0055,  0.0078],\n",
      "        [ 0.0046,  0.0053,  0.0007,  ..., -0.0123,  0.0262,  0.0164],\n",
      "        [ 0.0084,  0.0356, -0.0292,  ...,  0.0036,  0.0138, -0.0317],\n",
      "        ...,\n",
      "        [-0.0041,  0.0386, -0.0206,  ..., -0.0312, -0.0087,  0.0114],\n",
      "        [ 0.0347, -0.0062,  0.0109,  ...,  0.0344,  0.0050, -0.0327],\n",
      "        [ 0.0339,  0.0135, -0.0410,  ..., -0.0178,  0.0175,  0.0249]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0117, -0.0044, -0.0020,  ...,  0.0139,  0.0044, -0.0078],\n",
      "        [ 0.0039,  0.0166, -0.0081,  ...,  0.0118, -0.0152, -0.0197],\n",
      "        [-0.0077, -0.0195,  0.0070,  ...,  0.0166, -0.0074,  0.0155],\n",
      "        ...,\n",
      "        [ 0.0153,  0.0280, -0.0177,  ..., -0.0391, -0.0155, -0.0219],\n",
      "        [-0.0369,  0.0057,  0.0270,  ..., -0.0247,  0.0195, -0.0022],\n",
      "        [ 0.0007,  0.0352,  0.0095,  ...,  0.0139, -0.0459, -0.0330]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0053,  0.0054,  0.0045,  ..., -0.0071,  0.0082, -0.0183],\n",
      "        [-0.0265,  0.0166, -0.0017,  ..., -0.0215, -0.0006,  0.0153],\n",
      "        [-0.0121, -0.0116,  0.0045,  ..., -0.0016,  0.0439, -0.0160],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0023, -0.0175,  ..., -0.0056, -0.0129, -0.0013],\n",
      "        [ 0.0007, -0.0120, -0.0060,  ...,  0.0032,  0.0118,  0.0044],\n",
      "        [ 0.0215,  0.0017,  0.0021,  ...,  0.0194,  0.0067, -0.0108]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0272,  0.0067, -0.0007,  ...,  0.0018,  0.0079, -0.0037],\n",
      "        [-0.0058, -0.0107,  0.0014,  ..., -0.0136,  0.0049, -0.0188],\n",
      "        [ 0.0031, -0.0052, -0.0048,  ...,  0.0056, -0.0267,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0025, -0.0016,  0.0136,  ..., -0.0066, -0.0011,  0.0011],\n",
      "        [-0.0121, -0.0344, -0.0298,  ..., -0.0054,  0.0091,  0.0172],\n",
      "        [ 0.0192,  0.0063,  0.0105,  ...,  0.0208, -0.0121, -0.0176]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0070, -0.0255,  0.0107,  ..., -0.0044,  0.0211,  0.0067],\n",
      "        [-0.0250, -0.0270,  0.0151,  ...,  0.0155,  0.0162,  0.0145],\n",
      "        [ 0.0051,  0.0223, -0.0056,  ..., -0.0078, -0.0430,  0.0015],\n",
      "        ...,\n",
      "        [-0.0003,  0.0087,  0.0128,  ..., -0.0244, -0.0010,  0.0212],\n",
      "        [-0.0349, -0.0073,  0.0042,  ...,  0.0067,  0.0106, -0.0027],\n",
      "        [ 0.0023, -0.0186, -0.0072,  ..., -0.0032, -0.0084, -0.0156]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0045, -0.0067, -0.0317,  ..., -0.0117,  0.0034,  0.0073],\n",
      "        [ 0.0170, -0.0237,  0.0016,  ..., -0.0086, -0.0106, -0.0161],\n",
      "        [ 0.0146,  0.0014,  0.0179,  ...,  0.0060, -0.0089, -0.0208],\n",
      "        ...,\n",
      "        [ 0.0154, -0.0128, -0.0447,  ...,  0.0391, -0.0188, -0.0170],\n",
      "        [-0.0249,  0.0137,  0.0100,  ..., -0.0062,  0.0374, -0.0219],\n",
      "        [ 0.0057,  0.0204,  0.0276,  ..., -0.0342,  0.0176, -0.0025]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040,  0.0186,  0.0062,  ..., -0.0056,  0.0172,  0.0170],\n",
      "        [-0.0030, -0.0132, -0.0106,  ...,  0.0034, -0.0117,  0.0260],\n",
      "        [ 0.0088, -0.0026,  0.0120,  ..., -0.0464,  0.0128,  0.0038],\n",
      "        ...,\n",
      "        [-0.0160, -0.0342, -0.0059,  ...,  0.0137, -0.0187,  0.0044],\n",
      "        [ 0.0029, -0.0310, -0.0033,  ...,  0.0099,  0.0079, -0.0122],\n",
      "        [-0.0119, -0.0171, -0.0041,  ...,  0.0376, -0.0047, -0.0332]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4023, 0.3984, 0.3652,  ..., 0.3789, 0.3828, 0.3848],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2578, 0.2441, 0.2373,  ..., 0.2559, 0.2539, 0.2539],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0030, -0.0080, -0.0132,  ...,  0.0087, -0.0038, -0.0089],\n",
      "        [-0.0156, -0.0122,  0.0009,  ..., -0.0024,  0.0085, -0.0036],\n",
      "        [-0.0159,  0.0099,  0.0072,  ..., -0.0078, -0.0057,  0.0034],\n",
      "        ...,\n",
      "        [ 0.0427,  0.0049,  0.0014,  ...,  0.0493,  0.0188, -0.0464],\n",
      "        [ 0.0049,  0.0113, -0.0217,  ...,  0.0118,  0.0148, -0.0085],\n",
      "        [-0.0073, -0.0220, -0.0153,  ..., -0.0121,  0.0222, -0.0072]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0123,  0.0064, -0.0034,  ..., -0.0182,  0.0264,  0.0127],\n",
      "        [ 0.0099,  0.0187, -0.0039,  ...,  0.0062,  0.0079, -0.0008],\n",
      "        [ 0.0035, -0.0089,  0.0337,  ...,  0.0001,  0.0156, -0.0315],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0320, -0.0018,  ...,  0.0060, -0.0052, -0.0150],\n",
      "        [-0.0042,  0.0070,  0.0278,  ..., -0.0454, -0.0164,  0.0334],\n",
      "        [-0.0237, -0.0483, -0.0120,  ..., -0.0339, -0.0283,  0.0084]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0154,  0.0031, -0.0125,  ..., -0.0032,  0.0128,  0.0142],\n",
      "        [-0.0085,  0.0141, -0.0044,  ...,  0.0159, -0.0080, -0.0042],\n",
      "        [ 0.0183,  0.0013, -0.0016,  ..., -0.0222,  0.0369, -0.0111],\n",
      "        ...,\n",
      "        [-0.0023,  0.0214, -0.0066,  ..., -0.0074,  0.0269, -0.0181],\n",
      "        [-0.0129,  0.0115,  0.0137,  ...,  0.0014,  0.0311,  0.0081],\n",
      "        [-0.0051, -0.0096,  0.0251,  ...,  0.0170, -0.0002,  0.0427]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-3.8757e-03,  3.7766e-04, -5.8289e-03,  ...,  2.7466e-04,\n",
      "         -4.9744e-03, -3.4790e-03],\n",
      "        [ 2.4170e-02,  1.0681e-02,  4.2725e-03,  ..., -1.2268e-02,\n",
      "         -8.9722e-03,  2.3499e-03],\n",
      "        [ 6.5613e-03,  9.2773e-03, -6.0558e-05,  ...,  5.7373e-03,\n",
      "         -2.5757e-02, -1.0376e-02],\n",
      "        ...,\n",
      "        [-7.8735e-03,  1.0071e-02,  1.4221e-02,  ..., -1.1536e-02,\n",
      "         -7.6294e-03, -1.5259e-02],\n",
      "        [ 2.9907e-03,  2.4567e-03,  2.9541e-02,  ..., -2.3193e-03,\n",
      "         -2.4170e-02, -2.6611e-02],\n",
      "        [ 1.1414e-02,  5.1270e-03,  9.1553e-03,  ...,  1.3123e-02,\n",
      "         -8.9111e-03, -2.1240e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0208,  0.0082, -0.0063,  ...,  0.0070,  0.0044, -0.0140],\n",
      "        [-0.0265,  0.0352,  0.0205,  ...,  0.0383, -0.0060, -0.0231],\n",
      "        [ 0.0077, -0.0232, -0.0095,  ...,  0.0015, -0.0184, -0.0123],\n",
      "        ...,\n",
      "        [ 0.0200, -0.0120,  0.0145,  ..., -0.0075,  0.0168,  0.0154],\n",
      "        [ 0.0211, -0.0026,  0.0137,  ...,  0.0149,  0.0070, -0.0255],\n",
      "        [ 0.0058,  0.0369,  0.0206,  ..., -0.0049,  0.0194,  0.0292]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0067, -0.0025, -0.0058,  ...,  0.0311, -0.0299, -0.0243],\n",
      "        [-0.0097,  0.0544, -0.0474,  ..., -0.0067,  0.0043, -0.0100],\n",
      "        [ 0.0010, -0.0034,  0.0186,  ...,  0.0067, -0.0287, -0.0052],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0002,  0.0275,  ...,  0.0076, -0.0187,  0.0356],\n",
      "        [ 0.0082,  0.0029,  0.0140,  ..., -0.0095,  0.0109, -0.0121],\n",
      "        [-0.0014, -0.0292,  0.0337,  ...,  0.0065,  0.0063,  0.0361]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0359,  0.0212, -0.0086,  ...,  0.0056,  0.0133, -0.0006],\n",
      "        [ 0.0145, -0.0081,  0.0344,  ...,  0.0398,  0.0277,  0.0068],\n",
      "        [ 0.0204, -0.0273,  0.0187,  ..., -0.0026,  0.0020, -0.0051],\n",
      "        ...,\n",
      "        [-0.0056,  0.0010,  0.0250,  ...,  0.0068, -0.0280,  0.0139],\n",
      "        [-0.0077, -0.0200,  0.0089,  ...,  0.0143,  0.0544,  0.0061],\n",
      "        [ 0.0105, -0.0215,  0.0154,  ..., -0.0203, -0.0020,  0.0219]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4141, 0.4043, 0.3711,  ..., 0.3867, 0.3809, 0.3906],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2637, 0.2520, 0.2451,  ..., 0.2637, 0.2656, 0.2598],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0062,  0.0010,  0.0200,  ..., -0.0051,  0.0114,  0.0325],\n",
      "        [ 0.0043,  0.0126, -0.0088,  ..., -0.0366, -0.0168, -0.0281],\n",
      "        [-0.0052, -0.0064,  0.0092,  ...,  0.0118,  0.0091, -0.0292],\n",
      "        ...,\n",
      "        [-0.0141, -0.0011, -0.0287,  ..., -0.0065,  0.0140, -0.0052],\n",
      "        [ 0.0234, -0.0339,  0.0540,  ..., -0.0120,  0.0099, -0.0067],\n",
      "        [-0.0125,  0.0028, -0.0117,  ...,  0.0139,  0.0137, -0.0481]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0108, -0.0028,  0.0087,  ..., -0.0074,  0.0214,  0.0425],\n",
      "        [ 0.0281,  0.0261, -0.0221,  ..., -0.0063, -0.0183,  0.0080],\n",
      "        [-0.0203, -0.0003,  0.0209,  ...,  0.0225,  0.0135, -0.0012],\n",
      "        ...,\n",
      "        [-0.0315,  0.0094,  0.0275,  ..., -0.0030, -0.0044, -0.0217],\n",
      "        [-0.0289,  0.0042,  0.0141,  ..., -0.0354, -0.0054,  0.0013],\n",
      "        [-0.0150,  0.0339,  0.0250,  ...,  0.0679,  0.0148, -0.0339]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 8.5449e-03,  5.8594e-03, -4.0283e-02,  ..., -2.8931e-02,\n",
      "         -5.1575e-03,  1.4038e-02],\n",
      "        [-2.0142e-03, -1.9653e-02,  1.8787e-04,  ...,  4.0894e-03,\n",
      "         -2.5940e-03, -1.0633e-04],\n",
      "        [ 2.5146e-02, -1.8188e-02, -1.3828e-05,  ..., -5.0354e-03,\n",
      "          8.9111e-03,  1.2939e-02],\n",
      "        ...,\n",
      "        [ 1.5442e-02, -1.2756e-02,  1.7212e-02,  ...,  1.9775e-02,\n",
      "         -1.1108e-02,  3.3569e-03],\n",
      "        [-1.5259e-02,  2.6978e-02, -3.3203e-02,  ...,  4.5776e-03,\n",
      "         -7.0190e-03, -3.0273e-02],\n",
      "        [ 7.2327e-03,  2.2705e-02, -2.3315e-02,  ..., -7.0801e-03,\n",
      "         -7.0190e-03,  1.8188e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0109,  0.0061, -0.0222,  ..., -0.0040,  0.0033, -0.0121],\n",
      "        [ 0.0014,  0.0287,  0.0080,  ...,  0.0212, -0.0089,  0.0004],\n",
      "        [ 0.0048,  0.0057, -0.0033,  ...,  0.0020,  0.0221,  0.0104],\n",
      "        ...,\n",
      "        [ 0.0219, -0.0041,  0.0035,  ..., -0.0148, -0.0006,  0.0098],\n",
      "        [-0.0029, -0.0020, -0.0187,  ..., -0.0028,  0.0361,  0.0200],\n",
      "        [-0.0093, -0.0113, -0.0143,  ..., -0.0068,  0.0059,  0.0025]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0149,  0.0092, -0.0096,  ...,  0.0052, -0.0156, -0.0085],\n",
      "        [ 0.0214,  0.0061, -0.0422,  ...,  0.0383,  0.0177, -0.0099],\n",
      "        [ 0.0036,  0.0131,  0.0064,  ..., -0.0120,  0.0042, -0.0109],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0177, -0.0193,  ..., -0.0040, -0.0069, -0.0051],\n",
      "        [-0.0070,  0.0156,  0.0199,  ..., -0.0153,  0.0154,  0.0089],\n",
      "        [ 0.0159, -0.0012,  0.0203,  ..., -0.0073, -0.0283,  0.0238]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0084,  0.0236,  0.0062,  ..., -0.0317, -0.0081, -0.0021],\n",
      "        [ 0.0184,  0.0247, -0.0152,  ...,  0.0057,  0.0006,  0.0157],\n",
      "        [ 0.0013, -0.0437,  0.0210,  ..., -0.0015, -0.0282,  0.0112],\n",
      "        ...,\n",
      "        [-0.0038,  0.0183, -0.0103,  ..., -0.0034, -0.0188, -0.0175],\n",
      "        [ 0.0160,  0.0308,  0.0305,  ..., -0.0056, -0.0147,  0.0327],\n",
      "        [-0.0212,  0.0121, -0.0127,  ..., -0.0304,  0.0103,  0.0179]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 1.1215e-03,  1.0376e-02,  1.9409e-02,  ..., -2.6489e-02,\n",
      "          8.1177e-03,  7.7820e-03],\n",
      "        [ 2.6855e-02,  1.4099e-02, -2.8320e-02,  ...,  1.3916e-02,\n",
      "          2.1362e-02,  1.4526e-02],\n",
      "        [ 1.6093e-06,  3.9368e-03,  3.2227e-02,  ..., -1.6022e-03,\n",
      "          1.9653e-02, -5.4932e-03],\n",
      "        ...,\n",
      "        [-1.4832e-02,  7.7209e-03, -4.7119e-02,  ..., -7.8735e-03,\n",
      "         -5.0659e-03, -4.3640e-03],\n",
      "        [-2.7466e-02,  2.5635e-02, -6.1340e-03,  ..., -1.3611e-02,\n",
      "          2.3438e-02, -1.6235e-02],\n",
      "        [ 5.5237e-03,  2.5757e-02,  1.1597e-02,  ...,  1.1902e-02,\n",
      "         -1.3733e-02,  7.4463e-03]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4160, 0.4258, 0.3750,  ..., 0.4062, 0.3984, 0.3887],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2734, 0.2617, 0.2598,  ..., 0.2773, 0.2734, 0.2695],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0063, -0.0195, -0.0057,  ...,  0.0096, -0.0084,  0.0103],\n",
      "        [ 0.0250, -0.0228,  0.0089,  ..., -0.0014,  0.0045,  0.0058],\n",
      "        [ 0.0061, -0.0075,  0.0029,  ...,  0.0144, -0.0028, -0.0122],\n",
      "        ...,\n",
      "        [-0.0444, -0.0342,  0.0118,  ..., -0.0123, -0.0110,  0.0288],\n",
      "        [-0.0173, -0.0554,  0.0186,  ...,  0.0167, -0.0065,  0.0043],\n",
      "        [ 0.0184,  0.0193,  0.0280,  ...,  0.0007, -0.0197, -0.0150]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0211, -0.0065, -0.0112,  ..., -0.0051,  0.0052,  0.0024],\n",
      "        [ 0.0120, -0.0095, -0.0076,  ...,  0.0019,  0.0048, -0.0150],\n",
      "        [ 0.0048,  0.0055, -0.0051,  ..., -0.0055,  0.0124, -0.0101],\n",
      "        ...,\n",
      "        [-0.0442, -0.0479,  0.0245,  ...,  0.0110,  0.0088, -0.0096],\n",
      "        [ 0.0092,  0.0050,  0.0114,  ...,  0.0286, -0.0119,  0.0156],\n",
      "        [-0.0242, -0.0087,  0.0325,  ..., -0.0014, -0.0211, -0.0017]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0087, -0.0004,  0.0042,  ...,  0.0096,  0.0132,  0.0154],\n",
      "        [-0.0110, -0.0089,  0.0148,  ...,  0.0171,  0.0043,  0.0244],\n",
      "        [ 0.0054, -0.0354, -0.0071,  ..., -0.0320,  0.0059,  0.0036],\n",
      "        ...,\n",
      "        [-0.0048, -0.0034,  0.0136,  ...,  0.0101,  0.0217, -0.0153],\n",
      "        [ 0.0055,  0.0038, -0.0273,  ...,  0.0127,  0.0139, -0.0059],\n",
      "        [ 0.0232,  0.0034,  0.0193,  ..., -0.0220,  0.0023,  0.0069]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0019,  0.0072, -0.0153,  ...,  0.0143, -0.0089,  0.0239],\n",
      "        [-0.0056, -0.0047, -0.0244,  ...,  0.0137, -0.0325,  0.0181],\n",
      "        [ 0.0059, -0.0078, -0.0042,  ...,  0.0038, -0.0029,  0.0067],\n",
      "        ...,\n",
      "        [-0.0091, -0.0236,  0.0134,  ..., -0.0073, -0.0250, -0.0098],\n",
      "        [-0.0035, -0.0060, -0.0029,  ...,  0.0129,  0.0038, -0.0030],\n",
      "        [-0.0248,  0.0051,  0.0251,  ..., -0.0057, -0.0342, -0.0270]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0093,  0.0287, -0.0140,  ...,  0.0094, -0.0234,  0.0278],\n",
      "        [ 0.0087, -0.0085,  0.0342,  ..., -0.0126,  0.0096,  0.0113],\n",
      "        [ 0.0065, -0.0120, -0.0104,  ...,  0.0276, -0.0503, -0.0176],\n",
      "        ...,\n",
      "        [-0.0094,  0.0026,  0.0330,  ..., -0.0081,  0.0107,  0.0096],\n",
      "        [-0.0077, -0.0165,  0.0079,  ..., -0.0089, -0.0140, -0.0371],\n",
      "        [ 0.0004, -0.0151,  0.0003,  ..., -0.0005, -0.0009,  0.0065]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0093, -0.0156, -0.0283,  ...,  0.0238,  0.0108, -0.0118],\n",
      "        [ 0.0051, -0.0148, -0.0250,  ..., -0.0118, -0.0219,  0.0187],\n",
      "        [ 0.0425,  0.0496, -0.0245,  ..., -0.0103,  0.0349,  0.0172],\n",
      "        ...,\n",
      "        [-0.0131, -0.0437,  0.0111,  ...,  0.0066, -0.0033,  0.0029],\n",
      "        [-0.0361, -0.0173, -0.0016,  ..., -0.0283, -0.0164,  0.0051],\n",
      "        [ 0.0043,  0.0302, -0.0103,  ..., -0.0130, -0.0275, -0.0232]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0128, -0.0123,  0.0227,  ...,  0.0204,  0.0171, -0.0476],\n",
      "        [-0.0156,  0.0349,  0.0298,  ...,  0.0045,  0.0055,  0.0036],\n",
      "        [-0.0417, -0.0006, -0.0083,  ..., -0.0042,  0.0164, -0.0093],\n",
      "        ...,\n",
      "        [-0.0115, -0.0199, -0.0087,  ...,  0.0093,  0.0024, -0.0063],\n",
      "        [ 0.0376, -0.0065,  0.0137,  ..., -0.0078,  0.0181, -0.0004],\n",
      "        [-0.0060, -0.0220, -0.0217,  ...,  0.0051, -0.0182, -0.0019]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4062, 0.4004, 0.3770,  ..., 0.3848, 0.3848, 0.3887],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.2852, 0.2715, 0.2715,  ..., 0.2852, 0.2812, 0.2812],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0108,  0.0079, -0.0220,  ...,  0.0070,  0.0250, -0.0064],\n",
      "        [ 0.0194, -0.0425,  0.0175,  ...,  0.0147,  0.0067, -0.0205],\n",
      "        [-0.0112, -0.0018,  0.0055,  ..., -0.0232, -0.0062, -0.0242],\n",
      "        ...,\n",
      "        [ 0.0229, -0.0308, -0.0017,  ..., -0.0210,  0.0238, -0.0188],\n",
      "        [-0.0146, -0.0098,  0.0262,  ...,  0.0025, -0.0203,  0.0130],\n",
      "        [ 0.0067,  0.0157,  0.0179,  ..., -0.0051, -0.0295, -0.0123]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-1.7822e-02,  1.9653e-02,  7.7515e-03,  ...,  6.5002e-03,\n",
      "          1.7700e-02,  2.3438e-02],\n",
      "        [ 2.2583e-02, -3.4912e-02,  9.0942e-03,  ..., -2.6822e-05,\n",
      "         -2.7344e-02, -2.0905e-03],\n",
      "        [ 2.8687e-02, -3.3417e-03, -1.3855e-02,  ..., -2.1484e-02,\n",
      "          2.8229e-03, -3.5889e-02],\n",
      "        ...,\n",
      "        [ 1.6235e-02,  1.2085e-02, -1.3611e-02,  ..., -2.2583e-02,\n",
      "         -2.4658e-02, -5.4932e-02],\n",
      "        [ 1.7700e-02,  1.0193e-02,  1.2146e-02,  ...,  2.6611e-02,\n",
      "          1.1902e-02,  1.3000e-02],\n",
      "        [-3.7354e-02,  5.3223e-02,  6.2256e-02,  ..., -1.4038e-03,\n",
      "         -1.5869e-02,  3.1250e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0095,  0.0023, -0.0059,  ...,  0.0078, -0.0015, -0.0060],\n",
      "        [-0.0562, -0.0061, -0.0056,  ...,  0.0058,  0.0154, -0.0281],\n",
      "        [-0.0033, -0.0048, -0.0063,  ..., -0.0254,  0.0439,  0.0150],\n",
      "        ...,\n",
      "        [ 0.0085, -0.0014,  0.0141,  ..., -0.0139,  0.0055,  0.0173],\n",
      "        [-0.0208,  0.0002,  0.0045,  ...,  0.0354, -0.0031,  0.0123],\n",
      "        [ 0.0130,  0.0107,  0.0084,  ..., -0.0102,  0.0013, -0.0309]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 4.3457e-02, -1.1292e-03, -1.9165e-02,  ...,  7.4463e-03,\n",
      "         -1.5076e-02,  1.1749e-03],\n",
      "        [-2.5513e-02,  1.1292e-02, -2.3315e-02,  ..., -5.4016e-03,\n",
      "         -1.0729e-06, -1.2665e-03],\n",
      "        [-2.8564e-02, -1.0498e-02, -5.7983e-03,  ...,  2.6398e-03,\n",
      "          2.8076e-02,  1.3245e-02],\n",
      "        ...,\n",
      "        [-6.4087e-03, -1.7456e-02,  6.7749e-03,  ...,  5.7068e-03,\n",
      "         -2.1606e-02,  4.2534e-04],\n",
      "        [-5.1270e-03,  2.3193e-02,  3.1494e-02,  ..., -2.7161e-03,\n",
      "          8.9722e-03, -1.4893e-02],\n",
      "        [-1.8066e-02,  4.7302e-03,  4.1809e-03,  ..., -1.4038e-03,\n",
      "         -3.9978e-03,  6.6757e-04]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0175, -0.0168, -0.0245,  ...,  0.0050, -0.0048,  0.0120],\n",
      "        [ 0.0317,  0.0182, -0.0081,  ...,  0.0126,  0.0067,  0.0190],\n",
      "        [-0.0075,  0.0168, -0.0031,  ...,  0.0078, -0.0020, -0.0259],\n",
      "        ...,\n",
      "        [-0.0038,  0.0199,  0.0013,  ...,  0.0232,  0.0234,  0.0060],\n",
      "        [ 0.0023, -0.0073,  0.0104,  ...,  0.0190,  0.0182,  0.0215],\n",
      "        [ 0.0344,  0.0215,  0.0364,  ..., -0.0364,  0.0233, -0.0104]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0200, -0.0022,  0.0181,  ...,  0.0291, -0.0046,  0.0026],\n",
      "        [-0.0231, -0.0025, -0.0116,  ...,  0.0278,  0.0020, -0.0006],\n",
      "        [-0.0161,  0.0045, -0.0120,  ...,  0.0228, -0.0064, -0.0060],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0225, -0.0248,  ...,  0.0097,  0.0156,  0.0016],\n",
      "        [ 0.0147,  0.0057, -0.0103,  ..., -0.0154, -0.0126, -0.0071],\n",
      "        [ 0.0117, -0.0151, -0.0251,  ...,  0.0261,  0.0031, -0.0143]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0374,  0.0118,  0.0055,  ...,  0.0177, -0.0170,  0.0046],\n",
      "        [ 0.0126, -0.0154,  0.0091,  ...,  0.0168, -0.0240, -0.0076],\n",
      "        [-0.0103, -0.0216, -0.0075,  ..., -0.0115,  0.0082,  0.0255],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0247, -0.0073,  ...,  0.0007, -0.0102, -0.0171],\n",
      "        [-0.0124,  0.0233, -0.0195,  ...,  0.0102, -0.0146,  0.0131],\n",
      "        [ 0.0034,  0.0042, -0.0022,  ...,  0.0010, -0.0240, -0.0236]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4102, 0.4160, 0.3867,  ..., 0.3867, 0.4023, 0.4004],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3027, 0.2891, 0.2910,  ..., 0.3027, 0.3066, 0.2969],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0165, -0.0136,  0.0143,  ..., -0.0067, -0.0105, -0.0038],\n",
      "        [ 0.0114, -0.0089,  0.0195,  ..., -0.0004, -0.0209,  0.0013],\n",
      "        [-0.0137, -0.0018, -0.0114,  ...,  0.0179, -0.0093, -0.0012],\n",
      "        ...,\n",
      "        [ 0.0376, -0.0283,  0.0559,  ..., -0.0143, -0.0177,  0.0105],\n",
      "        [ 0.0223, -0.0226,  0.0459,  ...,  0.0121, -0.0164, -0.0601],\n",
      "        [ 0.0189, -0.0096,  0.0142,  ...,  0.0288,  0.0029,  0.0579]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0148,  0.0040,  0.0059,  ..., -0.0050,  0.0142,  0.0076],\n",
      "        [ 0.0059,  0.0067,  0.0073,  ...,  0.0112, -0.0017, -0.0085],\n",
      "        [ 0.0027, -0.0106,  0.0031,  ...,  0.0188, -0.0097, -0.0111],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0615, -0.0119,  ..., -0.0150,  0.0525, -0.0679],\n",
      "        [ 0.0254,  0.0217,  0.0138,  ..., -0.0154, -0.0219, -0.0199],\n",
      "        [-0.0400, -0.0454, -0.0031,  ...,  0.0320,  0.0234, -0.0060]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0143,  0.0087,  0.0063,  ...,  0.0236, -0.0082, -0.0036],\n",
      "        [-0.0024, -0.0125, -0.0090,  ...,  0.0087, -0.0247, -0.0172],\n",
      "        [ 0.0130,  0.0090, -0.0019,  ...,  0.0056, -0.0085, -0.0223],\n",
      "        ...,\n",
      "        [ 0.0093, -0.0044,  0.0146,  ...,  0.0090,  0.0052, -0.0154],\n",
      "        [ 0.0173,  0.0033, -0.0403,  ..., -0.0226,  0.0018, -0.0315],\n",
      "        [-0.0087, -0.0079, -0.0013,  ...,  0.0090,  0.0107,  0.0079]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0043, -0.0055,  0.0069,  ..., -0.0193,  0.0032, -0.0109],\n",
      "        [-0.0146, -0.0232,  0.0240,  ...,  0.0265, -0.0069,  0.0203],\n",
      "        [ 0.0118, -0.0135, -0.0056,  ..., -0.0153, -0.0030, -0.0168],\n",
      "        ...,\n",
      "        [-0.0084,  0.0123, -0.0182,  ..., -0.0280, -0.0299, -0.0193],\n",
      "        [-0.0099,  0.0115, -0.0057,  ..., -0.0215, -0.0024,  0.0092],\n",
      "        [-0.0186, -0.0095,  0.0127,  ...,  0.0317, -0.0183,  0.0143]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0347,  0.0232,  0.0157,  ...,  0.0073, -0.0071,  0.0043],\n",
      "        [-0.0038,  0.0317,  0.0167,  ...,  0.0164, -0.0021,  0.0078],\n",
      "        [ 0.0134,  0.0122, -0.0233,  ...,  0.0057,  0.0220, -0.0052],\n",
      "        ...,\n",
      "        [-0.0125, -0.0117, -0.0162,  ..., -0.0065, -0.0137, -0.0062],\n",
      "        [ 0.0052, -0.0017,  0.0003,  ...,  0.0040,  0.0388, -0.0025],\n",
      "        [-0.0014,  0.0194,  0.0147,  ..., -0.0144,  0.0093, -0.0041]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 2.2217e-02, -2.1606e-02,  6.8665e-03,  ...,  5.4016e-03,\n",
      "         -7.2021e-03, -5.8899e-03],\n",
      "        [ 2.3315e-02,  2.3804e-02,  1.5625e-02,  ...,  2.9449e-03,\n",
      "         -1.0864e-02,  2.8809e-02],\n",
      "        [-2.7710e-02, -1.6968e-02, -1.1902e-02,  ...,  1.2146e-02,\n",
      "         -9.7656e-03,  5.1575e-03],\n",
      "        ...,\n",
      "        [ 6.3477e-03, -1.7944e-02, -7.3547e-03,  ..., -1.2146e-02,\n",
      "         -5.9605e-08, -2.8198e-02],\n",
      "        [ 5.7068e-03,  1.5106e-03, -7.5684e-03,  ..., -2.0020e-02,\n",
      "         -2.5024e-02,  2.0630e-02],\n",
      "        [ 2.5177e-03, -2.1118e-02,  1.7578e-02,  ..., -3.0518e-02,\n",
      "         -8.6060e-03,  3.4485e-03]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0072, -0.0255, -0.0077,  ...,  0.0077,  0.0146, -0.0140],\n",
      "        [-0.0211,  0.0170,  0.0176,  ..., -0.0043, -0.0018,  0.0064],\n",
      "        [ 0.0004, -0.0023, -0.0049,  ...,  0.0118, -0.0227,  0.0110],\n",
      "        ...,\n",
      "        [ 0.0117,  0.0157,  0.0062,  ...,  0.0074, -0.0133, -0.0120],\n",
      "        [-0.0084, -0.0187, -0.0120,  ..., -0.0052, -0.0203,  0.0084],\n",
      "        [ 0.0032,  0.0061, -0.0025,  ...,  0.0040, -0.0366, -0.0170]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4238, 0.4277, 0.4004,  ..., 0.4199, 0.4219, 0.4043],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3203, 0.3125, 0.3105,  ..., 0.3223, 0.3242, 0.3145],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0089,  0.0049,  0.0028,  ..., -0.0111, -0.0413,  0.0051],\n",
      "        [ 0.0022, -0.0073,  0.0120,  ..., -0.0056,  0.0177,  0.0415],\n",
      "        [ 0.0036, -0.0190, -0.0162,  ...,  0.0002, -0.0085,  0.0102],\n",
      "        ...,\n",
      "        [-0.0310,  0.0128, -0.0073,  ...,  0.0278,  0.0056,  0.0176],\n",
      "        [ 0.0649, -0.0167, -0.0271,  ...,  0.0562,  0.0317,  0.0107],\n",
      "        [ 0.0260,  0.0195, -0.0006,  ...,  0.0211,  0.0449, -0.0476]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0126,  0.0264, -0.0093,  ...,  0.0160, -0.0160, -0.0234],\n",
      "        [ 0.0078, -0.0172,  0.0096,  ...,  0.0189,  0.0238,  0.0003],\n",
      "        [ 0.0106, -0.0015,  0.0042,  ...,  0.0347, -0.0160,  0.0254],\n",
      "        ...,\n",
      "        [-0.1099, -0.0198, -0.0669,  ...,  0.0242, -0.0021, -0.0439],\n",
      "        [ 0.0461,  0.0400, -0.0106,  ...,  0.0393,  0.0669, -0.0344],\n",
      "        [-0.0452, -0.0435, -0.0452,  ...,  0.0309, -0.0144, -0.0292]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0266,  0.0175, -0.0197,  ...,  0.0013,  0.0300,  0.0085],\n",
      "        [ 0.0115,  0.0041, -0.0059,  ..., -0.0211,  0.0386,  0.0021],\n",
      "        [-0.0024,  0.0115, -0.0053,  ...,  0.0082, -0.0016, -0.0104],\n",
      "        ...,\n",
      "        [ 0.0135,  0.0145, -0.0042,  ..., -0.0040, -0.0195, -0.0055],\n",
      "        [ 0.0029, -0.0040,  0.0017,  ...,  0.0131,  0.0098,  0.0317],\n",
      "        [-0.0015,  0.0074,  0.0070,  ..., -0.0058, -0.0106, -0.0004]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0168,  0.0122, -0.0137,  ...,  0.0096,  0.0278,  0.0172],\n",
      "        [-0.0039, -0.0361,  0.0087,  ...,  0.0033, -0.0072, -0.0280],\n",
      "        [-0.0040, -0.0165,  0.0152,  ...,  0.0258,  0.0179,  0.0304],\n",
      "        ...,\n",
      "        [-0.0352, -0.0154, -0.0240,  ...,  0.0177,  0.0137,  0.0028],\n",
      "        [ 0.0211,  0.0535,  0.0143,  ...,  0.0099, -0.0076, -0.0339],\n",
      "        [-0.0160, -0.0466, -0.0192,  ..., -0.0121,  0.0096, -0.0160]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 1.2146e-02, -6.0120e-03,  1.2329e-02,  ..., -2.2125e-03,\n",
      "          1.5869e-02,  6.5994e-04],\n",
      "        [-3.9062e-02,  7.7820e-03, -1.6602e-02,  ..., -6.1646e-03,\n",
      "         -1.4160e-02, -8.4400e-05],\n",
      "        [-2.0386e-02,  1.0742e-02,  3.7354e-02,  ..., -2.3926e-02,\n",
      "         -4.7607e-03,  3.8147e-03],\n",
      "        ...,\n",
      "        [-3.8330e-02, -2.9175e-02, -3.1250e-02,  ..., -2.8809e-02,\n",
      "         -1.6968e-02, -2.2339e-02],\n",
      "        [ 4.6997e-03,  2.0874e-02,  1.7700e-02,  ..., -1.2634e-02,\n",
      "         -1.3809e-03,  8.8501e-03],\n",
      "        [ 2.2736e-03, -4.3640e-03,  7.3547e-03,  ...,  7.6904e-03,\n",
      "         -4.4861e-03,  1.4954e-03]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0242,  0.0091, -0.0304,  ...,  0.0388,  0.0132,  0.0181],\n",
      "        [ 0.0194,  0.0078,  0.0228,  ..., -0.0047, -0.0013, -0.0035],\n",
      "        [ 0.0066, -0.0204, -0.0334,  ...,  0.0175, -0.0019,  0.0152],\n",
      "        ...,\n",
      "        [-0.0109,  0.0137,  0.0162,  ...,  0.0215,  0.0025, -0.0093],\n",
      "        [-0.0203,  0.0025,  0.0142,  ...,  0.0091,  0.0352,  0.0001],\n",
      "        [-0.0002,  0.0225,  0.0182,  ..., -0.0066,  0.0211,  0.0151]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0105,  0.0103,  0.0315,  ...,  0.0036,  0.0194, -0.0107],\n",
      "        [-0.0140,  0.0287, -0.0322,  ..., -0.0014, -0.0013,  0.0070],\n",
      "        [-0.0242, -0.0115,  0.0090,  ...,  0.0132,  0.0160,  0.0231],\n",
      "        ...,\n",
      "        [-0.0021,  0.0089,  0.0093,  ...,  0.0188, -0.0040,  0.0060],\n",
      "        [-0.0075, -0.0018, -0.0100,  ...,  0.0187,  0.0222,  0.0383],\n",
      "        [ 0.0094,  0.0231,  0.0099,  ..., -0.0147,  0.0164,  0.0151]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4473, 0.4473, 0.4277,  ..., 0.4297, 0.4414, 0.4258],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3398, 0.3301, 0.3281,  ..., 0.3359, 0.3398, 0.3340],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026,  0.0068, -0.0078,  ..., -0.0170, -0.0062,  0.0128],\n",
      "        [-0.0049, -0.0099,  0.0014,  ..., -0.0038,  0.0282, -0.0048],\n",
      "        [ 0.0089,  0.0244,  0.0070,  ...,  0.0094,  0.0179,  0.0021],\n",
      "        ...,\n",
      "        [-0.0254, -0.0187, -0.0038,  ..., -0.0442,  0.0009,  0.0265],\n",
      "        [ 0.0066,  0.0359,  0.0266,  ...,  0.0537,  0.0154, -0.0283],\n",
      "        [-0.0129,  0.0275,  0.0527,  ..., -0.0029, -0.0398, -0.0134]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0184,  0.0120,  0.0015,  ..., -0.0063, -0.0030, -0.0145],\n",
      "        [-0.0190,  0.0215, -0.0190,  ...,  0.0146,  0.0055,  0.0156],\n",
      "        [-0.0208,  0.0070, -0.0182,  ...,  0.0010,  0.0134,  0.0188],\n",
      "        ...,\n",
      "        [ 0.0011,  0.0430, -0.0135,  ..., -0.0178,  0.0226, -0.0302],\n",
      "        [-0.0708, -0.0332, -0.0026,  ...,  0.0212,  0.0128, -0.0129],\n",
      "        [-0.0186,  0.0088, -0.0212,  ..., -0.0226,  0.0140,  0.0359]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 3.5248e-03, -5.3711e-03, -3.8147e-03,  ...,  7.0190e-03,\n",
      "         -3.9673e-03,  7.7209e-03],\n",
      "        [ 2.5330e-03,  2.2827e-02, -1.8978e-04,  ...,  3.7354e-02,\n",
      "         -1.6357e-02, -5.4169e-04],\n",
      "        [-4.6875e-02, -2.7222e-02, -5.2185e-03,  ..., -5.7068e-03,\n",
      "         -7.4768e-03,  1.3000e-02],\n",
      "        ...,\n",
      "        [ 3.6001e-05, -3.8086e-02, -3.4790e-03,  ...,  1.6708e-03,\n",
      "          5.8289e-03, -1.8921e-02],\n",
      "        [ 1.4099e-02, -3.9978e-03, -2.3926e-02,  ..., -9.0942e-03,\n",
      "         -9.3994e-03, -4.3945e-03],\n",
      "        [-1.5488e-03,  2.0996e-02, -1.6174e-03,  ..., -2.5513e-02,\n",
      "         -1.4160e-02,  1.3367e-02]], dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.1851e-02, -5.5908e-02,  2.3804e-02,  ...,  5.2643e-04,\n",
      "          7.2098e-04, -2.4414e-02],\n",
      "        [ 2.5513e-02, -6.5918e-03,  3.4027e-03,  ..., -1.2573e-02,\n",
      "          7.0801e-03, -5.5237e-03],\n",
      "        [-3.6163e-03, -1.3367e-02, -2.4170e-02,  ...,  4.0894e-03,\n",
      "          9.5825e-03, -2.1210e-03],\n",
      "        ...,\n",
      "        [ 1.5198e-02, -5.5847e-03, -5.5847e-03,  ...,  9.8267e-03,\n",
      "         -1.8188e-02,  1.1047e-02],\n",
      "        [ 7.7820e-03, -1.9989e-03, -1.0925e-02,  ..., -1.0254e-02,\n",
      "         -1.7578e-02, -8.8215e-05],\n",
      "        [ 7.6904e-03,  2.0874e-02,  9.5825e-03,  ..., -1.6602e-02,\n",
      "         -1.1902e-02,  1.4282e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0118,  0.0045,  0.0065,  ...,  0.0045,  0.0039, -0.0023],\n",
      "        [ 0.0089,  0.0206,  0.0045,  ...,  0.0056,  0.0145, -0.0347],\n",
      "        [-0.0066, -0.0042, -0.0189,  ..., -0.0386, -0.0074,  0.0167],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0023,  0.0020,  ..., -0.0225, -0.0047,  0.0022],\n",
      "        [-0.0498,  0.0011, -0.0031,  ..., -0.0056, -0.0212,  0.0162],\n",
      "        [-0.0245, -0.0242,  0.0138,  ..., -0.0012,  0.0071,  0.0011]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0054, -0.0186,  0.0063,  ...,  0.0043, -0.0165,  0.0170],\n",
      "        [-0.0091, -0.0415, -0.0259,  ...,  0.0098,  0.0092, -0.0090],\n",
      "        [-0.0214, -0.0190,  0.0242,  ...,  0.0046, -0.0074,  0.0069],\n",
      "        ...,\n",
      "        [-0.0087, -0.0026,  0.0186,  ...,  0.0142,  0.0172, -0.0013],\n",
      "        [ 0.0057, -0.0148, -0.0427,  ...,  0.0271, -0.0083, -0.0152],\n",
      "        [ 0.0084, -0.0053, -0.0282,  ...,  0.0014,  0.0079,  0.0070]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0006, -0.0093,  0.0062,  ...,  0.0041,  0.0105,  0.0070],\n",
      "        [-0.0087, -0.0007, -0.0101,  ...,  0.0231, -0.0209, -0.0125],\n",
      "        [ 0.0156,  0.0067,  0.0183,  ..., -0.0162, -0.0117,  0.0500],\n",
      "        ...,\n",
      "        [ 0.0113,  0.0153, -0.0063,  ...,  0.0205, -0.0149,  0.0361],\n",
      "        [ 0.0065,  0.0251,  0.0349,  ..., -0.0160,  0.0325, -0.0137],\n",
      "        [ 0.0039, -0.0184, -0.0135,  ..., -0.0038,  0.0007,  0.0097]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4512, 0.4570, 0.4375,  ..., 0.4258, 0.4336, 0.4375],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3535, 0.3398, 0.3418,  ..., 0.3496, 0.3496, 0.3477],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0013, -0.0057,  0.0166,  ...,  0.0120,  0.0022, -0.0220],\n",
      "        [ 0.0060, -0.0201,  0.0096,  ..., -0.0084, -0.0090,  0.0098],\n",
      "        [ 0.0035, -0.0061, -0.0084,  ...,  0.0157, -0.0017, -0.0048],\n",
      "        ...,\n",
      "        [-0.0222, -0.0430,  0.0110,  ..., -0.0036,  0.0203,  0.0048],\n",
      "        [-0.0703, -0.0173,  0.0114,  ...,  0.0081,  0.0192,  0.0074],\n",
      "        [ 0.0179, -0.0014, -0.0059,  ..., -0.0151, -0.0094, -0.0021]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-3.2349e-03,  4.8218e-03,  8.1177e-03,  ..., -2.1820e-03,\n",
      "          2.3193e-03,  4.4861e-03],\n",
      "        [ 2.8839e-03,  8.9722e-03,  3.5858e-03,  ...,  8.7738e-05,\n",
      "         -1.8692e-03,  1.1108e-02],\n",
      "        [ 4.0894e-03,  5.6152e-03, -6.1646e-03,  ..., -3.8910e-03,\n",
      "          4.3030e-03,  1.9409e-02],\n",
      "        ...,\n",
      "        [-2.2221e-04, -3.0975e-03, -2.2949e-02,  ..., -1.9653e-02,\n",
      "          2.3804e-02,  2.6855e-02],\n",
      "        [-1.7456e-02,  1.9531e-02, -2.4414e-03,  ...,  2.0874e-02,\n",
      "          2.1118e-02,  5.9570e-02],\n",
      "        [-1.6357e-02, -2.6855e-02, -1.1597e-02,  ...,  2.5757e-02,\n",
      "         -2.6611e-02,  4.7607e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0041, -0.0126,  0.0147,  ..., -0.0042,  0.0112,  0.0117],\n",
      "        [-0.0070, -0.0413, -0.0003,  ..., -0.0106, -0.0010,  0.0099],\n",
      "        [-0.0078, -0.0134,  0.0005,  ..., -0.0186,  0.0003, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0066, -0.0083, -0.0044,  ...,  0.0203, -0.0052,  0.0068],\n",
      "        [ 0.0173,  0.0101, -0.0096,  ...,  0.0023,  0.0131, -0.0084],\n",
      "        [ 0.0025, -0.0056,  0.0258,  ..., -0.0067, -0.0107, -0.0403]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0066,  0.0041,  0.0305,  ..., -0.0042,  0.0164, -0.0100],\n",
      "        [ 0.0102, -0.0131, -0.0087,  ...,  0.0014, -0.0116,  0.0215],\n",
      "        [ 0.0128, -0.0036, -0.0118,  ..., -0.0269,  0.0015,  0.0092],\n",
      "        ...,\n",
      "        [-0.0120,  0.0014,  0.0056,  ...,  0.0137, -0.0151,  0.0108],\n",
      "        [ 0.0033,  0.0149, -0.0145,  ...,  0.0121, -0.0256,  0.0024],\n",
      "        [ 0.0178, -0.0248, -0.0046,  ...,  0.0143, -0.0215,  0.0116]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0014, -0.0047, -0.0222,  ...,  0.0206, -0.0102,  0.0140],\n",
      "        [-0.0186,  0.0049, -0.0103,  ..., -0.0047, -0.0106,  0.0188],\n",
      "        [-0.0110, -0.0080,  0.0586,  ..., -0.0209,  0.0211, -0.0026],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0063,  0.0250,  ..., -0.0217,  0.0067, -0.0029],\n",
      "        [ 0.0123,  0.0062,  0.0104,  ...,  0.0201, -0.0026,  0.0135],\n",
      "        [-0.0071, -0.0245, -0.0148,  ...,  0.0012, -0.0281, -0.0055]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0026,  0.0260, -0.0023,  ..., -0.0089, -0.0112, -0.0303],\n",
      "        [ 0.0023,  0.0165, -0.0080,  ...,  0.0078, -0.0189,  0.0233],\n",
      "        [-0.0081,  0.0115, -0.0212,  ..., -0.0058,  0.0225, -0.0186],\n",
      "        ...,\n",
      "        [-0.0327,  0.0297,  0.0033,  ...,  0.0108, -0.0019, -0.0008],\n",
      "        [-0.0115,  0.0059, -0.0166,  ..., -0.0099,  0.0171, -0.0109],\n",
      "        [ 0.0120, -0.0032,  0.0025,  ..., -0.0010, -0.0090, -0.0044]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0596, -0.0013,  0.0010,  ...,  0.0130, -0.0123, -0.0101],\n",
      "        [-0.0098,  0.0043, -0.0099,  ..., -0.0444, -0.0171,  0.0366],\n",
      "        [ 0.0129,  0.0342,  0.0165,  ...,  0.0094, -0.0226,  0.0071],\n",
      "        ...,\n",
      "        [-0.0131,  0.0253, -0.0092,  ..., -0.0175, -0.0034,  0.0008],\n",
      "        [ 0.0153, -0.0267, -0.0171,  ...,  0.0027, -0.0244, -0.0359],\n",
      "        [ 0.0137, -0.0229,  0.0284,  ..., -0.0107, -0.0198, -0.0099]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4531, 0.4668, 0.4375,  ..., 0.4336, 0.4336, 0.4473],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3633, 0.3574, 0.3516,  ..., 0.3633, 0.3613, 0.3574],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0181, -0.0054,  0.0166,  ..., -0.0060,  0.0056, -0.0052],\n",
      "        [ 0.0154, -0.0021, -0.0043,  ...,  0.0091,  0.0021,  0.0187],\n",
      "        [-0.0095, -0.0070,  0.0008,  ...,  0.0283, -0.0069,  0.0022],\n",
      "        ...,\n",
      "        [ 0.0024, -0.0082,  0.0295,  ..., -0.0110, -0.0496,  0.0114],\n",
      "        [-0.0126,  0.0168,  0.0197,  ...,  0.0253,  0.0359, -0.0052],\n",
      "        [-0.0747, -0.0110, -0.0092,  ..., -0.0090,  0.0164, -0.0413]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0041,  0.0107,  0.0046,  ...,  0.0119, -0.0004,  0.0045],\n",
      "        [-0.0022,  0.0109,  0.0141,  ...,  0.0078,  0.0065,  0.0147],\n",
      "        [ 0.0093,  0.0031,  0.0312,  ...,  0.0154,  0.0009,  0.0053],\n",
      "        ...,\n",
      "        [-0.0183,  0.0200,  0.0057,  ...,  0.0215, -0.0352,  0.0452],\n",
      "        [ 0.0005,  0.0038, -0.0138,  ..., -0.0168,  0.0284, -0.0089],\n",
      "        [-0.0232,  0.0054, -0.0214,  ...,  0.0004, -0.0201,  0.0623]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0011,  0.0074, -0.0023,  ..., -0.0052,  0.0197, -0.0261],\n",
      "        [-0.0276,  0.0260, -0.0068,  ...,  0.0211, -0.0211,  0.0265],\n",
      "        [ 0.0223,  0.0024, -0.0042,  ..., -0.0225,  0.0195, -0.0077],\n",
      "        ...,\n",
      "        [-0.0056, -0.0248,  0.0337,  ...,  0.0014, -0.0022, -0.0167],\n",
      "        [-0.0182, -0.0118,  0.0134,  ...,  0.0066, -0.0009,  0.0171],\n",
      "        [-0.0091,  0.0187, -0.0131,  ...,  0.0055, -0.0154,  0.0002]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0128, -0.0006,  0.0127,  ..., -0.0088,  0.0266,  0.0012],\n",
      "        [-0.0044,  0.0231, -0.0125,  ..., -0.0302,  0.0093, -0.0074],\n",
      "        [-0.0183, -0.0121,  0.0055,  ...,  0.0266,  0.0309,  0.0156],\n",
      "        ...,\n",
      "        [ 0.0087, -0.0079, -0.0139,  ...,  0.0215,  0.0082,  0.0190],\n",
      "        [ 0.0217, -0.0080,  0.0143,  ..., -0.0083,  0.0100, -0.0184],\n",
      "        [-0.0200,  0.0164, -0.0134,  ...,  0.0322, -0.0190,  0.0045]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0027, -0.0205,  0.0111,  ..., -0.0300,  0.0410,  0.0151],\n",
      "        [ 0.0141,  0.0108,  0.0090,  ..., -0.0359,  0.0496,  0.0058],\n",
      "        [ 0.0145,  0.0069,  0.0024,  ..., -0.0425, -0.0048, -0.0198],\n",
      "        ...,\n",
      "        [ 0.0122, -0.0266, -0.0291,  ...,  0.0344, -0.0254,  0.0005],\n",
      "        [-0.0023,  0.0312,  0.0081,  ...,  0.0096, -0.0219,  0.0052],\n",
      "        [ 0.0015, -0.0108, -0.0025,  ..., -0.0159,  0.0013,  0.0072]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0121,  0.0166, -0.0031,  ...,  0.0172,  0.0194,  0.0071],\n",
      "        [ 0.0168, -0.0045, -0.0144,  ...,  0.0018, -0.0131, -0.0132],\n",
      "        [-0.0083,  0.0009,  0.0117,  ..., -0.0012,  0.0354,  0.0206],\n",
      "        ...,\n",
      "        [-0.0126,  0.0378,  0.0234,  ..., -0.0027,  0.0017,  0.0078],\n",
      "        [ 0.0156, -0.0011, -0.0032,  ..., -0.0496, -0.0132,  0.0092],\n",
      "        [ 0.0070,  0.0038,  0.0228,  ..., -0.0135,  0.0181, -0.0216]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0259, -0.0302, -0.0189,  ...,  0.0215,  0.0398, -0.0119],\n",
      "        [-0.0247, -0.0142,  0.0004,  ...,  0.0356,  0.0435,  0.0046],\n",
      "        [ 0.0160, -0.0049, -0.0011,  ...,  0.0045, -0.0132, -0.0073],\n",
      "        ...,\n",
      "        [-0.0457, -0.0049,  0.0178,  ...,  0.0159, -0.0091, -0.0107],\n",
      "        [-0.0024, -0.0131,  0.0079,  ..., -0.0231, -0.0067,  0.0036],\n",
      "        [-0.0101, -0.0262, -0.0184,  ..., -0.0076, -0.0041, -0.0286]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4785, 0.4863, 0.4688,  ..., 0.4551, 0.4707, 0.4766],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3730, 0.3691, 0.3633,  ..., 0.3789, 0.3711, 0.3730],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0236, -0.0150, -0.0093,  ...,  0.0442, -0.0364,  0.0275],\n",
      "        [-0.0413, -0.0171, -0.0032,  ..., -0.0400,  0.0137, -0.0400],\n",
      "        [-0.0315,  0.0245, -0.0159,  ...,  0.0231,  0.0139, -0.0212],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0160,  0.0110,  ...,  0.0146,  0.0542, -0.0046],\n",
      "        [-0.0277, -0.0037, -0.0259,  ...,  0.0493,  0.0035, -0.0220],\n",
      "        [ 0.0277, -0.0121,  0.0116,  ..., -0.0530,  0.0114,  0.0238]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0193, -0.0015, -0.0067,  ...,  0.0145, -0.0149, -0.0197],\n",
      "        [ 0.0036, -0.0147,  0.0298,  ..., -0.0233,  0.0339, -0.0152],\n",
      "        [-0.0183,  0.0332, -0.0449,  ...,  0.0601, -0.0060, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0219,  0.0078, -0.0272,  ..., -0.0221, -0.0150,  0.0164],\n",
      "        [-0.0128, -0.0461, -0.0068,  ...,  0.0007, -0.0081, -0.0026],\n",
      "        [ 0.0277,  0.0071,  0.0332,  ...,  0.0299, -0.0344,  0.0168]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-1.8799e-02,  5.4626e-03, -2.4872e-03,  ..., -4.6387e-02,\n",
      "         -2.5024e-02,  2.1851e-02],\n",
      "        [-3.1738e-02, -1.3123e-02,  7.8125e-03,  ..., -2.4658e-02,\n",
      "         -3.2715e-02, -7.2937e-03],\n",
      "        [ 2.6733e-02, -2.5024e-02, -8.4839e-03,  ...,  4.6875e-02,\n",
      "          6.7520e-04, -1.5320e-02],\n",
      "        ...,\n",
      "        [-1.0742e-02, -4.5410e-02, -2.5269e-02,  ...,  1.4343e-02,\n",
      "         -6.3477e-03,  1.4221e-02],\n",
      "        [-3.6865e-02, -1.2695e-02,  2.3315e-02,  ..., -1.6968e-02,\n",
      "          8.4229e-03, -1.0498e-02],\n",
      "        [-2.9602e-03,  2.0752e-02, -3.9307e-02,  ...,  6.7444e-03,\n",
      "          3.4094e-05,  2.4170e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0237,  0.0162, -0.0214,  ..., -0.0029, -0.0264,  0.0089],\n",
      "        [ 0.0334,  0.0133,  0.0033,  ...,  0.0050, -0.0131, -0.0189],\n",
      "        [ 0.0137,  0.0061, -0.0092,  ..., -0.0275,  0.0062, -0.0029],\n",
      "        ...,\n",
      "        [-0.0027, -0.0121, -0.0102,  ..., -0.0161, -0.0183,  0.0039],\n",
      "        [ 0.0209, -0.0097, -0.0203,  ..., -0.0192, -0.0085,  0.0118],\n",
      "        [ 0.0049, -0.0237,  0.0258,  ...,  0.0101,  0.0162,  0.0146]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0173, -0.0155,  0.0254,  ...,  0.0137,  0.0227, -0.0349],\n",
      "        [ 0.0096, -0.0035,  0.0022,  ...,  0.0013,  0.0040,  0.0151],\n",
      "        [ 0.0050, -0.0128,  0.0066,  ...,  0.0121,  0.0081, -0.0070],\n",
      "        ...,\n",
      "        [-0.0030, -0.0011,  0.0026,  ..., -0.0131,  0.0050, -0.0036],\n",
      "        [ 0.0006,  0.0256, -0.0259,  ..., -0.0029,  0.0140, -0.0237],\n",
      "        [-0.0079,  0.0043,  0.0018,  ..., -0.0128,  0.0212,  0.0170]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0121, -0.0103,  0.0140,  ..., -0.0135,  0.0057, -0.0420],\n",
      "        [ 0.0037,  0.0197, -0.0092,  ..., -0.0103,  0.0118, -0.0264],\n",
      "        [-0.0064, -0.0072,  0.0069,  ...,  0.0192,  0.0129,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0068, -0.0452,  ..., -0.0053, -0.0150, -0.0201],\n",
      "        [-0.0135, -0.0471,  0.0123,  ...,  0.0198, -0.0262,  0.0334],\n",
      "        [ 0.0118,  0.0003, -0.0347,  ..., -0.0026, -0.0189,  0.0075]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0315,  0.0210,  0.0026,  ...,  0.0108,  0.0322, -0.0037],\n",
      "        [ 0.0104,  0.0082,  0.0188,  ..., -0.0129, -0.0056,  0.0009],\n",
      "        [ 0.0070, -0.0317, -0.0417,  ...,  0.0261,  0.0096,  0.0057],\n",
      "        ...,\n",
      "        [-0.0104,  0.0008, -0.0192,  ..., -0.0031,  0.0075,  0.0046],\n",
      "        [ 0.0165, -0.0130, -0.0203,  ..., -0.0325,  0.0092, -0.0186],\n",
      "        [-0.0022,  0.0008, -0.0041,  ..., -0.0425, -0.0581,  0.0146]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4863, 0.4863, 0.4746,  ..., 0.4688, 0.4863, 0.4863],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.3848, 0.3809, 0.3828,  ..., 0.3926, 0.3867, 0.3887],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 3.8452e-03, -1.5015e-02,  8.4305e-04,  ..., -1.5625e-02,\n",
      "         -6.1035e-04,  2.2411e-05],\n",
      "        [ 1.0071e-02, -2.9907e-03,  1.0803e-02,  ..., -1.7212e-02,\n",
      "         -2.6398e-03,  7.8125e-03],\n",
      "        [-5.4932e-03,  3.0518e-03,  1.8921e-02,  ..., -1.2817e-03,\n",
      "         -1.4893e-02,  6.4087e-03],\n",
      "        ...,\n",
      "        [-2.0874e-02,  6.2500e-02,  1.0742e-02,  ..., -3.7842e-02,\n",
      "         -6.1035e-02, -2.8809e-02],\n",
      "        [-3.6865e-02, -3.2471e-02,  2.2705e-02,  ..., -1.6846e-02,\n",
      "         -2.1667e-03,  1.0254e-02],\n",
      "        [ 1.0376e-02, -5.8838e-02, -1.3351e-03,  ...,  1.0193e-02,\n",
      "          4.0283e-02, -9.0942e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0019,  0.0060,  0.0042,  ...,  0.0011, -0.0176,  0.0024],\n",
      "        [-0.0159, -0.0014, -0.0143,  ...,  0.0171,  0.0151, -0.0026],\n",
      "        [ 0.0094,  0.0040, -0.0129,  ...,  0.0170,  0.0135, -0.0083],\n",
      "        ...,\n",
      "        [ 0.0059,  0.0295,  0.0537,  ..., -0.0308, -0.0149, -0.0183],\n",
      "        [ 0.0007, -0.0315, -0.0118,  ..., -0.0212, -0.0222,  0.0137],\n",
      "        [-0.0145, -0.0076, -0.0221,  ...,  0.0060,  0.0021, -0.0139]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0038,  0.0334, -0.0049,  ..., -0.0091, -0.0080, -0.0161],\n",
      "        [-0.0002, -0.0126, -0.0200,  ...,  0.0045, -0.0125,  0.0036],\n",
      "        [ 0.0096, -0.0162, -0.0265,  ...,  0.0126, -0.0476, -0.0288],\n",
      "        ...,\n",
      "        [ 0.0097,  0.0065,  0.0050,  ..., -0.0029,  0.0275,  0.0113],\n",
      "        [ 0.0198,  0.0273,  0.0070,  ..., -0.0009, -0.0079, -0.0143],\n",
      "        [-0.0179, -0.0132, -0.0566,  ...,  0.0097,  0.0081, -0.0317]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0066, -0.0047, -0.0164,  ..., -0.0171,  0.0137, -0.0072],\n",
      "        [-0.0102, -0.0179, -0.0043,  ..., -0.0029,  0.0126, -0.0031],\n",
      "        [ 0.0175,  0.0047,  0.0309,  ..., -0.0140,  0.0050, -0.0217],\n",
      "        ...,\n",
      "        [ 0.0334, -0.0056,  0.0376,  ..., -0.0193,  0.0127,  0.0033],\n",
      "        [-0.0070,  0.0220, -0.0142,  ..., -0.0058, -0.0140, -0.0063],\n",
      "        [ 0.0094, -0.0052, -0.0118,  ..., -0.0058, -0.0001, -0.0286]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0103,  0.0278, -0.0004,  ..., -0.0008,  0.0364,  0.0110],\n",
      "        [-0.0084,  0.0474, -0.0121,  ...,  0.0049, -0.0208,  0.0181],\n",
      "        [-0.0337,  0.0210,  0.0033,  ...,  0.0183, -0.0593,  0.0198],\n",
      "        ...,\n",
      "        [-0.0068,  0.0099,  0.0004,  ..., -0.0098,  0.0104,  0.0020],\n",
      "        [-0.0248,  0.0325, -0.0023,  ...,  0.0013,  0.0140, -0.0271],\n",
      "        [ 0.0023,  0.0097,  0.0128,  ..., -0.0071,  0.0042, -0.0092]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-1.9264e-04,  8.9722e-03, -1.7456e-02,  ..., -3.3936e-02,\n",
      "         -1.4648e-02,  2.8687e-02],\n",
      "        [ 6.7749e-03,  1.9897e-02, -6.1035e-03,  ...,  2.6611e-02,\n",
      "          2.6611e-02,  4.6492e-05],\n",
      "        [-4.8523e-03, -1.3245e-02, -2.0264e-02,  ...,  6.7444e-03,\n",
      "          3.5645e-02, -1.2573e-02],\n",
      "        ...,\n",
      "        [ 5.4626e-03, -5.2185e-03,  1.0437e-02,  ...,  1.7822e-02,\n",
      "         -8.6594e-04, -1.6479e-02],\n",
      "        [ 2.5757e-02, -1.0071e-02, -1.3638e-04,  ..., -5.2490e-03,\n",
      "          4.5776e-03, -8.2397e-04],\n",
      "        [-2.5482e-03, -3.0273e-02, -1.2573e-02,  ..., -8.6060e-03,\n",
      "          1.2939e-02, -8.3008e-03]], dtype=torch.float16, requires_grad=True) 45088768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0309,  0.0145,  0.0325,  ...,  0.0029, -0.0292, -0.0084],\n",
      "        [ 0.0014,  0.0029, -0.0157,  ..., -0.0046, -0.0159,  0.0106],\n",
      "        [-0.0161,  0.0053, -0.0242,  ...,  0.0156,  0.0115, -0.0055],\n",
      "        ...,\n",
      "        [-0.0103, -0.0142,  0.0177,  ...,  0.0085,  0.0422,  0.0057],\n",
      "        [-0.0006,  0.0112, -0.0388,  ...,  0.0181,  0.0049,  0.0142],\n",
      "        [ 0.0074,  0.0131, -0.0040,  ..., -0.0010, -0.0194,  0.0194]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5117, 0.5234, 0.5078,  ..., 0.5039, 0.5195, 0.5234],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4004, 0.3926, 0.3945,  ..., 0.3984, 0.4004, 0.4023],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0209, -0.0162,  0.0126,  ...,  0.0403, -0.0186,  0.0014],\n",
      "        [ 0.0245,  0.0129,  0.0054,  ..., -0.0110,  0.0164, -0.0078],\n",
      "        [-0.0041, -0.0082, -0.0214,  ...,  0.0088, -0.0104,  0.0170],\n",
      "        ...,\n",
      "        [-0.0056,  0.0010, -0.0205,  ...,  0.0123, -0.0006, -0.0054],\n",
      "        [-0.0203,  0.0104,  0.0067,  ...,  0.0203, -0.0371,  0.0038],\n",
      "        [-0.0229, -0.0361, -0.0197,  ..., -0.0104,  0.0270,  0.0674]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0229,  0.0013, -0.0093,  ...,  0.0121, -0.0054,  0.0022],\n",
      "        [ 0.0130, -0.0182,  0.0087,  ..., -0.0238,  0.0227,  0.0087],\n",
      "        [ 0.0092,  0.0041, -0.0442,  ...,  0.0177,  0.0160, -0.0148],\n",
      "        ...,\n",
      "        [ 0.0295,  0.0065,  0.0403,  ...,  0.0122,  0.0209, -0.0435],\n",
      "        [-0.0046,  0.0215,  0.0181,  ...,  0.0344, -0.0157,  0.0010],\n",
      "        [-0.0231, -0.0188, -0.0061,  ...,  0.0029, -0.0069,  0.0591]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0271, -0.0201, -0.0048,  ..., -0.0022, -0.0016, -0.0023],\n",
      "        [ 0.0315, -0.0227, -0.0099,  ..., -0.0645, -0.0427, -0.0194],\n",
      "        [-0.0522, -0.0167, -0.0164,  ..., -0.0082, -0.0131,  0.0056],\n",
      "        ...,\n",
      "        [-0.0011, -0.0179,  0.0154,  ...,  0.0248, -0.0032,  0.0275],\n",
      "        [-0.0024, -0.0122, -0.0154,  ...,  0.0311, -0.0053,  0.0095],\n",
      "        [ 0.0142,  0.0192,  0.0229,  ...,  0.0234, -0.0157, -0.0087]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0115,  0.0172,  0.0261,  ...,  0.0067,  0.0014, -0.0165],\n",
      "        [-0.0102,  0.0286,  0.0135,  ...,  0.0189,  0.0273, -0.0264],\n",
      "        [-0.0085, -0.0019,  0.0107,  ..., -0.0234,  0.0022, -0.0068],\n",
      "        ...,\n",
      "        [-0.0020,  0.0151,  0.0154,  ..., -0.0075, -0.0080,  0.0019],\n",
      "        [ 0.0505,  0.0058, -0.0052,  ...,  0.0039, -0.0205, -0.0041],\n",
      "        [ 0.0039,  0.0143,  0.0101,  ...,  0.0057, -0.0206,  0.0148]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-2.9663e-02, -1.2939e-02,  1.3000e-02,  ..., -1.0498e-02,\n",
      "         -3.1738e-03, -1.1902e-03],\n",
      "        [-4.0054e-04, -1.3489e-02, -2.7100e-02,  ...,  8.6670e-03,\n",
      "          2.3079e-04, -1.1780e-02],\n",
      "        [-5.9891e-04, -3.3691e-02,  2.5391e-02,  ..., -6.4392e-03,\n",
      "         -4.7119e-02, -1.6846e-02],\n",
      "        ...,\n",
      "        [-5.9891e-04, -3.0151e-02,  4.4556e-03,  ..., -3.9062e-03,\n",
      "          1.1780e-02, -7.1106e-03],\n",
      "        [ 1.6602e-02,  1.7090e-02,  3.3722e-03,  ...,  4.4861e-03,\n",
      "         -4.4441e-04, -3.4332e-03],\n",
      "        [-3.4668e-02,  3.1250e-02, -4.1127e-06,  ...,  3.2471e-02,\n",
      "         -1.4954e-02, -4.4678e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0057, -0.0125, -0.0320,  ..., -0.0003,  0.0039, -0.0205],\n",
      "        [ 0.0270, -0.0087,  0.0153,  ..., -0.0034,  0.0103, -0.0486],\n",
      "        [ 0.0214,  0.0172, -0.0320,  ...,  0.0010,  0.0200,  0.0058],\n",
      "        ...,\n",
      "        [-0.0090,  0.0077, -0.0192,  ..., -0.0344, -0.0126, -0.0220],\n",
      "        [ 0.0101,  0.0253,  0.0059,  ...,  0.0228, -0.0248,  0.0181],\n",
      "        [-0.0203, -0.0069,  0.0055,  ...,  0.0124, -0.0075, -0.0315]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0031, -0.0019,  0.0197,  ...,  0.0239,  0.0005,  0.0276],\n",
      "        [-0.0039,  0.0149, -0.0273,  ..., -0.0102,  0.0011, -0.0197],\n",
      "        [ 0.0106, -0.0491,  0.0131,  ...,  0.0417,  0.0175, -0.0327],\n",
      "        ...,\n",
      "        [ 0.0143, -0.0303, -0.0086,  ..., -0.0018,  0.0171,  0.0120],\n",
      "        [ 0.0349, -0.0071,  0.0140,  ...,  0.0048,  0.0133, -0.0095],\n",
      "        [-0.0043, -0.0162,  0.0223,  ...,  0.0255,  0.0349,  0.0187]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4941, 0.5195, 0.5117,  ..., 0.4863, 0.5156, 0.5078],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4102, 0.4062, 0.4082,  ..., 0.4121, 0.4160, 0.4102],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0033, -0.0132, -0.0197,  ...,  0.0061, -0.0141,  0.0042],\n",
      "        [-0.0126,  0.0050,  0.0128,  ..., -0.0135, -0.0002, -0.0071],\n",
      "        [-0.0013,  0.0056,  0.0026,  ..., -0.0042, -0.0184, -0.0179],\n",
      "        ...,\n",
      "        [ 0.0447,  0.0466,  0.0055,  ...,  0.0027,  0.0149, -0.0288],\n",
      "        [-0.0564, -0.0574, -0.0081,  ..., -0.0150, -0.0236,  0.0055],\n",
      "        [ 0.0403, -0.0422, -0.0243,  ...,  0.0023,  0.0172, -0.0312]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0102, -0.0134, -0.0058,  ..., -0.0006,  0.0050, -0.0020],\n",
      "        [-0.0187, -0.0085,  0.0071,  ..., -0.0064, -0.0214,  0.0106],\n",
      "        [-0.0087,  0.0019, -0.0017,  ...,  0.0092, -0.0080,  0.0074],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0525, -0.0117,  ..., -0.0562, -0.0552,  0.0334],\n",
      "        [ 0.0371, -0.0386,  0.0012,  ..., -0.0030,  0.0082,  0.0150],\n",
      "        [-0.0107, -0.0232,  0.0024,  ...,  0.0364,  0.0094, -0.0208]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0085,  0.0110,  0.0059,  ...,  0.0216, -0.0171, -0.0115],\n",
      "        [-0.0056, -0.0072,  0.0046,  ...,  0.0077, -0.0332, -0.0095],\n",
      "        [-0.0155,  0.0088, -0.0167,  ..., -0.0070, -0.0043, -0.0040],\n",
      "        ...,\n",
      "        [-0.0179, -0.0311, -0.0070,  ...,  0.0147,  0.0055,  0.0107],\n",
      "        [-0.0135, -0.0003,  0.0200,  ..., -0.0010,  0.0070, -0.0211],\n",
      "        [-0.0334, -0.0140, -0.0216,  ...,  0.0016,  0.0204,  0.0211]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0182,  0.0139,  0.0011,  ...,  0.0137,  0.0024,  0.0126],\n",
      "        [-0.0245, -0.0183, -0.0175,  ...,  0.0248,  0.0022, -0.0046],\n",
      "        [-0.0054, -0.0107,  0.0182,  ..., -0.0105, -0.0157,  0.0025],\n",
      "        ...,\n",
      "        [-0.0170,  0.0136, -0.0032,  ...,  0.0124,  0.0131,  0.0276],\n",
      "        [-0.0093, -0.0238, -0.0294,  ..., -0.0219, -0.0104,  0.0078],\n",
      "        [-0.0151, -0.0086, -0.0178,  ..., -0.0430, -0.0240, -0.0152]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0020, -0.0383,  0.0425,  ..., -0.0061, -0.0118,  0.0128],\n",
      "        [-0.0203, -0.0024, -0.0084,  ...,  0.0232,  0.0366,  0.0150],\n",
      "        [-0.0112,  0.0254,  0.0063,  ...,  0.0140,  0.0090,  0.0259],\n",
      "        ...,\n",
      "        [-0.0400,  0.0023,  0.0098,  ..., -0.0035, -0.0187,  0.0049],\n",
      "        [ 0.0199, -0.0047,  0.0588,  ...,  0.0236,  0.0244,  0.0054],\n",
      "        [-0.0086, -0.0029, -0.0116,  ...,  0.0469,  0.0037, -0.0069]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0292, -0.0101,  0.0130,  ...,  0.0181, -0.0054,  0.0299],\n",
      "        [-0.0173,  0.0011, -0.0005,  ...,  0.0136, -0.0031,  0.0236],\n",
      "        [ 0.0161,  0.0101, -0.0018,  ..., -0.0198,  0.0071, -0.0157],\n",
      "        ...,\n",
      "        [ 0.0135, -0.0166, -0.0280,  ..., -0.0449,  0.0017,  0.0240],\n",
      "        [ 0.0021, -0.0063,  0.0047,  ...,  0.0061,  0.0048,  0.0008],\n",
      "        [-0.0088, -0.0092,  0.0100,  ..., -0.0334, -0.0033, -0.0066]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-3.0762e-02, -6.5308e-03,  1.6235e-02,  ...,  6.6528e-03,\n",
      "          9.7046e-03, -1.7944e-02],\n",
      "        [-1.9531e-02,  1.4877e-03,  9.8877e-03,  ..., -4.6997e-03,\n",
      "          1.1475e-02, -5.8746e-04],\n",
      "        [ 2.7222e-02,  1.0834e-03,  2.9297e-02,  ..., -1.3428e-02,\n",
      "         -2.1606e-02, -4.6692e-03],\n",
      "        ...,\n",
      "        [ 6.6833e-03,  6.5308e-03,  4.0039e-02,  ...,  7.0801e-03,\n",
      "         -3.0396e-02, -1.9165e-02],\n",
      "        [-2.9419e-02,  1.1536e-02, -3.1982e-02,  ..., -4.1504e-02,\n",
      "         -2.8320e-02,  1.6174e-03],\n",
      "        [-1.4465e-02,  2.7180e-05,  2.9175e-02,  ...,  2.1240e-02,\n",
      "          7.1106e-03,  2.3193e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5469, 0.5547, 0.5430,  ..., 0.5508, 0.5625, 0.5508],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4180, 0.4160, 0.4199,  ..., 0.4277, 0.4238, 0.4238],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 0.0332, -0.0053, -0.0087,  ..., -0.0061,  0.0381,  0.0014],\n",
      "        [-0.0084, -0.0168,  0.0106,  ..., -0.0024, -0.0214,  0.0116],\n",
      "        [-0.0004,  0.0044,  0.0103,  ...,  0.0036, -0.0405, -0.0176],\n",
      "        ...,\n",
      "        [-0.0012, -0.0110, -0.0078,  ...,  0.0141,  0.0197,  0.0026],\n",
      "        [ 0.0039, -0.0232, -0.0276,  ...,  0.0156, -0.0049, -0.0104],\n",
      "        [-0.0156,  0.0525,  0.0278,  ..., -0.0108, -0.0605, -0.0457]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0251,  0.0190, -0.0135,  ..., -0.0229,  0.0347, -0.0006],\n",
      "        [-0.0267,  0.0085,  0.0142,  ..., -0.0012,  0.0204,  0.0315],\n",
      "        [ 0.0109,  0.0278, -0.0044,  ..., -0.0245, -0.0623, -0.0060],\n",
      "        ...,\n",
      "        [ 0.0255,  0.0212,  0.0018,  ..., -0.0442, -0.0513,  0.0244],\n",
      "        [-0.0312, -0.0347, -0.0092,  ...,  0.0447,  0.0249, -0.0010],\n",
      "        [-0.0151,  0.0237, -0.0139,  ..., -0.0003, -0.0183, -0.0114]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0574, -0.0261, -0.0253,  ..., -0.0148,  0.0084, -0.0139],\n",
      "        [-0.0547, -0.0214,  0.0258,  ...,  0.0052, -0.0073,  0.0069],\n",
      "        [ 0.0021,  0.0018, -0.0278,  ..., -0.0082, -0.0054, -0.0311],\n",
      "        ...,\n",
      "        [-0.0228, -0.0145,  0.0035,  ...,  0.0461, -0.0045,  0.0087],\n",
      "        [ 0.0081, -0.0325, -0.0052,  ..., -0.0160, -0.0171, -0.0005],\n",
      "        [ 0.0061,  0.0223, -0.0110,  ...,  0.0097,  0.0007,  0.0209]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0117,  0.0023, -0.0294,  ..., -0.0017,  0.0150,  0.0302],\n",
      "        [ 0.0080,  0.0491,  0.0084,  ..., -0.0040,  0.0486, -0.0066],\n",
      "        [-0.0078,  0.0150, -0.0208,  ..., -0.0160,  0.0038, -0.0042],\n",
      "        ...,\n",
      "        [ 0.0294,  0.0029,  0.0126,  ...,  0.0009,  0.0146,  0.0034],\n",
      "        [-0.0126,  0.0076, -0.0090,  ...,  0.0132, -0.0061, -0.0481],\n",
      "        [ 0.0162,  0.0013,  0.0033,  ..., -0.0226,  0.0225, -0.0189]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0134, -0.0079,  0.0133,  ...,  0.0262,  0.0280, -0.0408],\n",
      "        [-0.0084,  0.0131, -0.0182,  ..., -0.0282, -0.0267,  0.0192],\n",
      "        [ 0.0042,  0.0033,  0.0010,  ...,  0.0068, -0.0073, -0.0179],\n",
      "        ...,\n",
      "        [-0.0051,  0.0354, -0.0089,  ..., -0.0009, -0.0342, -0.0189],\n",
      "        [-0.0206,  0.0139,  0.0247,  ...,  0.0179,  0.0181,  0.0044],\n",
      "        [ 0.0182, -0.0011, -0.0049,  ..., -0.0253, -0.0034, -0.0047]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0112, -0.0126, -0.0132,  ...,  0.0309,  0.0142,  0.0060],\n",
      "        [ 0.0124,  0.0139,  0.0047,  ...,  0.0574,  0.0219, -0.0259],\n",
      "        [-0.0187,  0.0093, -0.0267,  ..., -0.0109,  0.0199,  0.0109],\n",
      "        ...,\n",
      "        [-0.0148, -0.0071, -0.0160,  ...,  0.0093,  0.0388,  0.0008],\n",
      "        [-0.0186, -0.0147, -0.0247,  ...,  0.0081,  0.0096,  0.0166],\n",
      "        [ 0.0054, -0.0071, -0.0089,  ...,  0.0120,  0.0172,  0.0121]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0152,  0.0396, -0.0024,  ...,  0.0120, -0.0087, -0.0096],\n",
      "        [-0.0364, -0.0137,  0.0024,  ...,  0.0293, -0.0024,  0.0114],\n",
      "        [ 0.0039, -0.0124, -0.0280,  ...,  0.0135, -0.0167, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0084, -0.0249,  0.0027,  ...,  0.0130,  0.0359, -0.0110],\n",
      "        [-0.0437, -0.0025,  0.0547,  ...,  0.0309,  0.0012,  0.0053],\n",
      "        [ 0.0017, -0.0188,  0.0188,  ..., -0.0139,  0.0106, -0.0050]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5156, 0.5352, 0.5352,  ..., 0.5195, 0.5430, 0.5312],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4355, 0.4316, 0.4336,  ..., 0.4434, 0.4414, 0.4395],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0310,  0.0189,  0.0073,  ..., -0.0041,  0.0129,  0.0391],\n",
      "        [-0.0190,  0.0408,  0.0251,  ..., -0.0206,  0.0065,  0.0047],\n",
      "        [-0.0148, -0.0249,  0.0076,  ...,  0.0038, -0.0276,  0.0332],\n",
      "        ...,\n",
      "        [-0.0530,  0.0292, -0.0069,  ...,  0.0231, -0.0172, -0.0179],\n",
      "        [ 0.0026,  0.0317,  0.0159,  ...,  0.0074,  0.0032,  0.0256],\n",
      "        [-0.0243, -0.0134,  0.0077,  ...,  0.0193,  0.0148, -0.0085]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0067, -0.0066,  0.0090,  ..., -0.0039, -0.0143,  0.0099],\n",
      "        [-0.0007,  0.0055,  0.0173,  ...,  0.0435, -0.0088, -0.0099],\n",
      "        [ 0.0093, -0.0166,  0.0051,  ...,  0.0005, -0.0061, -0.0037],\n",
      "        ...,\n",
      "        [-0.0060, -0.0146,  0.0339,  ...,  0.0071, -0.0269, -0.0004],\n",
      "        [ 0.0262, -0.0187, -0.0442,  ...,  0.0610, -0.0104,  0.0471],\n",
      "        [ 0.0300,  0.0145, -0.0045,  ..., -0.0325, -0.0378,  0.0111]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0211, -0.0057, -0.0374,  ...,  0.0322,  0.0376, -0.0109],\n",
      "        [ 0.0194,  0.0173,  0.0092,  ...,  0.0009, -0.0166,  0.0148],\n",
      "        [-0.0277,  0.0038,  0.0231,  ..., -0.0115,  0.0496, -0.0269],\n",
      "        ...,\n",
      "        [ 0.0082, -0.0199, -0.0210,  ...,  0.0193, -0.0145, -0.0109],\n",
      "        [ 0.0008, -0.0227, -0.0303,  ..., -0.0128, -0.0082, -0.0160],\n",
      "        [ 0.0208, -0.0237, -0.0189,  ..., -0.0280, -0.0305,  0.0200]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0039, -0.0092,  0.0057,  ...,  0.0317,  0.0099, -0.0283],\n",
      "        [ 0.0036,  0.0315, -0.0222,  ..., -0.0125, -0.0101,  0.0011],\n",
      "        [-0.0013, -0.0139, -0.0164,  ...,  0.0165,  0.0072, -0.0056],\n",
      "        ...,\n",
      "        [ 0.0231, -0.0079, -0.0124,  ...,  0.0143,  0.0469, -0.0023],\n",
      "        [ 0.0327, -0.0221, -0.0104,  ..., -0.0002, -0.0052,  0.0229],\n",
      "        [ 0.0413,  0.0176,  0.0269,  ..., -0.0131,  0.0027, -0.0199]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0396,  0.0150, -0.0113,  ...,  0.0110,  0.0031, -0.0147],\n",
      "        [-0.0222,  0.0084, -0.0200,  ...,  0.0298,  0.0305,  0.0339],\n",
      "        [ 0.0090,  0.0146, -0.0077,  ..., -0.0162, -0.0242,  0.0302],\n",
      "        ...,\n",
      "        [ 0.0289,  0.0170,  0.0033,  ..., -0.0063,  0.0177, -0.0102],\n",
      "        [-0.0178,  0.0287, -0.0255,  ..., -0.0026, -0.0152,  0.0078],\n",
      "        [ 0.0078, -0.0183, -0.0098,  ..., -0.0067, -0.0145, -0.0007]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 9.0332e-03, -1.9897e-02,  2.9907e-02,  ..., -1.7944e-02,\n",
      "          1.4160e-02,  9.2983e-05],\n",
      "        [-7.4463e-03,  1.2207e-02, -1.4404e-02,  ..., -1.9165e-02,\n",
      "         -2.5146e-02,  7.3242e-03],\n",
      "        [ 3.2471e-02,  1.1902e-02,  5.7678e-03,  ..., -3.2715e-02,\n",
      "          4.1809e-03,  2.8076e-02],\n",
      "        ...,\n",
      "        [-6.3782e-03,  1.1230e-02,  1.3428e-02,  ...,  1.1597e-02,\n",
      "         -5.1025e-02,  3.1006e-02],\n",
      "        [-2.1362e-02, -4.2419e-03, -1.5747e-02,  ...,  1.9409e-02,\n",
      "         -3.3722e-03,  2.3438e-02],\n",
      "        [ 5.3711e-03,  1.4465e-02, -5.1575e-03,  ...,  2.7222e-02,\n",
      "         -7.3547e-03,  2.6398e-03]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0053,  0.0160,  0.0284,  ..., -0.0013,  0.0102, -0.0386],\n",
      "        [-0.0161,  0.0013, -0.0264,  ...,  0.0200, -0.0157, -0.0228],\n",
      "        [ 0.0283,  0.0099,  0.0417,  ...,  0.0293, -0.0056, -0.0159],\n",
      "        ...,\n",
      "        [-0.0068,  0.0289,  0.0294,  ..., -0.0179,  0.0193, -0.0359],\n",
      "        [ 0.0134,  0.0304,  0.0098,  ...,  0.0148, -0.0139, -0.0136],\n",
      "        [-0.0051, -0.0090, -0.0097,  ..., -0.0082, -0.0166,  0.0013]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5430, 0.5508, 0.5508,  ..., 0.5508, 0.5508, 0.5547],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4551, 0.4453, 0.4434,  ..., 0.4512, 0.4551, 0.4492],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0011, -0.0182,  0.0071,  ..., -0.0114, -0.0115, -0.0049],\n",
      "        [ 0.0124, -0.0061, -0.0055,  ...,  0.0125,  0.0011, -0.0008],\n",
      "        [ 0.0131,  0.0055, -0.0079,  ..., -0.0007, -0.0222,  0.0199],\n",
      "        ...,\n",
      "        [-0.0049,  0.0332, -0.0522,  ...,  0.0107, -0.0012, -0.0452],\n",
      "        [ 0.0179, -0.0315, -0.0143,  ...,  0.0254,  0.0413, -0.0608],\n",
      "        [ 0.0108,  0.0154, -0.0493,  ...,  0.0476,  0.0197, -0.0339]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0008, -0.0014,  0.0147,  ..., -0.0046,  0.0093, -0.0211],\n",
      "        [-0.0145,  0.0022, -0.0209,  ..., -0.0136,  0.0036,  0.0079],\n",
      "        [ 0.0072,  0.0085, -0.0178,  ..., -0.0164, -0.0069, -0.0037],\n",
      "        ...,\n",
      "        [-0.0330, -0.0109,  0.0023,  ..., -0.0093, -0.0466, -0.0028],\n",
      "        [ 0.0435, -0.0223,  0.0131,  ...,  0.0146,  0.0371,  0.0327],\n",
      "        [ 0.0193, -0.0123,  0.0041,  ..., -0.0019, -0.0215, -0.0134]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0256,  0.0081,  0.0265,  ..., -0.0178, -0.0276,  0.0178],\n",
      "        [ 0.0206, -0.0146, -0.0082,  ..., -0.0469,  0.0325, -0.0129],\n",
      "        [-0.0064,  0.0178, -0.0040,  ...,  0.0049, -0.0435,  0.0129],\n",
      "        ...,\n",
      "        [ 0.0276,  0.0053, -0.0042,  ...,  0.0142, -0.0013, -0.0249],\n",
      "        [ 0.0439,  0.0076,  0.0408,  ...,  0.0079,  0.0112, -0.0038],\n",
      "        [ 0.0147,  0.0177, -0.0471,  ...,  0.0281,  0.0021,  0.0010]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0101,  0.0243, -0.0454,  ..., -0.0469,  0.0193,  0.0017],\n",
      "        [-0.0157, -0.0215,  0.0021,  ..., -0.0131,  0.0204,  0.0300],\n",
      "        [ 0.0284,  0.0131, -0.0542,  ...,  0.0260,  0.0238, -0.0192],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0104,  0.0332,  ..., -0.0122, -0.0413, -0.0074],\n",
      "        [-0.0089, -0.0552, -0.0157,  ...,  0.0087, -0.0254, -0.0280],\n",
      "        [-0.0461, -0.0054, -0.0112,  ..., -0.0225, -0.0023,  0.0046]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0013,  0.0034, -0.0035,  ...,  0.0294, -0.0095,  0.0347],\n",
      "        [-0.0243,  0.0200, -0.0121,  ...,  0.0121,  0.0069,  0.0267],\n",
      "        [ 0.0156,  0.0306,  0.0232,  ...,  0.0217,  0.0029,  0.0085],\n",
      "        ...,\n",
      "        [-0.0120,  0.0306,  0.0070,  ...,  0.0146,  0.0126, -0.0112],\n",
      "        [ 0.0092, -0.0325, -0.0146,  ..., -0.0156, -0.0150,  0.0030],\n",
      "        [-0.0288,  0.0165,  0.0139,  ..., -0.0087, -0.0139,  0.0192]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0078,  0.0393, -0.0098,  ...,  0.0085, -0.0061, -0.0260],\n",
      "        [ 0.0022, -0.0320,  0.0171,  ..., -0.0330, -0.0072, -0.0271],\n",
      "        [-0.0164,  0.0259, -0.0085,  ...,  0.0054,  0.0012,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0085, -0.0115,  0.0028,  ..., -0.0247, -0.0265,  0.0113],\n",
      "        [-0.0140, -0.0171,  0.0569,  ..., -0.0203, -0.0009,  0.0097],\n",
      "        [-0.0265,  0.0099, -0.0124,  ...,  0.0242,  0.0342, -0.0160]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0206,  0.0036,  0.0238,  ...,  0.0286,  0.0181,  0.0172],\n",
      "        [-0.0154, -0.0251, -0.0020,  ..., -0.0155, -0.0297, -0.0223],\n",
      "        [ 0.0128,  0.0147, -0.0058,  ...,  0.0096,  0.0029, -0.0228],\n",
      "        ...,\n",
      "        [-0.0139, -0.0048,  0.0265,  ..., -0.0045,  0.0038,  0.0040],\n",
      "        [-0.0231,  0.0046,  0.0001,  ...,  0.0132, -0.0054, -0.0505],\n",
      "        [-0.0085, -0.0270, -0.0046,  ..., -0.0083,  0.0070, -0.0055]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5625, 0.5664, 0.5586,  ..., 0.5469, 0.5664, 0.5586],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4629, 0.4629, 0.4551,  ..., 0.4668, 0.4590, 0.4570],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[ 2.7924e-03,  6.8359e-03, -5.4626e-03,  ..., -3.3379e-06,\n",
      "         -4.1809e-03,  5.2795e-03],\n",
      "        [-2.4170e-02, -2.1118e-02, -2.5635e-02,  ..., -1.3062e-02,\n",
      "          6.8665e-04, -4.9133e-03],\n",
      "        [-1.8921e-02,  3.3936e-02, -1.3123e-03,  ..., -7.7248e-05,\n",
      "         -8.9722e-03,  1.4587e-02],\n",
      "        ...,\n",
      "        [-7.6904e-03,  6.0059e-02, -3.4180e-02,  ...,  9.7046e-03,\n",
      "         -1.0010e-02, -3.2471e-02],\n",
      "        [-2.3560e-02, -2.3438e-02,  1.9775e-02,  ..., -8.6060e-03,\n",
      "         -2.2339e-02,  2.1973e-02],\n",
      "        [ 4.8256e-04, -4.1260e-02, -1.1047e-02,  ...,  1.1597e-02,\n",
      "          6.8970e-03, -3.9795e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0381, -0.0104, -0.0168,  ..., -0.0020,  0.0068,  0.0026],\n",
      "        [-0.0161,  0.0216, -0.0023,  ..., -0.0219,  0.0087, -0.0275],\n",
      "        [ 0.0183,  0.0089,  0.0188,  ...,  0.0181,  0.0250,  0.0014],\n",
      "        ...,\n",
      "        [-0.0347,  0.0378,  0.0057,  ..., -0.0067, -0.0040, -0.0106],\n",
      "        [ 0.0031, -0.0171, -0.0688,  ..., -0.0106, -0.0110, -0.0391],\n",
      "        [ 0.0160, -0.0095, -0.0562,  ..., -0.0596, -0.0239,  0.0049]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 1.1841e-02,  5.0354e-03,  2.9907e-02,  ...,  2.3682e-02,\n",
      "          1.6098e-03,  1.9775e-02],\n",
      "        [-3.0365e-03,  2.4170e-02,  4.0771e-02,  ..., -2.0386e-02,\n",
      "          3.9978e-03, -5.8899e-03],\n",
      "        [-1.4404e-02,  5.7373e-03, -8.3618e-03,  ..., -6.2943e-04,\n",
      "          2.8931e-02,  1.2268e-02],\n",
      "        ...,\n",
      "        [ 8.7280e-03,  2.2217e-02,  1.2146e-02,  ..., -2.5513e-02,\n",
      "         -7.1716e-03, -1.5015e-02],\n",
      "        [ 4.3631e-05,  3.4668e-02, -2.4292e-02,  ...,  1.8188e-02,\n",
      "          3.0151e-02, -1.1841e-02],\n",
      "        [ 2.1076e-04, -1.6968e-02,  6.9580e-03,  ...,  3.5706e-03,\n",
      "         -1.4771e-02,  2.5146e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0046,  0.0344,  0.0322,  ...,  0.0020, -0.0175, -0.0074],\n",
      "        [-0.0203, -0.0217, -0.0094,  ...,  0.0117, -0.0267,  0.0142],\n",
      "        [ 0.0229,  0.0262, -0.0123,  ..., -0.0371,  0.0162, -0.0183],\n",
      "        ...,\n",
      "        [-0.0294, -0.0157,  0.0223,  ..., -0.0081, -0.0092, -0.0176],\n",
      "        [-0.0110, -0.0070,  0.0332,  ..., -0.0014, -0.0153,  0.0065],\n",
      "        [ 0.0120,  0.0110,  0.0045,  ...,  0.0063,  0.0157, -0.0282]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0090,  0.0262, -0.0103,  ..., -0.0060, -0.0286,  0.0327],\n",
      "        [ 0.0074,  0.0233, -0.0110,  ..., -0.0232,  0.0112,  0.0042],\n",
      "        [ 0.0381,  0.0012, -0.0120,  ..., -0.0113,  0.0267,  0.0381],\n",
      "        ...,\n",
      "        [-0.0144,  0.0075, -0.0151,  ..., -0.0050,  0.0106,  0.0082],\n",
      "        [ 0.0132,  0.0138, -0.0327,  ..., -0.0179, -0.0065,  0.0179],\n",
      "        [ 0.0021,  0.0145,  0.0015,  ...,  0.0215, -0.0197, -0.0144]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 0.0461,  0.0077, -0.0003,  ...,  0.0145, -0.0166, -0.0150],\n",
      "        [ 0.0187,  0.0112,  0.0248,  ...,  0.0299,  0.0075,  0.0018],\n",
      "        [ 0.0019, -0.0186, -0.0121,  ...,  0.0042,  0.0076, -0.0272],\n",
      "        ...,\n",
      "        [ 0.0138, -0.0114,  0.0088,  ..., -0.0159, -0.0051, -0.0209],\n",
      "        [ 0.0284,  0.0188,  0.0310,  ...,  0.0106, -0.0405,  0.0030],\n",
      "        [ 0.0121, -0.0197, -0.0077,  ..., -0.0145, -0.0067,  0.0015]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0027, -0.0015,  0.0074,  ...,  0.0162,  0.0053,  0.0095],\n",
      "        [-0.0055,  0.0070,  0.0334,  ...,  0.0036, -0.0216,  0.0342],\n",
      "        [-0.0165, -0.0255, -0.0034,  ..., -0.0459,  0.0023, -0.0371],\n",
      "        ...,\n",
      "        [ 0.0327,  0.0118,  0.0104,  ...,  0.0106,  0.0251,  0.0386],\n",
      "        [-0.0107, -0.0154, -0.0051,  ...,  0.0171,  0.0150, -0.0037],\n",
      "        [ 0.0104, -0.0023,  0.0131,  ..., -0.0057,  0.0171,  0.0045]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5273, 0.5391, 0.5312,  ..., 0.5273, 0.5352, 0.5547],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4688, 0.4707, 0.4668,  ..., 0.4727, 0.4746, 0.4727],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-3.7003e-04, -1.4038e-02, -1.3428e-03,  ..., -3.1586e-03,\n",
      "         -5.0964e-03, -2.3804e-03],\n",
      "        [ 1.6357e-02,  1.1597e-02,  1.0986e-02,  ..., -2.5940e-03,\n",
      "         -9.5215e-03, -1.8921e-02],\n",
      "        [-6.1035e-03, -2.1973e-02,  4.6143e-02,  ...,  1.6113e-02,\n",
      "          4.1809e-03, -3.1494e-02],\n",
      "        ...,\n",
      "        [-8.6060e-03, -4.0588e-03, -3.4485e-03,  ...,  1.4465e-02,\n",
      "         -1.0071e-02,  4.6387e-03],\n",
      "        [-1.3065e-04, -1.9775e-02, -1.6113e-02,  ...,  1.7944e-02,\n",
      "          1.0559e-02, -3.2471e-02],\n",
      "        [-6.9336e-02, -1.3550e-02,  3.9795e-02,  ..., -7.2327e-03,\n",
      "         -5.9605e-05, -5.8899e-03]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 2.6367e-02, -2.2339e-02,  3.7766e-04,  ..., -9.5825e-03,\n",
      "          1.3184e-02,  8.3618e-03],\n",
      "        [ 3.8086e-02,  1.5747e-02,  4.5166e-03,  ..., -3.7384e-03,\n",
      "         -7.3547e-03, -3.6716e-05],\n",
      "        [-5.4016e-03, -1.1719e-02,  2.4902e-02,  ..., -1.1673e-03,\n",
      "         -8.9111e-03, -1.2589e-03],\n",
      "        ...,\n",
      "        [ 7.4158e-03,  2.6123e-02, -3.2425e-04,  ...,  5.5176e-02,\n",
      "          4.6082e-03, -2.6093e-03],\n",
      "        [ 1.1169e-02, -6.5430e-02,  3.6621e-02,  ...,  6.2256e-03,\n",
      "         -9.2773e-03, -2.3926e-02],\n",
      "        [-3.9062e-02, -1.3428e-02,  1.8311e-02,  ...,  3.2349e-03,\n",
      "          1.3672e-02, -2.0386e-02]], dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0024, -0.0354,  0.0256,  ..., -0.0308, -0.0339,  0.0315],\n",
      "        [ 0.0148,  0.0251,  0.0302,  ..., -0.0118,  0.0374,  0.0049],\n",
      "        [-0.0184,  0.0133, -0.0171,  ...,  0.0058, -0.0260, -0.0126],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0058,  0.0007,  ..., -0.0258, -0.0022,  0.0383],\n",
      "        [-0.0018, -0.0469,  0.0219,  ...,  0.0317, -0.0040,  0.0254],\n",
      "        [-0.0011,  0.0054,  0.0161,  ..., -0.0107, -0.0562, -0.0242]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0067,  0.0102, -0.0112,  ...,  0.0078, -0.0172, -0.0222],\n",
      "        [-0.0203, -0.0199, -0.0007,  ...,  0.0168, -0.0211, -0.0209],\n",
      "        [-0.0188, -0.0032,  0.0178,  ..., -0.0518, -0.0151, -0.0117],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0045,  0.0034,  ..., -0.0258,  0.0084,  0.0143],\n",
      "        [ 0.0271, -0.0123,  0.0130,  ...,  0.0305, -0.0024,  0.0081],\n",
      "        [-0.0054,  0.0114,  0.0043,  ..., -0.0027,  0.0132, -0.0165]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0334, -0.0315, -0.0045,  ...,  0.0122, -0.0076,  0.0036],\n",
      "        [-0.0192, -0.0035,  0.0322,  ..., -0.0036, -0.0026,  0.0315],\n",
      "        [-0.0219, -0.0107, -0.0303,  ...,  0.0140,  0.0087, -0.0057],\n",
      "        ...,\n",
      "        [-0.0244, -0.0114,  0.0197,  ..., -0.0139, -0.0229, -0.0153],\n",
      "        [ 0.0315,  0.0250, -0.0129,  ...,  0.0139, -0.0396,  0.0027],\n",
      "        [-0.0085, -0.0242, -0.0216,  ..., -0.0016,  0.0103, -0.0222]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[ 1.9531e-02, -1.4648e-02, -3.1738e-02,  ...,  2.8076e-02,\n",
      "         -5.0659e-03,  6.6528e-03],\n",
      "        [ 2.5757e-02, -7.3242e-04,  4.3701e-02,  ...,  1.4648e-02,\n",
      "          4.4861e-03, -7.6294e-03],\n",
      "        [ 2.2583e-03, -7.8735e-03, -1.9897e-02,  ..., -7.5912e-04,\n",
      "          1.2207e-02,  1.9165e-02],\n",
      "        ...,\n",
      "        [-3.1738e-02,  4.3640e-03, -2.3315e-02,  ...,  2.0874e-02,\n",
      "         -2.1729e-02, -1.1719e-02],\n",
      "        [-5.8746e-04, -3.1982e-02,  3.5095e-03,  ...,  2.0752e-02,\n",
      "          2.4414e-03,  2.3723e-05],\n",
      "        [-6.3782e-03, -1.3306e-02, -2.6367e-02,  ...,  1.0254e-02,\n",
      "         -2.9144e-03, -1.7822e-02]], dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0010,  0.0090,  0.0110,  ...,  0.0164,  0.0219,  0.0026],\n",
      "        [-0.0286, -0.0287,  0.0354,  ...,  0.0002, -0.0106, -0.0159],\n",
      "        [-0.0337, -0.0217, -0.0386,  ..., -0.0170, -0.0074,  0.0157],\n",
      "        ...,\n",
      "        [-0.0206, -0.0125,  0.0146,  ...,  0.0168, -0.0134, -0.0216],\n",
      "        [-0.0087,  0.0043, -0.0137,  ...,  0.0093, -0.0146,  0.0172],\n",
      "        [-0.0059, -0.0228,  0.0013,  ..., -0.0141,  0.0018,  0.0176]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.5742, 0.5820, 0.5625,  ..., 0.5508, 0.5625, 0.5820],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4785, 0.4883, 0.4785,  ..., 0.4805, 0.4824, 0.4785],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0332,  0.0133,  0.0212,  ...,  0.0168, -0.0008, -0.0210],\n",
      "        [-0.0082, -0.0215, -0.0337,  ...,  0.0026, -0.0034, -0.0074],\n",
      "        [-0.0126,  0.0199,  0.0170,  ...,  0.0017,  0.0026,  0.0017],\n",
      "        ...,\n",
      "        [ 0.0161, -0.0076, -0.0208,  ..., -0.0126,  0.0242, -0.0234],\n",
      "        [-0.0194,  0.0040, -0.0098,  ..., -0.0021,  0.0012,  0.0034],\n",
      "        [-0.0549, -0.0082,  0.0303,  ...,  0.0410,  0.0121,  0.0300]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[-0.0126, -0.0219,  0.0136,  ...,  0.0066, -0.0023, -0.0223],\n",
      "        [ 0.0250, -0.0125,  0.0127,  ...,  0.0085, -0.0003, -0.0071],\n",
      "        [ 0.0091,  0.0018, -0.0088,  ..., -0.0028,  0.0084, -0.0276],\n",
      "        ...,\n",
      "        [ 0.0237, -0.0330, -0.0233,  ..., -0.0121, -0.0133, -0.0488],\n",
      "        [ 0.0159,  0.0172,  0.0170,  ..., -0.0229, -0.0216,  0.0198],\n",
      "        [-0.0771, -0.0320,  0.0109,  ..., -0.0105, -0.0004,  0.0040]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0130, -0.0278, -0.0049,  ...,  0.0087,  0.0006,  0.0131],\n",
      "        [ 0.0214,  0.0077,  0.0175,  ...,  0.0057,  0.0364,  0.0131],\n",
      "        [ 0.0121, -0.0034, -0.0160,  ...,  0.0140,  0.0001,  0.0315],\n",
      "        ...,\n",
      "        [-0.0028, -0.0085, -0.0140,  ..., -0.0178, -0.0027,  0.0260],\n",
      "        [ 0.0109, -0.0242,  0.0165,  ..., -0.0097, -0.0292,  0.0100],\n",
      "        [-0.0118, -0.0255,  0.0275,  ...,  0.0206, -0.0052,  0.0398]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0067,  0.0277, -0.0039,  ..., -0.0045, -0.0116, -0.0491],\n",
      "        [-0.0078,  0.0040, -0.0267,  ...,  0.0262, -0.0190, -0.0151],\n",
      "        [ 0.0113, -0.0002,  0.0024,  ..., -0.0153,  0.0254,  0.0088],\n",
      "        ...,\n",
      "        [ 0.0075, -0.0164,  0.0272,  ..., -0.0198, -0.0108, -0.0162],\n",
      "        [ 0.0018,  0.0203, -0.0212,  ..., -0.0223, -0.0140, -0.0187],\n",
      "        [ 0.0081,  0.0160, -0.0103,  ...,  0.0334, -0.0190,  0.0063]],\n",
      "       dtype=torch.float16, requires_grad=True) 16777216\n",
      "Parameter containing:\n",
      "tensor([[ 0.0024, -0.0095,  0.0304,  ...,  0.0154,  0.0179,  0.0029],\n",
      "        [-0.0728, -0.0264,  0.0028,  ...,  0.0093, -0.0172,  0.0178],\n",
      "        [ 0.0179,  0.0030, -0.0118,  ..., -0.0040, -0.0151, -0.0076],\n",
      "        ...,\n",
      "        [-0.0151, -0.0254,  0.0099,  ...,  0.0036,  0.0184,  0.0312],\n",
      "        [ 0.0337,  0.0165, -0.0330,  ...,  0.0364, -0.0136, -0.0089],\n",
      "        [ 0.0236, -0.0325, -0.0312,  ..., -0.0147,  0.0137,  0.0058]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0022, -0.0276, -0.0073,  ..., -0.0371, -0.0150,  0.0234],\n",
      "        [ 0.0422,  0.0020, -0.0052,  ..., -0.0020, -0.0156,  0.0132],\n",
      "        [-0.0141, -0.0110,  0.0250,  ...,  0.0312, -0.0022,  0.0238],\n",
      "        ...,\n",
      "        [ 0.0028,  0.0325, -0.0007,  ...,  0.0107,  0.0059,  0.0212],\n",
      "        [ 0.0006,  0.0476, -0.0081,  ..., -0.0082,  0.0215,  0.0221],\n",
      "        [ 0.0253, -0.0361,  0.0171,  ...,  0.0058,  0.0018,  0.0265]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([[-0.0413, -0.0168,  0.0293,  ...,  0.0072, -0.0114,  0.0031],\n",
      "        [ 0.0223,  0.0320,  0.0063,  ..., -0.0188,  0.0012, -0.0009],\n",
      "        [-0.0066,  0.0075, -0.0190,  ...,  0.0106,  0.0021, -0.0102],\n",
      "        ...,\n",
      "        [ 0.0129, -0.0109, -0.0007,  ...,  0.0014,  0.0349, -0.0176],\n",
      "        [ 0.0056,  0.0025, -0.0227,  ...,  0.0256,  0.0122, -0.0005],\n",
      "        [ 0.0420, -0.0381, -0.0028,  ..., -0.0070,  0.0036,  0.0253]],\n",
      "       dtype=torch.float16, requires_grad=True) 45088768\n",
      "Parameter containing:\n",
      "tensor([0.4863, 0.4844, 0.4355,  ..., 0.4316, 0.4551, 0.4805],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([0.4336, 0.4375, 0.4414,  ..., 0.4238, 0.4102, 0.4277],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([1.8672, 1.8672, 1.8047,  ..., 1.7188, 1.8281, 1.6016],\n",
      "       dtype=torch.float16, requires_grad=True) 4096\n",
      "Parameter containing:\n",
      "tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
      "        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],\n",
      "        [-0.0125,  0.0036,  0.0195,  ..., -0.0271,  0.0143, -0.0082],\n",
      "        ...,\n",
      "        [-0.0281, -0.0195, -0.0024,  ...,  0.0123, -0.0117, -0.0237],\n",
      "        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],\n",
      "        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337]],\n",
      "       dtype=torch.float16, requires_grad=True) 131072000\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p,p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
      "          2.5787e-03, -3.9368e-03],\n",
      "        [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
      "          7.7057e-04, -5.0049e-03],\n",
      "        ...,\n",
      "        [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
      "          9.5825e-03, -1.8005e-03],\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05]], dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
      "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
      "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
      "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
      "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight tensor([[-0.0162,  0.0079, -0.0013,  ...,  0.0166, -0.0099, -0.0135],\n",
      "        [ 0.0192,  0.0015,  0.0036,  ..., -0.0211,  0.0152,  0.0234],\n",
      "        [-0.0236, -0.0217,  0.0017,  ...,  0.0150, -0.0165, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0128, -0.0007, -0.0008,  ...,  0.0002,  0.0031,  0.0081],\n",
      "        [-0.0056,  0.0173, -0.0032,  ..., -0.0032,  0.0115, -0.0110],\n",
      "        [ 0.0037, -0.0021,  0.0013,  ...,  0.0070, -0.0115,  0.0095]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight tensor([[ 0.0008, -0.0006,  0.0019,  ...,  0.0059, -0.0006,  0.0103],\n",
      "        [-0.0069, -0.0005, -0.0077,  ..., -0.0106,  0.0126,  0.0048],\n",
      "        [ 0.0018,  0.0096,  0.0010,  ...,  0.0048, -0.0139, -0.0142],\n",
      "        ...,\n",
      "        [-0.0063, -0.0057,  0.0103,  ...,  0.0031,  0.0040, -0.0022],\n",
      "        [ 0.0031,  0.0048, -0.0010,  ...,  0.0054,  0.0156,  0.0007],\n",
      "        [ 0.0001,  0.0025,  0.0056,  ..., -0.0007, -0.0007,  0.0015]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight tensor([[-1.6212e-05, -1.9226e-03,  4.8828e-03,  ...,  5.9204e-03,\n",
      "          3.4485e-03, -9.5215e-03],\n",
      "        [ 2.7618e-03,  1.8463e-03, -1.2970e-03,  ..., -1.0300e-03,\n",
      "          1.8082e-03,  6.2561e-03],\n",
      "        [ 2.3346e-03, -2.7275e-04,  9.2697e-04,  ..., -1.6556e-03,\n",
      "         -5.7373e-03, -6.3705e-04],\n",
      "        ...,\n",
      "        [ 4.1809e-03, -3.3264e-03,  5.8899e-03,  ...,  1.2131e-03,\n",
      "          2.6093e-03,  4.3030e-03],\n",
      "        [-3.3569e-03, -2.4872e-03, -2.5787e-03,  ...,  6.1951e-03,\n",
      "         -3.4790e-03, -5.1117e-04],\n",
      "        [ 6.1951e-03, -6.5613e-04,  2.6245e-03,  ...,  5.4932e-03,\n",
      "         -7.5989e-03, -6.6833e-03]], dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight tensor([[ 1.5747e-02,  1.7090e-02,  3.1494e-02,  ..., -1.5869e-02,\n",
      "          6.5002e-03,  1.5869e-02],\n",
      "        [-2.1667e-03, -6.0120e-03,  5.6458e-03,  ...,  1.6113e-02,\n",
      "         -8.6670e-03,  9.8877e-03],\n",
      "        [ 6.8359e-03, -2.1606e-02,  2.0508e-02,  ..., -1.3000e-02,\n",
      "          1.8921e-02,  1.9409e-02],\n",
      "        ...,\n",
      "        [ 1.4126e-05, -3.2227e-02,  5.7983e-03,  ..., -8.9111e-03,\n",
      "         -1.3489e-02,  4.0283e-02],\n",
      "        [ 2.6611e-02,  2.0142e-02, -1.7090e-02,  ..., -3.4332e-03,\n",
      "         -6.4087e-03, -1.8921e-02],\n",
      "        [-5.9891e-04, -1.1353e-02, -2.3682e-02,  ...,  1.1063e-03,\n",
      "          5.9204e-03, -2.4780e-02]], dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight tensor([[ 0.0027, -0.0145,  0.0083,  ..., -0.0175, -0.0054,  0.0014],\n",
      "        [ 0.0046, -0.0042,  0.0090,  ...,  0.0160, -0.0138,  0.0334],\n",
      "        [ 0.0020,  0.0339, -0.0044,  ..., -0.0146,  0.0220,  0.0167],\n",
      "        ...,\n",
      "        [-0.0089, -0.0114,  0.0052,  ...,  0.0231, -0.0135,  0.0295],\n",
      "        [-0.0177,  0.0374,  0.0090,  ..., -0.0069, -0.0122, -0.0219],\n",
      "        [ 0.0120, -0.0013, -0.0079,  ..., -0.0003, -0.0030, -0.0302]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight tensor([[ 0.0003, -0.0292,  0.0148,  ..., -0.0210, -0.0270,  0.0065],\n",
      "        [-0.0111, -0.0312,  0.0128,  ...,  0.0190,  0.0060,  0.0025],\n",
      "        [-0.0059,  0.0149, -0.0084,  ..., -0.0227,  0.0075,  0.0017],\n",
      "        ...,\n",
      "        [-0.0091, -0.0016, -0.0067,  ...,  0.0295, -0.0028,  0.0183],\n",
      "        [-0.0166,  0.0073,  0.0189,  ...,  0.0014, -0.0166,  0.0031],\n",
      "        [ 0.0190,  0.0197, -0.0004,  ...,  0.0118, -0.0143, -0.0388]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight tensor([0.0297, 0.0136, 0.0020,  ..., 0.0103, 0.0110, 0.0061],\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight tensor([0.0503, 0.0525, 0.0500,  ..., 0.0525, 0.0535, 0.0491],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.self_attn.q_proj.weight tensor([[-0.0125,  0.0073, -0.0381,  ..., -0.0024, -0.0588,  0.0356],\n",
      "        [-0.0006, -0.0082,  0.0079,  ..., -0.0083, -0.0488,  0.0277],\n",
      "        [ 0.0306,  0.0325,  0.0205,  ..., -0.0001, -0.0747,  0.0229],\n",
      "        ...,\n",
      "        [-0.0002,  0.0018,  0.0036,  ..., -0.0087, -0.0039, -0.0060],\n",
      "        [-0.0021, -0.0038, -0.0042,  ...,  0.0088,  0.0052,  0.0062],\n",
      "        [ 0.0003,  0.0048,  0.0067,  ..., -0.0079, -0.0005, -0.0111]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.self_attn.k_proj.weight tensor([[-2.4780e-02, -2.5024e-03,  3.8330e-02,  ...,  1.7944e-02,\n",
      "          2.0752e-02, -9.6436e-03],\n",
      "        [-2.9541e-02,  4.5776e-03, -1.1353e-02,  ..., -1.5869e-02,\n",
      "          9.3994e-03, -5.8838e-02],\n",
      "        [-2.5024e-02,  2.9419e-02, -6.4941e-02,  ...,  3.4912e-02,\n",
      "          7.8735e-03, -5.6396e-02],\n",
      "        ...,\n",
      "        [-1.1108e-02,  1.8921e-02, -1.5030e-03,  ...,  1.0925e-02,\n",
      "          6.0797e-05,  6.7139e-03],\n",
      "        [ 7.9956e-03, -1.9165e-02,  3.9978e-03,  ..., -1.2146e-02,\n",
      "         -1.4648e-03, -6.5002e-03],\n",
      "        [-7.9956e-03,  1.4709e-02,  7.1716e-04,  ...,  3.9978e-03,\n",
      "         -1.5640e-03,  6.0425e-03]], dtype=torch.float16)\n",
      "model.layers.1.self_attn.v_proj.weight tensor([[ 0.0068, -0.0084, -0.0041,  ..., -0.0061, -0.0312, -0.0262],\n",
      "        [-0.0036,  0.0024, -0.0024,  ...,  0.0008,  0.0110,  0.0332],\n",
      "        [ 0.0059, -0.0118,  0.0109,  ...,  0.0004, -0.0009, -0.0019],\n",
      "        ...,\n",
      "        [-0.0053,  0.0027,  0.0046,  ..., -0.0017, -0.0010,  0.0015],\n",
      "        [-0.0063, -0.0056,  0.0094,  ..., -0.0081,  0.0030, -0.0010],\n",
      "        [-0.0010,  0.0016,  0.0042,  ...,  0.0019, -0.0058,  0.0087]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.self_attn.o_proj.weight tensor([[ 0.0048, -0.0194,  0.0142,  ..., -0.0050, -0.0001,  0.0003],\n",
      "        [-0.0030, -0.0054, -0.0132,  ..., -0.0007, -0.0027,  0.0032],\n",
      "        [ 0.0223,  0.0126, -0.0120,  ...,  0.0047,  0.0020, -0.0022],\n",
      "        ...,\n",
      "        [ 0.0033,  0.0069,  0.0051,  ...,  0.0003,  0.0022, -0.0019],\n",
      "        [ 0.0022, -0.0125,  0.0050,  ..., -0.0022,  0.0018,  0.0021],\n",
      "        [-0.0043, -0.0205,  0.0054,  ...,  0.0013, -0.0043,  0.0019]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.mlp.gate_proj.weight tensor([[ 0.0292, -0.0095,  0.0374,  ..., -0.0339, -0.0186, -0.0113],\n",
      "        [-0.0281, -0.0018, -0.0315,  ...,  0.0144, -0.0219, -0.0408],\n",
      "        [-0.0061, -0.0289,  0.0031,  ..., -0.0093, -0.0151, -0.0211],\n",
      "        ...,\n",
      "        [-0.0400,  0.0164, -0.0042,  ..., -0.0016,  0.0144, -0.0014],\n",
      "        [-0.0161,  0.0036, -0.0374,  ...,  0.0206,  0.0195, -0.0120],\n",
      "        [-0.0249,  0.0146,  0.0055,  ...,  0.0364,  0.0099, -0.0347]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.mlp.down_proj.weight tensor([[ 0.0079,  0.0097,  0.0559,  ..., -0.0170,  0.0024, -0.0197],\n",
      "        [ 0.0107,  0.0064, -0.0359,  ...,  0.0091, -0.0132,  0.0029],\n",
      "        [-0.0171, -0.0146, -0.0232,  ..., -0.0014, -0.0106,  0.0034],\n",
      "        ...,\n",
      "        [-0.0102, -0.0065, -0.0162,  ...,  0.0430, -0.0063,  0.0074],\n",
      "        [-0.0129,  0.0215,  0.0149,  ..., -0.0299, -0.0189,  0.0233],\n",
      "        [ 0.0244, -0.0038, -0.0165,  ..., -0.0135, -0.0142,  0.0298]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.mlp.up_proj.weight tensor([[ 0.0017,  0.0356, -0.0175,  ...,  0.0116,  0.0302, -0.0177],\n",
      "        [ 0.0085, -0.0023, -0.0134,  ..., -0.0079,  0.0101,  0.0095],\n",
      "        [ 0.0278, -0.0447,  0.0129,  ...,  0.0168, -0.0454,  0.0289],\n",
      "        ...,\n",
      "        [-0.0137, -0.0271,  0.0133,  ...,  0.0256,  0.0107, -0.0251],\n",
      "        [ 0.0349,  0.0015, -0.0167,  ...,  0.0043, -0.0071, -0.0012],\n",
      "        [-0.0159, -0.0182, -0.0098,  ..., -0.0231, -0.0075,  0.0219]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.input_layernorm.weight tensor([0.1138, 0.1099, 0.1006,  ..., 0.0630, 0.0942, 0.0742],\n",
      "       dtype=torch.float16)\n",
      "model.layers.1.post_attention_layernorm.weight tensor([0.0996, 0.1006, 0.0962,  ..., 0.1074, 0.0996, 0.1016],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.self_attn.q_proj.weight tensor([[-0.0233, -0.0091,  0.0077,  ..., -0.0102, -0.0187, -0.0078],\n",
      "        [-0.0148,  0.0107, -0.0374,  ..., -0.0364, -0.0178,  0.0271],\n",
      "        [-0.0143,  0.0330, -0.0256,  ..., -0.0053, -0.0201,  0.0211],\n",
      "        ...,\n",
      "        [-0.0476,  0.0135, -0.0226,  ..., -0.0068, -0.0303, -0.0364],\n",
      "        [ 0.0024, -0.0093,  0.0017,  ..., -0.0012, -0.0156,  0.0137],\n",
      "        [-0.0066, -0.0200,  0.0253,  ...,  0.0625, -0.0469,  0.0022]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.self_attn.k_proj.weight tensor([[-0.0011,  0.0087, -0.0071,  ..., -0.0181, -0.0072,  0.0104],\n",
      "        [ 0.0186, -0.0118, -0.0044,  ...,  0.0171, -0.0067, -0.0009],\n",
      "        [ 0.0135,  0.0128,  0.0354,  ..., -0.0135,  0.0107,  0.0052],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0491, -0.0284,  ...,  0.0305, -0.0400, -0.0062],\n",
      "        [-0.0061, -0.0123, -0.0007,  ...,  0.0041,  0.0013, -0.0040],\n",
      "        [ 0.0059, -0.0156, -0.0045,  ..., -0.0830, -0.0278,  0.0864]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.self_attn.v_proj.weight tensor([[-0.0045,  0.0008,  0.0212,  ...,  0.0396, -0.0060,  0.0048],\n",
      "        [ 0.0028,  0.0141, -0.0219,  ..., -0.0093,  0.0053, -0.0159],\n",
      "        [-0.0007,  0.0152, -0.0013,  ..., -0.0162, -0.0293, -0.0298],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0306, -0.0190,  ..., -0.0131, -0.0017,  0.0011],\n",
      "        [-0.0366, -0.0006,  0.0178,  ..., -0.0076, -0.0104, -0.0060],\n",
      "        [-0.0122,  0.0168,  0.0013,  ..., -0.0052,  0.0157, -0.0078]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.self_attn.o_proj.weight tensor([[-0.0018,  0.0153,  0.0092,  ...,  0.0181,  0.0033, -0.0280],\n",
      "        [-0.0010, -0.0184,  0.0059,  ...,  0.0047, -0.0119, -0.0208],\n",
      "        [-0.0204,  0.0032, -0.0145,  ..., -0.0187, -0.0085,  0.0096],\n",
      "        ...,\n",
      "        [-0.0072, -0.0275,  0.0303,  ..., -0.0042, -0.0216,  0.0206],\n",
      "        [ 0.0205, -0.0157,  0.0359,  ...,  0.0166,  0.0019, -0.0226],\n",
      "        [-0.0120,  0.0013,  0.0097,  ..., -0.0086,  0.0108, -0.0146]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.2.mlp.gate_proj.weight tensor([[ 0.0086,  0.0239, -0.0128,  ..., -0.0086,  0.0275,  0.0019],\n",
      "        [-0.0002,  0.0024, -0.0044,  ...,  0.0105, -0.0146, -0.0077],\n",
      "        [ 0.0018, -0.0089,  0.0266,  ..., -0.0121,  0.0182,  0.0073],\n",
      "        ...,\n",
      "        [ 0.0297, -0.0104, -0.0149,  ..., -0.0339, -0.0408, -0.0122],\n",
      "        [-0.0125, -0.0157, -0.0031,  ...,  0.0223, -0.0496,  0.0131],\n",
      "        [-0.0016,  0.0123,  0.0048,  ..., -0.0129, -0.0291,  0.0056]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.mlp.down_proj.weight tensor([[ 9.8267e-03,  1.5259e-02,  3.2959e-02,  ..., -2.1729e-02,\n",
      "         -2.4414e-03, -3.1494e-02],\n",
      "        [ 2.0142e-02, -2.1973e-02, -1.3733e-02,  ...,  1.2756e-02,\n",
      "          2.9907e-03, -2.3804e-02],\n",
      "        [ 3.9062e-02, -1.8677e-02, -3.0518e-03,  ..., -2.8442e-02,\n",
      "          1.8539e-03, -3.3936e-02],\n",
      "        ...,\n",
      "        [-1.1780e-02,  2.8076e-02, -8.3008e-03,  ..., -1.5616e-05,\n",
      "         -3.7689e-03, -1.3000e-02],\n",
      "        [-2.1484e-02,  3.4668e-02,  9.8267e-03,  ...,  1.0742e-02,\n",
      "         -1.4221e-02, -1.0010e-02],\n",
      "        [ 3.5400e-02,  9.2773e-03, -2.2461e-02,  ...,  2.7710e-02,\n",
      "         -4.1992e-02, -7.0496e-03]], dtype=torch.float16)\n",
      "model.layers.2.mlp.up_proj.weight tensor([[ 0.0016, -0.0012,  0.0139,  ..., -0.0198,  0.0069, -0.0007],\n",
      "        [ 0.0074, -0.0179, -0.0089,  ...,  0.0035,  0.0118,  0.0090],\n",
      "        [ 0.0272,  0.0162,  0.0286,  ..., -0.0131, -0.0060,  0.0203],\n",
      "        ...,\n",
      "        [-0.0089, -0.0154,  0.0018,  ..., -0.0164, -0.0078,  0.0214],\n",
      "        [ 0.0146, -0.0041,  0.0129,  ..., -0.0020,  0.0057, -0.0260],\n",
      "        [-0.0148, -0.0288, -0.0232,  ..., -0.0058,  0.0009, -0.0072]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.input_layernorm.weight tensor([0.1738, 0.1777, 0.1738,  ..., 0.1768, 0.1709, 0.1748],\n",
      "       dtype=torch.float16)\n",
      "model.layers.2.post_attention_layernorm.weight tensor([0.1338, 0.1367, 0.1357,  ..., 0.1357, 0.1387, 0.1357],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.self_attn.q_proj.weight tensor([[ 0.0072,  0.0124,  0.0007,  ...,  0.0283,  0.0149,  0.0217],\n",
      "        [-0.0113,  0.0168, -0.0194,  ..., -0.0437,  0.0007, -0.0057],\n",
      "        [ 0.0154,  0.0062, -0.0200,  ...,  0.0106, -0.0198, -0.0344],\n",
      "        ...,\n",
      "        [ 0.0703, -0.0684,  0.0415,  ..., -0.0762, -0.0120,  0.0060],\n",
      "        [-0.0801,  0.0388, -0.0036,  ...,  0.0334,  0.0718,  0.0206],\n",
      "        [-0.0154, -0.0510,  0.0713,  ..., -0.0239, -0.0087, -0.0129]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.self_attn.k_proj.weight tensor([[ 0.0096, -0.0084, -0.0079,  ..., -0.0055,  0.0032,  0.0425],\n",
      "        [-0.0122,  0.0121, -0.0237,  ...,  0.0018, -0.0102, -0.0376],\n",
      "        [ 0.0002, -0.0081, -0.0022,  ...,  0.0061, -0.0066, -0.0322],\n",
      "        ...,\n",
      "        [ 0.0894, -0.0850,  0.0037,  ..., -0.0630, -0.0019,  0.0278],\n",
      "        [-0.0864,  0.0277,  0.0435,  ...,  0.0086,  0.0481, -0.0003],\n",
      "        [ 0.0011, -0.0510,  0.0923,  ..., -0.0157,  0.0069, -0.0115]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.self_attn.v_proj.weight tensor([[-0.0014,  0.0118, -0.0049,  ..., -0.0126,  0.0042, -0.0125],\n",
      "        [-0.0101,  0.0251, -0.0173,  ..., -0.0066, -0.0087, -0.0086],\n",
      "        [ 0.0026, -0.0021,  0.0029,  ..., -0.0391, -0.0127, -0.0280],\n",
      "        ...,\n",
      "        [-0.0077, -0.0098,  0.0025,  ...,  0.0018, -0.0023, -0.0074],\n",
      "        [ 0.0036,  0.0001,  0.0068,  ...,  0.0034, -0.0066,  0.0028],\n",
      "        [-0.0091, -0.0064,  0.0103,  ..., -0.0006,  0.0014,  0.0106]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.self_attn.o_proj.weight tensor([[-0.0273,  0.0070, -0.0172,  ...,  0.0052,  0.0069, -0.0029],\n",
      "        [ 0.0270, -0.0281, -0.0084,  ...,  0.0076, -0.0031,  0.0005],\n",
      "        [-0.0013, -0.0106, -0.0010,  ...,  0.0005,  0.0029,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0148,  0.0130,  ...,  0.0014,  0.0031,  0.0107],\n",
      "        [ 0.0201,  0.0243, -0.0036,  ...,  0.0005, -0.0017, -0.0013],\n",
      "        [ 0.0090, -0.0036,  0.0055,  ...,  0.0041,  0.0028,  0.0088]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.mlp.gate_proj.weight tensor([[-4.2114e-03, -1.2634e-02, -1.7700e-02,  ..., -2.8687e-02,\n",
      "          4.3701e-02,  1.2817e-02],\n",
      "        [ 1.6113e-02,  3.7956e-04, -1.1475e-02,  ..., -7.1716e-03,\n",
      "         -2.3926e-02, -1.0986e-02],\n",
      "        [-2.7710e-02,  1.4877e-03, -1.2878e-02,  ...,  8.2397e-03,\n",
      "         -4.6692e-03, -3.4424e-02],\n",
      "        ...,\n",
      "        [-1.4038e-03,  3.8086e-02,  3.4790e-03,  ...,  1.7090e-02,\n",
      "         -1.2817e-02, -5.1575e-03],\n",
      "        [-5.4016e-03,  7.1716e-03, -2.4261e-03,  ...,  8.6308e-05,\n",
      "          1.0834e-03, -8.3618e-03],\n",
      "        [-2.1362e-03, -1.5991e-02,  2.0752e-03,  ...,  2.1484e-02,\n",
      "         -2.5146e-02,  1.1108e-02]], dtype=torch.float16)\n",
      "model.layers.3.mlp.down_proj.weight tensor([[-0.0008, -0.0067,  0.0055,  ..., -0.0081,  0.0058,  0.0179],\n",
      "        [ 0.0262,  0.0061,  0.0149,  ..., -0.0160, -0.0166, -0.0109],\n",
      "        [ 0.0132,  0.0131, -0.0173,  ..., -0.0139, -0.0079,  0.0054],\n",
      "        ...,\n",
      "        [ 0.0212, -0.0088, -0.0148,  ...,  0.0009,  0.0157,  0.0173],\n",
      "        [-0.0216,  0.0131, -0.0156,  ..., -0.0016, -0.0162,  0.0134],\n",
      "        [ 0.0080, -0.0156, -0.0264,  ..., -0.0017,  0.0082,  0.0084]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.mlp.up_proj.weight tensor([[-0.0117,  0.0143,  0.0057,  ..., -0.0026,  0.0160, -0.0109],\n",
      "        [-0.0116, -0.0155,  0.0364,  ...,  0.0188,  0.0159, -0.0066],\n",
      "        [-0.0030, -0.0014, -0.0157,  ...,  0.0013,  0.0054, -0.0123],\n",
      "        ...,\n",
      "        [-0.0073, -0.0052, -0.0024,  ...,  0.0234,  0.0114, -0.0243],\n",
      "        [ 0.0038, -0.0103, -0.0082,  ...,  0.0118, -0.0201, -0.0093],\n",
      "        [ 0.0420,  0.0009, -0.0013,  ...,  0.0454,  0.0067,  0.0024]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.input_layernorm.weight tensor([0.2832, 0.2832, 0.2812,  ..., 0.2793, 0.2891, 0.2910],\n",
      "       dtype=torch.float16)\n",
      "model.layers.3.post_attention_layernorm.weight tensor([0.1748, 0.1748, 0.1699,  ..., 0.1738, 0.1709, 0.1748],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.self_attn.q_proj.weight tensor([[-0.0176,  0.0045, -0.0019,  ..., -0.0047, -0.0041,  0.0100],\n",
      "        [ 0.0231, -0.0128, -0.0090,  ...,  0.0089, -0.0253, -0.0028],\n",
      "        [-0.0034, -0.0280,  0.0143,  ...,  0.0008, -0.0121, -0.0303],\n",
      "        ...,\n",
      "        [ 0.0226, -0.0038, -0.0037,  ...,  0.0014,  0.0247, -0.0630],\n",
      "        [-0.0312,  0.0138,  0.0269,  ..., -0.0518,  0.0018,  0.0320],\n",
      "        [-0.0121, -0.0339,  0.0649,  ...,  0.0593,  0.0398, -0.0371]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.self_attn.k_proj.weight tensor([[-0.0085,  0.0123, -0.0120,  ..., -0.0182, -0.0030, -0.0001],\n",
      "        [-0.0295, -0.0049,  0.0010,  ..., -0.0027,  0.0132,  0.0052],\n",
      "        [ 0.0166, -0.0002, -0.0141,  ...,  0.0146,  0.0184,  0.0371],\n",
      "        ...,\n",
      "        [-0.0098,  0.1206,  0.0586,  ...,  0.0283, -0.0197, -0.0114],\n",
      "        [ 0.0337,  0.0500, -0.0183,  ...,  0.0547, -0.0400,  0.0684],\n",
      "        [ 0.0091,  0.0610,  0.0165,  ..., -0.0026,  0.0454, -0.0168]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.self_attn.v_proj.weight tensor([[ 0.0006, -0.0016, -0.0080,  ..., -0.0156, -0.0075, -0.0004],\n",
      "        [-0.0034,  0.0249,  0.0068,  ...,  0.0088,  0.0017,  0.0110],\n",
      "        [ 0.0140,  0.0012, -0.0366,  ...,  0.0078, -0.0053, -0.0223],\n",
      "        ...,\n",
      "        [-0.0019, -0.0094, -0.0212,  ..., -0.0130, -0.0074,  0.0022],\n",
      "        [-0.0036, -0.0063, -0.0006,  ...,  0.0022, -0.0053,  0.0081],\n",
      "        [-0.0028, -0.0192, -0.0016,  ...,  0.0073, -0.0026,  0.0150]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.self_attn.o_proj.weight tensor([[ 3.4424e-02,  9.8877e-03, -1.9379e-03,  ...,  1.1292e-03,\n",
      "         -2.5146e-02, -3.7384e-03],\n",
      "        [ 4.0283e-03, -1.2517e-06, -6.1035e-03,  ...,  4.5471e-03,\n",
      "         -2.0386e-02,  3.6774e-03],\n",
      "        [-1.3199e-03,  5.5542e-03, -1.0254e-02,  ..., -1.0925e-02,\n",
      "          8.1177e-03, -3.5400e-03],\n",
      "        ...,\n",
      "        [-1.0132e-02,  3.0884e-02, -1.5869e-02,  ..., -2.4780e-02,\n",
      "          3.6011e-03, -1.4343e-02],\n",
      "        [-1.3657e-03, -4.7913e-03, -1.4954e-02,  ..., -6.5613e-03,\n",
      "         -1.2054e-03, -7.3547e-03],\n",
      "        [ 1.1597e-02,  2.2217e-02, -7.1716e-03,  ..., -3.0060e-03,\n",
      "          1.4221e-02,  5.3883e-05]], dtype=torch.float16)\n",
      "model.layers.4.mlp.gate_proj.weight tensor([[-2.4109e-03,  8.7280e-03, -1.1414e-02,  ..., -1.5747e-02,\n",
      "         -7.3314e-06, -1.4709e-02],\n",
      "        [-2.4658e-02,  1.2756e-02,  1.1353e-02,  ...,  5.1575e-03,\n",
      "          1.3367e-02, -6.3171e-03],\n",
      "        [-8.1177e-03, -2.1240e-02, -3.8300e-03,  ...,  2.1362e-03,\n",
      "         -3.4027e-03,  2.6123e-02],\n",
      "        ...,\n",
      "        [-1.1230e-02, -3.4180e-03,  1.0803e-02,  ..., -2.7100e-02,\n",
      "          4.3335e-03, -1.9455e-03],\n",
      "        [-3.0151e-02,  1.7456e-02, -2.8442e-02,  ...,  1.6357e-02,\n",
      "          2.4780e-02, -5.0049e-03],\n",
      "        [ 6.9427e-04, -1.0498e-02, -1.0437e-02,  ..., -1.1536e-02,\n",
      "          1.8066e-02,  1.6113e-02]], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.4.mlp.down_proj.weight tensor([[ 0.0233, -0.0024,  0.0260,  ..., -0.0194,  0.0047,  0.0315],\n",
      "        [-0.0064, -0.0449,  0.0403,  ..., -0.0238, -0.0201, -0.0051],\n",
      "        [ 0.0004, -0.0173,  0.0229,  ...,  0.0082, -0.0018,  0.0188],\n",
      "        ...,\n",
      "        [-0.0275, -0.0034, -0.0171,  ...,  0.0216,  0.0016,  0.0139],\n",
      "        [-0.0161,  0.0088, -0.0271,  ...,  0.0143, -0.0344, -0.0060],\n",
      "        [ 0.0178, -0.0286, -0.0151,  ..., -0.0032, -0.0457, -0.0269]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.mlp.up_proj.weight tensor([[-0.0136, -0.0076, -0.0422,  ...,  0.0114,  0.0045, -0.0026],\n",
      "        [-0.0206, -0.0420,  0.0066,  ..., -0.0103,  0.0115, -0.0250],\n",
      "        [-0.0016,  0.0317,  0.0039,  ..., -0.0364, -0.0150,  0.0045],\n",
      "        ...,\n",
      "        [-0.0069, -0.0315, -0.0165,  ...,  0.0079, -0.0085,  0.0491],\n",
      "        [ 0.0099, -0.0074, -0.0152,  ..., -0.0417, -0.0086,  0.0120],\n",
      "        [ 0.0011,  0.0223,  0.0039,  ...,  0.0009, -0.0008, -0.0032]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.input_layernorm.weight tensor([0.2617, 0.2578, 0.2598,  ..., 0.2559, 0.2637, 0.2715],\n",
      "       dtype=torch.float16)\n",
      "model.layers.4.post_attention_layernorm.weight tensor([0.1885, 0.1865, 0.1816,  ..., 0.1885, 0.1865, 0.1855],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.self_attn.q_proj.weight tensor([[-0.0064, -0.0048, -0.0155,  ..., -0.0111, -0.0221,  0.0144],\n",
      "        [-0.0242,  0.0145,  0.0396,  ..., -0.0109, -0.0161, -0.0330],\n",
      "        [ 0.0098,  0.0084, -0.0110,  ...,  0.0253, -0.0098,  0.0084],\n",
      "        ...,\n",
      "        [-0.0109,  0.0062, -0.0049,  ...,  0.0474,  0.0056,  0.0325],\n",
      "        [ 0.0369,  0.0152,  0.0189,  ..., -0.0413, -0.0349, -0.0312],\n",
      "        [ 0.0087,  0.0220,  0.0322,  ..., -0.0312, -0.0166,  0.0009]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.self_attn.k_proj.weight tensor([[-0.0145,  0.0295,  0.0062,  ...,  0.0155, -0.0024, -0.0049],\n",
      "        [ 0.0015, -0.0009, -0.0298,  ...,  0.0187, -0.0081,  0.0344],\n",
      "        [-0.0082, -0.0168,  0.0007,  ...,  0.0046, -0.0104,  0.0037],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0007, -0.0234,  ...,  0.0371,  0.0079, -0.0299],\n",
      "        [ 0.0542,  0.0209, -0.0181,  ..., -0.0014, -0.0156, -0.0361],\n",
      "        [-0.0282, -0.0242,  0.0405,  ..., -0.0176, -0.0332,  0.0099]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.self_attn.v_proj.weight tensor([[-0.0359,  0.0062,  0.0143,  ...,  0.0184,  0.0015,  0.0188],\n",
      "        [ 0.0062,  0.0146, -0.0262,  ..., -0.0153,  0.0033, -0.0092],\n",
      "        [ 0.0177,  0.0038,  0.0007,  ..., -0.0206,  0.0292,  0.0041],\n",
      "        ...,\n",
      "        [ 0.0243,  0.0126,  0.0060,  ...,  0.0361, -0.0084, -0.0013],\n",
      "        [-0.0036,  0.0008, -0.0052,  ...,  0.0004, -0.0012, -0.0026],\n",
      "        [-0.0076, -0.0090,  0.0210,  ..., -0.0070, -0.0042, -0.0206]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.self_attn.o_proj.weight tensor([[-0.0133, -0.0060,  0.0039,  ..., -0.0151, -0.0109,  0.0244],\n",
      "        [ 0.0022,  0.0017, -0.0126,  ..., -0.0177,  0.0021, -0.0007],\n",
      "        [-0.0084, -0.0069,  0.0143,  ...,  0.0078,  0.0040,  0.0222],\n",
      "        ...,\n",
      "        [ 0.0178,  0.0057, -0.0047,  ...,  0.0177,  0.0339, -0.0219],\n",
      "        [ 0.0050, -0.0044, -0.0026,  ..., -0.0152, -0.0082, -0.0037],\n",
      "        [ 0.0059,  0.0025, -0.0125,  ...,  0.0075, -0.0084, -0.0195]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.mlp.gate_proj.weight tensor([[ 0.0014,  0.0003, -0.0299,  ..., -0.0003, -0.0052, -0.0159],\n",
      "        [ 0.0210, -0.0150, -0.0116,  ...,  0.0287, -0.0008, -0.0022],\n",
      "        [ 0.0106,  0.0057, -0.0153,  ...,  0.0208,  0.0181, -0.0315],\n",
      "        ...,\n",
      "        [ 0.0041,  0.0068,  0.0415,  ..., -0.0688,  0.0125, -0.0215],\n",
      "        [-0.0297,  0.0034,  0.0121,  ...,  0.0047,  0.0193, -0.0057],\n",
      "        [ 0.0292, -0.0339, -0.0148,  ..., -0.0040, -0.0018, -0.0415]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.mlp.down_proj.weight tensor([[-0.0054, -0.0286, -0.0033,  ..., -0.0027,  0.0071,  0.0140],\n",
      "        [-0.0160,  0.0087,  0.0227,  ...,  0.0221, -0.0046, -0.0309],\n",
      "        [ 0.0177,  0.0228,  0.0120,  ..., -0.0294, -0.0452,  0.0074],\n",
      "        ...,\n",
      "        [ 0.0087, -0.0242, -0.0019,  ..., -0.0018, -0.0248, -0.0204],\n",
      "        [-0.0084, -0.0369,  0.0154,  ...,  0.0148,  0.0089, -0.0515],\n",
      "        [ 0.0164,  0.0105, -0.0164,  ...,  0.0322,  0.0088, -0.0140]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.mlp.up_proj.weight tensor([[-0.0041, -0.0155, -0.0013,  ..., -0.0135, -0.0055, -0.0125],\n",
      "        [-0.0344, -0.0035, -0.0006,  ...,  0.0078,  0.0225,  0.0222],\n",
      "        [-0.0001,  0.0112,  0.0062,  ...,  0.0045,  0.0126,  0.0171],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0256, -0.0074,  ..., -0.0137,  0.0074,  0.0291],\n",
      "        [ 0.0461, -0.0247, -0.0239,  ..., -0.0047, -0.0160,  0.0234],\n",
      "        [ 0.0093,  0.0223,  0.0066,  ..., -0.0096, -0.0034, -0.0212]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.input_layernorm.weight tensor([0.2637, 0.2637, 0.2598,  ..., 0.2520, 0.2676, 0.2695],\n",
      "       dtype=torch.float16)\n",
      "model.layers.5.post_attention_layernorm.weight tensor([0.2041, 0.1934, 0.1895,  ..., 0.2051, 0.1992, 0.2031],\n",
      "       dtype=torch.float16)\n",
      "model.layers.6.self_attn.q_proj.weight tensor([[-0.0058, -0.0055,  0.0051,  ..., -0.0325, -0.0019, -0.0033],\n",
      "        [-0.0003, -0.0167,  0.0098,  ...,  0.0123, -0.0030, -0.0289],\n",
      "        [-0.0167,  0.0204, -0.0337,  ...,  0.0259, -0.0432, -0.0072],\n",
      "        ...,\n",
      "        [ 0.0157,  0.0098, -0.0349,  ...,  0.0210,  0.0059, -0.0055],\n",
      "        [ 0.0325,  0.0374,  0.0060,  ..., -0.0104, -0.0913, -0.0193],\n",
      "        [-0.0620, -0.0537, -0.0002,  ...,  0.0168,  0.0021,  0.0118]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.6.self_attn.k_proj.weight tensor([[ 1.4771e-02, -1.3367e-02,  1.9531e-02,  ..., -7.8125e-03,\n",
      "          1.9775e-02,  5.5420e-02],\n",
      "        [-1.6357e-02, -5.2795e-03,  4.5654e-02,  ...,  7.0801e-03,\n",
      "         -1.7578e-02,  1.1658e-02],\n",
      "        [-1.0010e-02, -1.3306e-02, -8.3542e-04,  ..., -2.6245e-03,\n",
      "         -2.6978e-02, -1.2939e-02],\n",
      "        ...,\n",
      "        [ 1.7578e-02, -3.8330e-02, -1.5991e-02,  ...,  5.7861e-02,\n",
      "          5.1498e-05, -4.5654e-02],\n",
      "        [-1.9897e-02,  4.7302e-03, -2.9175e-02,  ..., -4.6875e-02,\n",
      "         -4.1504e-02,  1.2451e-02],\n",
      "        [-4.8340e-02, -2.5757e-02, -3.5156e-02,  ...,  1.6235e-02,\n",
      "         -4.0039e-02, -2.5024e-02]], dtype=torch.float16)\n",
      "model.layers.6.self_attn.v_proj.weight tensor([[ 1.2512e-02,  3.4668e-02,  6.0120e-03,  ...,  1.2390e-02,\n",
      "          2.5635e-03,  5.0049e-03],\n",
      "        [ 3.6133e-02, -6.9046e-04, -7.0496e-03,  ...,  2.8687e-03,\n",
      "          1.7319e-03,  1.3580e-03],\n",
      "        [-8.7280e-03, -5.5542e-03,  1.0925e-02,  ...,  7.2937e-03,\n",
      "          2.1118e-02,  1.8501e-04],\n",
      "        ...,\n",
      "        [-2.2339e-02, -5.7983e-03,  1.0620e-02,  ..., -1.2054e-03,\n",
      "         -1.6113e-02,  6.4392e-03],\n",
      "        [-2.1240e-02, -9.4604e-03,  4.0588e-03,  ..., -4.0894e-03,\n",
      "         -1.0681e-04,  2.2217e-02],\n",
      "        [ 3.5400e-02,  6.2866e-03,  1.8433e-02,  ..., -2.0874e-02,\n",
      "         -5.5552e-05, -2.4719e-03]], dtype=torch.float16)\n",
      "model.layers.6.self_attn.o_proj.weight tensor([[ 0.0138, -0.0155, -0.0381,  ..., -0.0105,  0.0033,  0.0093],\n",
      "        [-0.0209,  0.0057,  0.0029,  ..., -0.0187,  0.0038, -0.0036],\n",
      "        [ 0.0018, -0.0040, -0.0040,  ...,  0.0115,  0.0025,  0.0098],\n",
      "        ...,\n",
      "        [-0.0013, -0.0146, -0.0128,  ...,  0.0076, -0.0063,  0.0212],\n",
      "        [-0.0226,  0.0081,  0.0019,  ..., -0.0004,  0.0051, -0.0251],\n",
      "        [-0.0187, -0.0164,  0.0064,  ..., -0.0168,  0.0072,  0.0100]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.6.mlp.gate_proj.weight tensor([[ 0.0211,  0.0251, -0.0148,  ..., -0.0062, -0.0095, -0.0081],\n",
      "        [ 0.0339, -0.0052, -0.0139,  ...,  0.0070,  0.0208,  0.0075],\n",
      "        [-0.0198,  0.0226, -0.0143,  ..., -0.0221, -0.0017,  0.0092],\n",
      "        ...,\n",
      "        [ 0.0074, -0.0194, -0.0026,  ...,  0.0150, -0.0255, -0.0039],\n",
      "        [ 0.0115, -0.0020, -0.0166,  ..., -0.0250, -0.0045, -0.0028],\n",
      "        [-0.0315, -0.0500,  0.0239,  ..., -0.0417,  0.0018,  0.0206]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.6.mlp.down_proj.weight tensor([[-0.0089,  0.0183, -0.0028,  ..., -0.0009,  0.0136,  0.0168],\n",
      "        [-0.0153, -0.0151,  0.0088,  ..., -0.0023, -0.0278, -0.0112],\n",
      "        [-0.0015,  0.0167, -0.0127,  ...,  0.0260,  0.0244,  0.0150],\n",
      "        ...,\n",
      "        [ 0.0062, -0.0042,  0.0247,  ...,  0.0311, -0.0302, -0.0044],\n",
      "        [ 0.0203, -0.0048,  0.0090,  ...,  0.0107, -0.0143, -0.0110],\n",
      "        [ 0.0161, -0.0178, -0.0049,  ...,  0.0322, -0.0068,  0.0057]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.6.mlp.up_proj.weight tensor([[-0.0082, -0.0091, -0.0137,  ...,  0.0074, -0.0046, -0.0160],\n",
      "        [ 0.0069, -0.0432,  0.0366,  ...,  0.0030,  0.0025, -0.0078],\n",
      "        [-0.0255, -0.0200, -0.0212,  ...,  0.0066,  0.0192,  0.0239],\n",
      "        ...,\n",
      "        [-0.0269,  0.0030,  0.0199,  ...,  0.0175,  0.0014,  0.0425],\n",
      "        [ 0.0232,  0.0151, -0.0012,  ..., -0.0281,  0.0164, -0.0052],\n",
      "        [-0.0201,  0.0223, -0.0225,  ...,  0.0064, -0.0156, -0.0459]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.6.input_layernorm.weight tensor([0.3184, 0.3555, 0.3281,  ..., 0.3164, 0.3359, 0.3203],\n",
      "       dtype=torch.float16)\n",
      "model.layers.6.post_attention_layernorm.weight tensor([0.2168, 0.2061, 0.2041,  ..., 0.2178, 0.2109, 0.2129],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.self_attn.q_proj.weight tensor([[ 6.8970e-03,  2.3804e-03,  3.3569e-03,  ...,  4.0283e-03,\n",
      "         -1.6479e-02,  3.4094e-05],\n",
      "        [-5.4016e-03,  6.1035e-03,  2.3956e-03,  ...,  1.5747e-02,\n",
      "         -1.0559e-02, -1.2146e-02],\n",
      "        [ 1.0605e-03,  7.9346e-03, -1.9531e-02,  ..., -1.8311e-02,\n",
      "         -1.2054e-03,  9.0942e-03],\n",
      "        ...,\n",
      "        [ 1.4191e-03, -3.7354e-02, -2.9785e-02,  ..., -1.5137e-02,\n",
      "          3.2959e-02,  3.6621e-02],\n",
      "        [ 1.0559e-02,  5.4932e-02, -4.1504e-02,  ..., -1.8555e-02,\n",
      "         -6.6406e-02, -5.2979e-02],\n",
      "        [ 8.0078e-02, -2.3926e-02, -1.3123e-02,  ...,  8.8867e-02,\n",
      "          3.6133e-02, -5.7983e-03]], dtype=torch.float16)\n",
      "model.layers.7.self_attn.k_proj.weight tensor([[-0.0029, -0.0063, -0.0132,  ...,  0.0085,  0.0039, -0.0046],\n",
      "        [-0.0042, -0.0155,  0.0042,  ..., -0.0061,  0.0027,  0.0220],\n",
      "        [-0.0055, -0.0066,  0.0183,  ..., -0.0079, -0.0117, -0.0178],\n",
      "        ...,\n",
      "        [ 0.0430,  0.0104,  0.0215,  ...,  0.0369,  0.0386,  0.0276],\n",
      "        [ 0.0425,  0.0540, -0.0020,  ..., -0.0337, -0.0332, -0.0034],\n",
      "        [ 0.0334,  0.0084,  0.0109,  ...,  0.0095,  0.0055, -0.0280]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.self_attn.v_proj.weight tensor([[-0.0150,  0.0020,  0.0162,  ...,  0.0266,  0.0059, -0.0134],\n",
      "        [ 0.0289, -0.0306, -0.0019,  ...,  0.0189, -0.0172, -0.0214],\n",
      "        [-0.0076, -0.0059, -0.0049,  ..., -0.0155,  0.0096,  0.0031],\n",
      "        ...,\n",
      "        [-0.0011, -0.0383,  0.0034,  ...,  0.0042, -0.0049, -0.0094],\n",
      "        [-0.0325,  0.0164, -0.0270,  ..., -0.0128,  0.0135, -0.0001],\n",
      "        [-0.0101, -0.0010, -0.0070,  ...,  0.0247,  0.0078,  0.0131]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.self_attn.o_proj.weight tensor([[ 0.0148, -0.0164, -0.0050,  ...,  0.0072, -0.0253,  0.0022],\n",
      "        [-0.0020,  0.0242,  0.0110,  ..., -0.0425, -0.0121, -0.0072],\n",
      "        [ 0.0050,  0.0052, -0.0206,  ..., -0.0102, -0.0111, -0.0081],\n",
      "        ...,\n",
      "        [-0.0172, -0.0272, -0.0061,  ...,  0.0234, -0.0203, -0.0004],\n",
      "        [-0.0197,  0.0081,  0.0116,  ...,  0.0078, -0.0129,  0.0145],\n",
      "        [-0.0004,  0.0042,  0.0016,  ...,  0.0080, -0.0145,  0.0059]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.mlp.gate_proj.weight tensor([[ 0.0052, -0.0012, -0.0152,  ..., -0.0053, -0.0311, -0.0062],\n",
      "        [-0.0293,  0.0178, -0.0076,  ..., -0.0014, -0.0101, -0.0065],\n",
      "        [ 0.0172, -0.0304, -0.0077,  ...,  0.0303, -0.0194,  0.0061],\n",
      "        ...,\n",
      "        [-0.0058,  0.0057, -0.0120,  ..., -0.0281, -0.0037, -0.0276],\n",
      "        [-0.0036,  0.0204,  0.0115,  ..., -0.0059,  0.0052, -0.0330],\n",
      "        [ 0.0192, -0.0175, -0.0046,  ...,  0.0161,  0.0194, -0.0007]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.mlp.down_proj.weight tensor([[-8.5449e-03, -1.2085e-02,  9.9945e-04,  ..., -1.3489e-02,\n",
      "         -8.3008e-03, -2.0630e-02],\n",
      "        [-2.8839e-03, -6.2256e-03,  1.9409e-02,  ..., -4.6082e-03,\n",
      "          5.0354e-03,  1.7700e-03],\n",
      "        [ 1.1841e-02,  1.3672e-02,  3.2715e-02,  ..., -1.6479e-02,\n",
      "         -9.3384e-03, -1.4099e-02],\n",
      "        ...,\n",
      "        [-5.8289e-03, -1.8005e-03, -2.1240e-02,  ...,  1.7578e-02,\n",
      "          1.7090e-03, -2.6733e-02],\n",
      "        [ 4.2969e-02, -1.1108e-02,  3.6316e-03,  ..., -1.8555e-02,\n",
      "          5.7602e-04, -1.3245e-02],\n",
      "        [ 1.5991e-02,  1.8921e-02, -3.3447e-02,  ...,  3.9368e-03,\n",
      "         -2.3804e-02, -2.0862e-05]], dtype=torch.float16)\n",
      "model.layers.7.mlp.up_proj.weight tensor([[ 0.0211,  0.0007, -0.0019,  ...,  0.0154, -0.0352,  0.0039],\n",
      "        [-0.0113, -0.0219,  0.0305,  ...,  0.0019, -0.0132,  0.0195],\n",
      "        [ 0.0214, -0.0159,  0.0087,  ..., -0.0114,  0.0190, -0.0303],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0002, -0.0179,  ..., -0.0041, -0.0228,  0.0028],\n",
      "        [ 0.0008,  0.0079,  0.0172,  ..., -0.0127,  0.0184, -0.0012],\n",
      "        [-0.0243, -0.0159,  0.0062,  ...,  0.0178,  0.0106,  0.0049]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.input_layernorm.weight tensor([0.3223, 0.3652, 0.3379,  ..., 0.3242, 0.3574, 0.3301],\n",
      "       dtype=torch.float16)\n",
      "model.layers.7.post_attention_layernorm.weight tensor([0.2314, 0.2158, 0.2178,  ..., 0.2256, 0.2246, 0.2236],\n",
      "       dtype=torch.float16)\n",
      "model.layers.8.self_attn.q_proj.weight tensor([[ 7.9956e-03, -1.0925e-02, -2.5024e-02,  ..., -5.2795e-03,\n",
      "          2.9602e-03,  2.9144e-03],\n",
      "        [-1.9775e-02, -1.4954e-02, -3.4180e-02,  ..., -3.2349e-03,\n",
      "         -1.0529e-03, -3.7231e-03],\n",
      "        [-3.1128e-02, -2.3956e-03, -1.0376e-02,  ...,  4.6387e-03,\n",
      "          1.7578e-02, -1.9043e-02],\n",
      "        ...,\n",
      "        [-6.8359e-03,  8.3984e-02,  4.2480e-02,  ..., -2.8564e-02,\n",
      "         -2.7344e-02, -4.5410e-02],\n",
      "        [ 1.5378e-05, -6.2988e-02, -1.6479e-02,  ...,  3.6377e-02,\n",
      "          2.0142e-02,  1.8616e-03],\n",
      "        [-6.2988e-02, -5.7617e-02, -1.3916e-02,  ..., -4.7363e-02,\n",
      "          2.9419e-02, -1.8311e-02]], dtype=torch.float16)\n",
      "model.layers.8.self_attn.k_proj.weight tensor([[-0.0064,  0.0045, -0.0060,  ..., -0.0057,  0.0060,  0.0098],\n",
      "        [-0.0068,  0.0056, -0.0070,  ...,  0.0243, -0.0017,  0.0154],\n",
      "        [-0.0113,  0.0023,  0.0052,  ...,  0.0018, -0.0148, -0.0171],\n",
      "        ...,\n",
      "        [-0.0027, -0.0378, -0.0076,  ..., -0.0322, -0.0154,  0.0276],\n",
      "        [ 0.0035,  0.0249,  0.0256,  ..., -0.0376,  0.0444, -0.0576],\n",
      "        [ 0.0154, -0.0047, -0.0330,  ...,  0.0226,  0.0095, -0.0125]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.8.self_attn.v_proj.weight tensor([[-0.0260, -0.0129,  0.0044,  ..., -0.0159, -0.0114, -0.0079],\n",
      "        [-0.0226, -0.0135,  0.0071,  ...,  0.0270,  0.0047,  0.0198],\n",
      "        [-0.0052,  0.0162,  0.0125,  ..., -0.0034,  0.0074, -0.0208],\n",
      "        ...,\n",
      "        [ 0.0295, -0.0159,  0.0030,  ...,  0.0220, -0.0175,  0.0221],\n",
      "        [ 0.0261, -0.0156,  0.0023,  ...,  0.0079, -0.0080,  0.0007],\n",
      "        [ 0.0165,  0.0001, -0.0055,  ..., -0.0021,  0.0041, -0.0066]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.8.self_attn.o_proj.weight tensor([[ 1.3611e-02,  1.3184e-02, -1.3306e-02,  ..., -1.0498e-02,\n",
      "          1.3046e-03,  1.5747e-02],\n",
      "        [ 1.0071e-02,  4.2725e-03, -3.2471e-02,  ..., -6.6223e-03,\n",
      "         -9.5215e-03, -1.3855e-02],\n",
      "        [ 1.4954e-03,  1.0681e-02, -3.9062e-03,  ...,  1.1292e-03,\n",
      "          1.4771e-02,  2.1118e-02],\n",
      "        ...,\n",
      "        [-1.7090e-02,  2.4048e-02, -6.9885e-03,  ..., -9.3937e-05,\n",
      "         -1.9836e-03, -2.1362e-03],\n",
      "        [ 4.1809e-03, -9.6321e-05, -4.1809e-03,  ...,  1.3199e-03,\n",
      "          7.7515e-03, -2.2461e-02],\n",
      "        [-2.6733e-02,  6.4087e-03, -7.9346e-03,  ...,  2.2217e-02,\n",
      "         -9.7656e-03, -2.6703e-03]], dtype=torch.float16)\n",
      "model.layers.8.mlp.gate_proj.weight tensor([[ 0.0112,  0.0041, -0.0420,  ..., -0.0070, -0.0259, -0.0231],\n",
      "        [-0.0029, -0.0432,  0.0104,  ..., -0.0012, -0.0464,  0.0201],\n",
      "        [ 0.0183,  0.0167,  0.0231,  ...,  0.0247,  0.0222, -0.0117],\n",
      "        ...,\n",
      "        [-0.0131,  0.0030,  0.0073,  ...,  0.0242,  0.0146,  0.0056],\n",
      "        [-0.0129, -0.0012,  0.0070,  ..., -0.0010, -0.0231,  0.0251],\n",
      "        [-0.0248,  0.0056, -0.0048,  ..., -0.0146,  0.0119, -0.0010]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.8.mlp.down_proj.weight tensor([[-0.0109, -0.0055,  0.0183,  ...,  0.0096,  0.0140, -0.0236],\n",
      "        [ 0.0142,  0.0132, -0.0208,  ..., -0.0062,  0.0070, -0.0231],\n",
      "        [ 0.0015, -0.0229, -0.0269,  ..., -0.0129,  0.0058, -0.0066],\n",
      "        ...,\n",
      "        [ 0.0150, -0.0210, -0.0359,  ...,  0.0032,  0.0054, -0.0175],\n",
      "        [ 0.0151,  0.0040, -0.0063,  ..., -0.0034, -0.0070, -0.0176],\n",
      "        [-0.0002,  0.0133,  0.0006,  ...,  0.0012,  0.0026,  0.0012]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.8.mlp.up_proj.weight tensor([[-0.0103,  0.0176, -0.0146,  ...,  0.0292,  0.0072, -0.0103],\n",
      "        [-0.0496,  0.0175, -0.0420,  ...,  0.0146, -0.0033, -0.0064],\n",
      "        [ 0.0104,  0.0045,  0.0087,  ..., -0.0139, -0.0199,  0.0298],\n",
      "        ...,\n",
      "        [-0.0142,  0.0393, -0.0187,  ...,  0.0145, -0.0090,  0.0094],\n",
      "        [-0.0001, -0.0046, -0.0079,  ..., -0.0161,  0.0204, -0.0153],\n",
      "        [-0.0145,  0.0161, -0.0142,  ..., -0.0155,  0.0015,  0.0033]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.8.input_layernorm.weight tensor([0.3320, 0.3457, 0.3301,  ..., 0.3203, 0.3438, 0.3223],\n",
      "       dtype=torch.float16)\n",
      "model.layers.8.post_attention_layernorm.weight tensor([0.2383, 0.2236, 0.2178,  ..., 0.2363, 0.2285, 0.2256],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.self_attn.q_proj.weight tensor([[-0.0194,  0.0025,  0.0168,  ..., -0.0049, -0.0095, -0.0164],\n",
      "        [ 0.0022, -0.0349,  0.0134,  ...,  0.0051, -0.0066,  0.0017],\n",
      "        [ 0.0023, -0.0017, -0.0148,  ..., -0.0223, -0.0299, -0.0223],\n",
      "        ...,\n",
      "        [ 0.0037, -0.0452,  0.0189,  ...,  0.0242,  0.0159, -0.0205],\n",
      "        [ 0.0146, -0.0212,  0.0110,  ...,  0.0099,  0.0139,  0.0097],\n",
      "        [-0.0010,  0.0547,  0.0197,  ..., -0.0557,  0.0488, -0.0043]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.self_attn.k_proj.weight tensor([[-0.0347,  0.0101, -0.0167,  ...,  0.0084, -0.0195, -0.0075],\n",
      "        [-0.0057,  0.0325,  0.0094,  ..., -0.0108,  0.0100,  0.0022],\n",
      "        [-0.0118, -0.0110,  0.0189,  ..., -0.0047, -0.0124, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0114, -0.0173, -0.0068,  ..., -0.0115,  0.0219,  0.0222],\n",
      "        [ 0.0325, -0.0101,  0.0215,  ...,  0.0452, -0.0121,  0.0515],\n",
      "        [ 0.0061, -0.0264,  0.0302,  ...,  0.0095,  0.0256, -0.0223]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.self_attn.v_proj.weight tensor([[-0.0032,  0.0104, -0.0009,  ...,  0.0075, -0.0212, -0.0184],\n",
      "        [-0.0164, -0.0175, -0.0008,  ...,  0.0298, -0.0009, -0.0031],\n",
      "        [ 0.0023, -0.0259,  0.0205,  ..., -0.0261,  0.0068, -0.0112],\n",
      "        ...,\n",
      "        [-0.0069, -0.0131,  0.0044,  ...,  0.0035, -0.0223, -0.0006],\n",
      "        [-0.0149,  0.0032, -0.0104,  ..., -0.0121,  0.0001, -0.0128],\n",
      "        [ 0.0236,  0.0206, -0.0018,  ..., -0.0147, -0.0059, -0.0038]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.self_attn.o_proj.weight tensor([[ 0.0096,  0.0106,  0.0052,  ..., -0.0104,  0.0010, -0.0020],\n",
      "        [-0.0051, -0.0149, -0.0023,  ..., -0.0002,  0.0073,  0.0145],\n",
      "        [-0.0337, -0.0315,  0.0118,  ...,  0.0264,  0.0014,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0164, -0.0147,  0.0123,  ...,  0.0077,  0.0068, -0.0090],\n",
      "        [ 0.0012,  0.0179,  0.0023,  ...,  0.0128,  0.0001, -0.0013],\n",
      "        [ 0.0140,  0.0505, -0.0205,  ..., -0.0042,  0.0074,  0.0063]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.mlp.gate_proj.weight tensor([[-0.0079,  0.0264, -0.0068,  ..., -0.0249, -0.0106,  0.0023],\n",
      "        [-0.0248,  0.0275,  0.0291,  ...,  0.0292,  0.0312, -0.0035],\n",
      "        [ 0.0376,  0.0291, -0.0243,  ..., -0.0264,  0.0111, -0.0041],\n",
      "        ...,\n",
      "        [-0.0134, -0.0133, -0.0293,  ..., -0.0011, -0.0403,  0.0045],\n",
      "        [-0.0137, -0.0240,  0.0237,  ..., -0.0147,  0.0021,  0.0135],\n",
      "        [ 0.0118, -0.0176, -0.0099,  ...,  0.0129, -0.0148,  0.0104]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.mlp.down_proj.weight tensor([[-0.0045,  0.0249, -0.0415,  ...,  0.0004, -0.0145,  0.0103],\n",
      "        [ 0.0253, -0.0215, -0.0184,  ..., -0.0013, -0.0277, -0.0312],\n",
      "        [-0.0017,  0.0040,  0.0015,  ..., -0.0154,  0.0021, -0.0029],\n",
      "        ...,\n",
      "        [-0.0096,  0.0190,  0.0011,  ..., -0.0006, -0.0299,  0.0079],\n",
      "        [-0.0347,  0.0008,  0.0141,  ..., -0.0251, -0.0008, -0.0320],\n",
      "        [ 0.0220,  0.0173,  0.0149,  ...,  0.0435,  0.0166,  0.0181]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.mlp.up_proj.weight tensor([[-0.0128,  0.0117,  0.0121,  ..., -0.0005, -0.0053,  0.0116],\n",
      "        [ 0.0100, -0.0020,  0.0014,  ...,  0.0146,  0.0178, -0.0204],\n",
      "        [-0.0205, -0.0093, -0.0200,  ..., -0.0038, -0.0099, -0.0031],\n",
      "        ...,\n",
      "        [-0.0037,  0.0098, -0.0131,  ..., -0.0069, -0.0300, -0.0192],\n",
      "        [-0.0004,  0.0176, -0.0315,  ..., -0.0317, -0.0099, -0.0153],\n",
      "        [ 0.0011, -0.0354, -0.0129,  ..., -0.0134,  0.0237, -0.0002]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.input_layernorm.weight tensor([0.3516, 0.3594, 0.3223,  ..., 0.3496, 0.3457, 0.3398],\n",
      "       dtype=torch.float16)\n",
      "model.layers.9.post_attention_layernorm.weight tensor([0.2422, 0.2305, 0.2217,  ..., 0.2373, 0.2363, 0.2324],\n",
      "       dtype=torch.float16)\n",
      "model.layers.10.self_attn.q_proj.weight tensor([[-1.4114e-03,  4.8218e-03, -1.6479e-03,  ...,  1.0193e-02,\n",
      "         -4.2419e-03, -8.3618e-03],\n",
      "        [-4.0588e-03, -7.3853e-03,  1.6602e-02,  ..., -6.0120e-03,\n",
      "          5.9814e-02,  8.7261e-05],\n",
      "        [-9.9487e-03, -8.6060e-03, -5.4016e-03,  ...,  2.0386e-02,\n",
      "         -1.5320e-02, -1.7090e-02],\n",
      "        ...,\n",
      "        [ 3.2959e-02,  6.3965e-02, -1.5198e-02,  ...,  3.0762e-02,\n",
      "          3.6621e-02, -8.3008e-03],\n",
      "        [-5.9082e-02,  5.7617e-02, -2.0409e-04,  ..., -6.8359e-02,\n",
      "         -6.8665e-04, -8.0566e-03],\n",
      "        [-4.9805e-02, -2.1606e-02, -4.9744e-03,  ..., -9.5215e-03,\n",
      "         -3.2471e-02, -1.8311e-02]], dtype=torch.float16)\n",
      "model.layers.10.self_attn.k_proj.weight tensor([[-2.2339e-02,  7.4463e-03, -5.5542e-03,  ..., -1.0193e-02,\n",
      "          4.5471e-03, -4.3335e-03],\n",
      "        [-1.0803e-02,  7.2937e-03, -5.9509e-03,  ...,  1.0559e-02,\n",
      "         -1.2024e-02,  7.5684e-03],\n",
      "        [-5.0964e-03,  9.3842e-04,  6.5308e-03,  ..., -1.5991e-02,\n",
      "          2.3071e-02, -4.2915e-06],\n",
      "        ...,\n",
      "        [-4.4922e-02, -5.9326e-02,  7.8125e-03,  ...,  3.1250e-02,\n",
      "         -7.8125e-02,  8.9111e-03],\n",
      "        [-3.7109e-02, -7.2937e-03,  2.9541e-02,  ..., -1.9165e-02,\n",
      "         -4.4189e-02, -2.2217e-02],\n",
      "        [-2.8564e-02,  2.6733e-02, -2.1240e-02,  ...,  1.5625e-02,\n",
      "          4.8340e-02, -9.1553e-03]], dtype=torch.float16)\n",
      "model.layers.10.self_attn.v_proj.weight tensor([[-0.0400,  0.0035, -0.0065,  ...,  0.0071,  0.0110,  0.0103],\n",
      "        [-0.0080,  0.0132,  0.0123,  ..., -0.0078, -0.0388,  0.0187],\n",
      "        [ 0.0149, -0.0125,  0.0152,  ...,  0.0155,  0.0065, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0231,  0.0291,  0.0002,  ...,  0.0282, -0.0052, -0.0159],\n",
      "        [ 0.0025,  0.0078,  0.0007,  ...,  0.0018,  0.0117,  0.0001],\n",
      "        [ 0.0143,  0.0087, -0.0153,  ..., -0.0046, -0.0125,  0.0299]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.10.self_attn.o_proj.weight tensor([[-0.0030,  0.0015, -0.0315,  ..., -0.0005,  0.0104,  0.0082],\n",
      "        [ 0.0010, -0.0121,  0.0126,  ...,  0.0035,  0.0138,  0.0047],\n",
      "        [ 0.0132, -0.0361, -0.0200,  ...,  0.0085, -0.0094, -0.0055],\n",
      "        ...,\n",
      "        [-0.0011,  0.0125, -0.0121,  ..., -0.0049,  0.0037,  0.0217],\n",
      "        [ 0.0054, -0.0123, -0.0220,  ...,  0.0117,  0.0101,  0.0034],\n",
      "        [ 0.0090, -0.0129, -0.0256,  ...,  0.0118, -0.0003,  0.0022]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.10.mlp.gate_proj.weight tensor([[-0.0317, -0.0420, -0.0264,  ..., -0.0052, -0.0015, -0.0097],\n",
      "        [-0.0173, -0.0147, -0.0137,  ...,  0.0029,  0.0145,  0.0166],\n",
      "        [-0.0157, -0.0212,  0.0092,  ...,  0.0304, -0.0035,  0.0012],\n",
      "        ...,\n",
      "        [-0.0469, -0.0023,  0.0125,  ...,  0.0075, -0.0311,  0.0199],\n",
      "        [ 0.0060,  0.0195,  0.0192,  ...,  0.0047, -0.0115, -0.0032],\n",
      "        [-0.0079, -0.0148, -0.0209,  ..., -0.0121,  0.0034, -0.0157]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.10.mlp.down_proj.weight tensor([[-9.7656e-03, -2.7832e-02, -4.0283e-03,  ...,  2.9175e-02,\n",
      "         -1.4771e-02, -1.4544e-05],\n",
      "        [-3.9978e-03,  1.8066e-02, -7.7515e-03,  ..., -8.6060e-03,\n",
      "          1.0803e-02, -6.9427e-04],\n",
      "        [-3.2227e-02, -2.0447e-03,  1.6479e-02,  ..., -2.5024e-02,\n",
      "         -2.5269e-02,  8.1177e-03],\n",
      "        ...,\n",
      "        [ 1.4343e-02, -4.9744e-03,  2.6550e-03,  ...,  1.4954e-02,\n",
      "          1.3306e-02,  7.6599e-03],\n",
      "        [-2.2278e-03,  4.6387e-02, -1.5747e-02,  ...,  1.7700e-02,\n",
      "          3.1494e-02, -3.1982e-02],\n",
      "        [-4.8584e-02, -7.2002e-05, -2.8198e-02,  ..., -1.7822e-02,\n",
      "         -1.4587e-02,  1.0254e-02]], dtype=torch.float16)\n",
      "model.layers.10.mlp.up_proj.weight tensor([[-0.0430, -0.0033, -0.0374,  ...,  0.0109, -0.0082, -0.0209],\n",
      "        [-0.0073,  0.0097,  0.0042,  ...,  0.0234,  0.0366, -0.0137],\n",
      "        [-0.0004, -0.0015,  0.0120,  ...,  0.0243, -0.0330, -0.0023],\n",
      "        ...,\n",
      "        [ 0.0220,  0.0214, -0.0012,  ..., -0.0432, -0.0025, -0.0016],\n",
      "        [-0.0043, -0.0018, -0.0278,  ...,  0.0209, -0.0153,  0.0142],\n",
      "        [-0.0020,  0.0143,  0.0034,  ...,  0.0003,  0.0077,  0.0153]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.10.input_layernorm.weight tensor([0.3633, 0.3594, 0.3203,  ..., 0.3398, 0.3477, 0.3359],\n",
      "       dtype=torch.float16)\n",
      "model.layers.10.post_attention_layernorm.weight tensor([0.2461, 0.2324, 0.2236,  ..., 0.2402, 0.2373, 0.2373],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.self_attn.q_proj.weight tensor([[ 0.0153,  0.0165, -0.0070,  ...,  0.0217, -0.0050,  0.0003],\n",
      "        [-0.0132,  0.0091,  0.0111,  ...,  0.0142,  0.0142, -0.0004],\n",
      "        [ 0.0009,  0.0063, -0.0240,  ..., -0.0020, -0.0103,  0.0059],\n",
      "        ...,\n",
      "        [ 0.0006, -0.0048,  0.0325,  ...,  0.0344,  0.0405, -0.0302],\n",
      "        [ 0.0447,  0.0179, -0.0120,  ..., -0.0082,  0.0184, -0.0293],\n",
      "        [ 0.0718, -0.0206, -0.0214,  ..., -0.0205,  0.0461,  0.0574]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.self_attn.k_proj.weight tensor([[ 0.0050,  0.0187, -0.0009,  ..., -0.0106,  0.0024, -0.0212],\n",
      "        [-0.0003, -0.0034, -0.0254,  ..., -0.0079, -0.0046,  0.0264],\n",
      "        [ 0.0147, -0.0254,  0.0073,  ...,  0.0072,  0.0052,  0.0053],\n",
      "        ...,\n",
      "        [-0.0085,  0.0211,  0.0025,  ..., -0.0190, -0.0120,  0.0305],\n",
      "        [ 0.0645, -0.0107, -0.0118,  ..., -0.0029,  0.0055, -0.0077],\n",
      "        [ 0.0060,  0.0056,  0.0181,  ..., -0.0048,  0.0610,  0.0243]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.11.self_attn.v_proj.weight tensor([[ 0.0050,  0.0046, -0.0101,  ..., -0.0190,  0.0007,  0.0059],\n",
      "        [ 0.0023, -0.0065, -0.0121,  ...,  0.0066,  0.0056, -0.0243],\n",
      "        [ 0.0025,  0.0069, -0.0069,  ...,  0.0147,  0.0046, -0.0225],\n",
      "        ...,\n",
      "        [-0.0154,  0.0063, -0.0141,  ...,  0.0056, -0.0189, -0.0023],\n",
      "        [-0.0083,  0.0150, -0.0103,  ..., -0.0144, -0.0172,  0.0034],\n",
      "        [ 0.0131, -0.0228,  0.0027,  ..., -0.0393, -0.0039, -0.0131]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.self_attn.o_proj.weight tensor([[-0.0183, -0.0008,  0.0052,  ...,  0.0092,  0.0112,  0.0015],\n",
      "        [ 0.0016, -0.0261,  0.0173,  ..., -0.0120,  0.0018, -0.0162],\n",
      "        [-0.0077, -0.0050,  0.0193,  ...,  0.0010, -0.0002,  0.0190],\n",
      "        ...,\n",
      "        [ 0.0038, -0.0052,  0.0232,  ...,  0.0016,  0.0012, -0.0074],\n",
      "        [ 0.0026,  0.0217,  0.0096,  ...,  0.0155,  0.0111, -0.0317],\n",
      "        [-0.0034, -0.0044,  0.0019,  ..., -0.0059,  0.0015, -0.0129]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.mlp.gate_proj.weight tensor([[-0.0447, -0.0100,  0.0017,  ...,  0.0038, -0.0060,  0.0381],\n",
      "        [ 0.0021, -0.0109,  0.0234,  ...,  0.0110, -0.0186, -0.0435],\n",
      "        [-0.0022, -0.0442, -0.0378,  ..., -0.0007, -0.0039,  0.0034],\n",
      "        ...,\n",
      "        [-0.0181, -0.0344, -0.0189,  ..., -0.0041, -0.0282,  0.0013],\n",
      "        [ 0.0025,  0.0062, -0.0259,  ..., -0.0128,  0.0097,  0.0005],\n",
      "        [-0.0077,  0.0101,  0.0120,  ...,  0.0344, -0.0032, -0.0184]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.mlp.down_proj.weight tensor([[-0.0199,  0.0457,  0.0177,  ...,  0.0194,  0.0145, -0.0043],\n",
      "        [-0.0145, -0.0045, -0.0256,  ..., -0.0312, -0.0093,  0.0155],\n",
      "        [-0.0200, -0.0090,  0.0068,  ...,  0.0015, -0.0187, -0.0063],\n",
      "        ...,\n",
      "        [ 0.0188,  0.0254, -0.0124,  ...,  0.0018,  0.0012,  0.0087],\n",
      "        [-0.0156, -0.0299, -0.0309,  ..., -0.0220, -0.0058,  0.0140],\n",
      "        [ 0.0352,  0.0194,  0.0222,  ...,  0.0038, -0.0125,  0.0405]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.mlp.up_proj.weight tensor([[-0.0101, -0.0012, -0.0260,  ...,  0.0260,  0.0037,  0.0310],\n",
      "        [ 0.0155,  0.0017, -0.0187,  ...,  0.0096, -0.0410,  0.0038],\n",
      "        [-0.0106, -0.0291, -0.0024,  ..., -0.0089, -0.0159,  0.0166],\n",
      "        ...,\n",
      "        [ 0.0422,  0.0126, -0.0036,  ..., -0.0019,  0.0020,  0.0197],\n",
      "        [-0.0107,  0.0082,  0.0101,  ...,  0.0197,  0.0076,  0.0067],\n",
      "        [ 0.0014, -0.0024, -0.0081,  ..., -0.0143,  0.0058,  0.0265]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.input_layernorm.weight tensor([0.3945, 0.3926, 0.3594,  ..., 0.3828, 0.3770, 0.3672],\n",
      "       dtype=torch.float16)\n",
      "model.layers.11.post_attention_layernorm.weight tensor([0.2539, 0.2363, 0.2334,  ..., 0.2500, 0.2490, 0.2451],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.self_attn.q_proj.weight tensor([[-0.0045, -0.0197,  0.0095,  ...,  0.0189, -0.0055,  0.0078],\n",
      "        [ 0.0046,  0.0053,  0.0007,  ..., -0.0123,  0.0262,  0.0164],\n",
      "        [ 0.0084,  0.0356, -0.0292,  ...,  0.0036,  0.0138, -0.0317],\n",
      "        ...,\n",
      "        [-0.0041,  0.0386, -0.0206,  ..., -0.0312, -0.0087,  0.0114],\n",
      "        [ 0.0347, -0.0062,  0.0109,  ...,  0.0344,  0.0050, -0.0327],\n",
      "        [ 0.0339,  0.0135, -0.0410,  ..., -0.0178,  0.0175,  0.0249]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.self_attn.k_proj.weight tensor([[ 0.0117, -0.0044, -0.0020,  ...,  0.0139,  0.0044, -0.0078],\n",
      "        [ 0.0039,  0.0166, -0.0081,  ...,  0.0118, -0.0152, -0.0197],\n",
      "        [-0.0077, -0.0195,  0.0070,  ...,  0.0166, -0.0074,  0.0155],\n",
      "        ...,\n",
      "        [ 0.0153,  0.0280, -0.0177,  ..., -0.0391, -0.0155, -0.0219],\n",
      "        [-0.0369,  0.0057,  0.0270,  ..., -0.0247,  0.0195, -0.0022],\n",
      "        [ 0.0007,  0.0352,  0.0095,  ...,  0.0139, -0.0459, -0.0330]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.self_attn.v_proj.weight tensor([[ 0.0053,  0.0054,  0.0045,  ..., -0.0071,  0.0082, -0.0183],\n",
      "        [-0.0265,  0.0166, -0.0017,  ..., -0.0215, -0.0006,  0.0153],\n",
      "        [-0.0121, -0.0116,  0.0045,  ..., -0.0016,  0.0439, -0.0160],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0023, -0.0175,  ..., -0.0056, -0.0129, -0.0013],\n",
      "        [ 0.0007, -0.0120, -0.0060,  ...,  0.0032,  0.0118,  0.0044],\n",
      "        [ 0.0215,  0.0017,  0.0021,  ...,  0.0194,  0.0067, -0.0108]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.self_attn.o_proj.weight tensor([[ 0.0272,  0.0067, -0.0007,  ...,  0.0018,  0.0079, -0.0037],\n",
      "        [-0.0058, -0.0107,  0.0014,  ..., -0.0136,  0.0049, -0.0188],\n",
      "        [ 0.0031, -0.0052, -0.0048,  ...,  0.0056, -0.0267,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0025, -0.0016,  0.0136,  ..., -0.0066, -0.0011,  0.0011],\n",
      "        [-0.0121, -0.0344, -0.0298,  ..., -0.0054,  0.0091,  0.0172],\n",
      "        [ 0.0192,  0.0063,  0.0105,  ...,  0.0208, -0.0121, -0.0176]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.mlp.gate_proj.weight tensor([[ 0.0070, -0.0255,  0.0107,  ..., -0.0044,  0.0211,  0.0067],\n",
      "        [-0.0250, -0.0270,  0.0151,  ...,  0.0155,  0.0162,  0.0145],\n",
      "        [ 0.0051,  0.0223, -0.0056,  ..., -0.0078, -0.0430,  0.0015],\n",
      "        ...,\n",
      "        [-0.0003,  0.0087,  0.0128,  ..., -0.0244, -0.0010,  0.0212],\n",
      "        [-0.0349, -0.0073,  0.0042,  ...,  0.0067,  0.0106, -0.0027],\n",
      "        [ 0.0023, -0.0186, -0.0072,  ..., -0.0032, -0.0084, -0.0156]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.mlp.down_proj.weight tensor([[ 0.0045, -0.0067, -0.0317,  ..., -0.0117,  0.0034,  0.0073],\n",
      "        [ 0.0170, -0.0237,  0.0016,  ..., -0.0086, -0.0106, -0.0161],\n",
      "        [ 0.0146,  0.0014,  0.0179,  ...,  0.0060, -0.0089, -0.0208],\n",
      "        ...,\n",
      "        [ 0.0154, -0.0128, -0.0447,  ...,  0.0391, -0.0188, -0.0170],\n",
      "        [-0.0249,  0.0137,  0.0100,  ..., -0.0062,  0.0374, -0.0219],\n",
      "        [ 0.0057,  0.0204,  0.0276,  ..., -0.0342,  0.0176, -0.0025]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.mlp.up_proj.weight tensor([[ 0.0040,  0.0186,  0.0062,  ..., -0.0056,  0.0172,  0.0170],\n",
      "        [-0.0030, -0.0132, -0.0106,  ...,  0.0034, -0.0117,  0.0260],\n",
      "        [ 0.0088, -0.0026,  0.0120,  ..., -0.0464,  0.0128,  0.0038],\n",
      "        ...,\n",
      "        [-0.0160, -0.0342, -0.0059,  ...,  0.0137, -0.0187,  0.0044],\n",
      "        [ 0.0029, -0.0310, -0.0033,  ...,  0.0099,  0.0079, -0.0122],\n",
      "        [-0.0119, -0.0171, -0.0041,  ...,  0.0376, -0.0047, -0.0332]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.input_layernorm.weight tensor([0.4023, 0.3984, 0.3652,  ..., 0.3789, 0.3828, 0.3848],\n",
      "       dtype=torch.float16)\n",
      "model.layers.12.post_attention_layernorm.weight tensor([0.2578, 0.2441, 0.2373,  ..., 0.2559, 0.2539, 0.2539],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.self_attn.q_proj.weight tensor([[-0.0030, -0.0080, -0.0132,  ...,  0.0087, -0.0038, -0.0089],\n",
      "        [-0.0156, -0.0122,  0.0009,  ..., -0.0024,  0.0085, -0.0036],\n",
      "        [-0.0159,  0.0099,  0.0072,  ..., -0.0078, -0.0057,  0.0034],\n",
      "        ...,\n",
      "        [ 0.0427,  0.0049,  0.0014,  ...,  0.0493,  0.0188, -0.0464],\n",
      "        [ 0.0049,  0.0113, -0.0217,  ...,  0.0118,  0.0148, -0.0085],\n",
      "        [-0.0073, -0.0220, -0.0153,  ..., -0.0121,  0.0222, -0.0072]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.self_attn.k_proj.weight tensor([[ 0.0123,  0.0064, -0.0034,  ..., -0.0182,  0.0264,  0.0127],\n",
      "        [ 0.0099,  0.0187, -0.0039,  ...,  0.0062,  0.0079, -0.0008],\n",
      "        [ 0.0035, -0.0089,  0.0337,  ...,  0.0001,  0.0156, -0.0315],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0320, -0.0018,  ...,  0.0060, -0.0052, -0.0150],\n",
      "        [-0.0042,  0.0070,  0.0278,  ..., -0.0454, -0.0164,  0.0334],\n",
      "        [-0.0237, -0.0483, -0.0120,  ..., -0.0339, -0.0283,  0.0084]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.self_attn.v_proj.weight tensor([[ 0.0154,  0.0031, -0.0125,  ..., -0.0032,  0.0128,  0.0142],\n",
      "        [-0.0085,  0.0141, -0.0044,  ...,  0.0159, -0.0080, -0.0042],\n",
      "        [ 0.0183,  0.0013, -0.0016,  ..., -0.0222,  0.0369, -0.0111],\n",
      "        ...,\n",
      "        [-0.0023,  0.0214, -0.0066,  ..., -0.0074,  0.0269, -0.0181],\n",
      "        [-0.0129,  0.0115,  0.0137,  ...,  0.0014,  0.0311,  0.0081],\n",
      "        [-0.0051, -0.0096,  0.0251,  ...,  0.0170, -0.0002,  0.0427]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.self_attn.o_proj.weight tensor([[-3.8757e-03,  3.7766e-04, -5.8289e-03,  ...,  2.7466e-04,\n",
      "         -4.9744e-03, -3.4790e-03],\n",
      "        [ 2.4170e-02,  1.0681e-02,  4.2725e-03,  ..., -1.2268e-02,\n",
      "         -8.9722e-03,  2.3499e-03],\n",
      "        [ 6.5613e-03,  9.2773e-03, -6.0558e-05,  ...,  5.7373e-03,\n",
      "         -2.5757e-02, -1.0376e-02],\n",
      "        ...,\n",
      "        [-7.8735e-03,  1.0071e-02,  1.4221e-02,  ..., -1.1536e-02,\n",
      "         -7.6294e-03, -1.5259e-02],\n",
      "        [ 2.9907e-03,  2.4567e-03,  2.9541e-02,  ..., -2.3193e-03,\n",
      "         -2.4170e-02, -2.6611e-02],\n",
      "        [ 1.1414e-02,  5.1270e-03,  9.1553e-03,  ...,  1.3123e-02,\n",
      "         -8.9111e-03, -2.1240e-02]], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.13.mlp.gate_proj.weight tensor([[ 0.0208,  0.0082, -0.0063,  ...,  0.0070,  0.0044, -0.0140],\n",
      "        [-0.0265,  0.0352,  0.0205,  ...,  0.0383, -0.0060, -0.0231],\n",
      "        [ 0.0077, -0.0232, -0.0095,  ...,  0.0015, -0.0184, -0.0123],\n",
      "        ...,\n",
      "        [ 0.0200, -0.0120,  0.0145,  ..., -0.0075,  0.0168,  0.0154],\n",
      "        [ 0.0211, -0.0026,  0.0137,  ...,  0.0149,  0.0070, -0.0255],\n",
      "        [ 0.0058,  0.0369,  0.0206,  ..., -0.0049,  0.0194,  0.0292]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.mlp.down_proj.weight tensor([[-0.0067, -0.0025, -0.0058,  ...,  0.0311, -0.0299, -0.0243],\n",
      "        [-0.0097,  0.0544, -0.0474,  ..., -0.0067,  0.0043, -0.0100],\n",
      "        [ 0.0010, -0.0034,  0.0186,  ...,  0.0067, -0.0287, -0.0052],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0002,  0.0275,  ...,  0.0076, -0.0187,  0.0356],\n",
      "        [ 0.0082,  0.0029,  0.0140,  ..., -0.0095,  0.0109, -0.0121],\n",
      "        [-0.0014, -0.0292,  0.0337,  ...,  0.0065,  0.0063,  0.0361]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.mlp.up_proj.weight tensor([[-0.0359,  0.0212, -0.0086,  ...,  0.0056,  0.0133, -0.0006],\n",
      "        [ 0.0145, -0.0081,  0.0344,  ...,  0.0398,  0.0277,  0.0068],\n",
      "        [ 0.0204, -0.0273,  0.0187,  ..., -0.0026,  0.0020, -0.0051],\n",
      "        ...,\n",
      "        [-0.0056,  0.0010,  0.0250,  ...,  0.0068, -0.0280,  0.0139],\n",
      "        [-0.0077, -0.0200,  0.0089,  ...,  0.0143,  0.0544,  0.0061],\n",
      "        [ 0.0105, -0.0215,  0.0154,  ..., -0.0203, -0.0020,  0.0219]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.input_layernorm.weight tensor([0.4141, 0.4043, 0.3711,  ..., 0.3867, 0.3809, 0.3906],\n",
      "       dtype=torch.float16)\n",
      "model.layers.13.post_attention_layernorm.weight tensor([0.2637, 0.2520, 0.2451,  ..., 0.2637, 0.2656, 0.2598],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.self_attn.q_proj.weight tensor([[-0.0062,  0.0010,  0.0200,  ..., -0.0051,  0.0114,  0.0325],\n",
      "        [ 0.0043,  0.0126, -0.0088,  ..., -0.0366, -0.0168, -0.0281],\n",
      "        [-0.0052, -0.0064,  0.0092,  ...,  0.0118,  0.0091, -0.0292],\n",
      "        ...,\n",
      "        [-0.0141, -0.0011, -0.0287,  ..., -0.0065,  0.0140, -0.0052],\n",
      "        [ 0.0234, -0.0339,  0.0540,  ..., -0.0120,  0.0099, -0.0067],\n",
      "        [-0.0125,  0.0028, -0.0117,  ...,  0.0139,  0.0137, -0.0481]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.self_attn.k_proj.weight tensor([[-0.0108, -0.0028,  0.0087,  ..., -0.0074,  0.0214,  0.0425],\n",
      "        [ 0.0281,  0.0261, -0.0221,  ..., -0.0063, -0.0183,  0.0080],\n",
      "        [-0.0203, -0.0003,  0.0209,  ...,  0.0225,  0.0135, -0.0012],\n",
      "        ...,\n",
      "        [-0.0315,  0.0094,  0.0275,  ..., -0.0030, -0.0044, -0.0217],\n",
      "        [-0.0289,  0.0042,  0.0141,  ..., -0.0354, -0.0054,  0.0013],\n",
      "        [-0.0150,  0.0339,  0.0250,  ...,  0.0679,  0.0148, -0.0339]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.self_attn.v_proj.weight tensor([[ 8.5449e-03,  5.8594e-03, -4.0283e-02,  ..., -2.8931e-02,\n",
      "         -5.1575e-03,  1.4038e-02],\n",
      "        [-2.0142e-03, -1.9653e-02,  1.8787e-04,  ...,  4.0894e-03,\n",
      "         -2.5940e-03, -1.0633e-04],\n",
      "        [ 2.5146e-02, -1.8188e-02, -1.3828e-05,  ..., -5.0354e-03,\n",
      "          8.9111e-03,  1.2939e-02],\n",
      "        ...,\n",
      "        [ 1.5442e-02, -1.2756e-02,  1.7212e-02,  ...,  1.9775e-02,\n",
      "         -1.1108e-02,  3.3569e-03],\n",
      "        [-1.5259e-02,  2.6978e-02, -3.3203e-02,  ...,  4.5776e-03,\n",
      "         -7.0190e-03, -3.0273e-02],\n",
      "        [ 7.2327e-03,  2.2705e-02, -2.3315e-02,  ..., -7.0801e-03,\n",
      "         -7.0190e-03,  1.8188e-02]], dtype=torch.float16)\n",
      "model.layers.14.self_attn.o_proj.weight tensor([[-0.0109,  0.0061, -0.0222,  ..., -0.0040,  0.0033, -0.0121],\n",
      "        [ 0.0014,  0.0287,  0.0080,  ...,  0.0212, -0.0089,  0.0004],\n",
      "        [ 0.0048,  0.0057, -0.0033,  ...,  0.0020,  0.0221,  0.0104],\n",
      "        ...,\n",
      "        [ 0.0219, -0.0041,  0.0035,  ..., -0.0148, -0.0006,  0.0098],\n",
      "        [-0.0029, -0.0020, -0.0187,  ..., -0.0028,  0.0361,  0.0200],\n",
      "        [-0.0093, -0.0113, -0.0143,  ..., -0.0068,  0.0059,  0.0025]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.mlp.gate_proj.weight tensor([[-0.0149,  0.0092, -0.0096,  ...,  0.0052, -0.0156, -0.0085],\n",
      "        [ 0.0214,  0.0061, -0.0422,  ...,  0.0383,  0.0177, -0.0099],\n",
      "        [ 0.0036,  0.0131,  0.0064,  ..., -0.0120,  0.0042, -0.0109],\n",
      "        ...,\n",
      "        [ 0.0074,  0.0177, -0.0193,  ..., -0.0040, -0.0069, -0.0051],\n",
      "        [-0.0070,  0.0156,  0.0199,  ..., -0.0153,  0.0154,  0.0089],\n",
      "        [ 0.0159, -0.0012,  0.0203,  ..., -0.0073, -0.0283,  0.0238]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.mlp.down_proj.weight tensor([[ 0.0084,  0.0236,  0.0062,  ..., -0.0317, -0.0081, -0.0021],\n",
      "        [ 0.0184,  0.0247, -0.0152,  ...,  0.0057,  0.0006,  0.0157],\n",
      "        [ 0.0013, -0.0437,  0.0210,  ..., -0.0015, -0.0282,  0.0112],\n",
      "        ...,\n",
      "        [-0.0038,  0.0183, -0.0103,  ..., -0.0034, -0.0188, -0.0175],\n",
      "        [ 0.0160,  0.0308,  0.0305,  ..., -0.0056, -0.0147,  0.0327],\n",
      "        [-0.0212,  0.0121, -0.0127,  ..., -0.0304,  0.0103,  0.0179]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.mlp.up_proj.weight tensor([[ 1.1215e-03,  1.0376e-02,  1.9409e-02,  ..., -2.6489e-02,\n",
      "          8.1177e-03,  7.7820e-03],\n",
      "        [ 2.6855e-02,  1.4099e-02, -2.8320e-02,  ...,  1.3916e-02,\n",
      "          2.1362e-02,  1.4526e-02],\n",
      "        [ 1.6093e-06,  3.9368e-03,  3.2227e-02,  ..., -1.6022e-03,\n",
      "          1.9653e-02, -5.4932e-03],\n",
      "        ...,\n",
      "        [-1.4832e-02,  7.7209e-03, -4.7119e-02,  ..., -7.8735e-03,\n",
      "         -5.0659e-03, -4.3640e-03],\n",
      "        [-2.7466e-02,  2.5635e-02, -6.1340e-03,  ..., -1.3611e-02,\n",
      "          2.3438e-02, -1.6235e-02],\n",
      "        [ 5.5237e-03,  2.5757e-02,  1.1597e-02,  ...,  1.1902e-02,\n",
      "         -1.3733e-02,  7.4463e-03]], dtype=torch.float16)\n",
      "model.layers.14.input_layernorm.weight tensor([0.4160, 0.4258, 0.3750,  ..., 0.4062, 0.3984, 0.3887],\n",
      "       dtype=torch.float16)\n",
      "model.layers.14.post_attention_layernorm.weight tensor([0.2734, 0.2617, 0.2598,  ..., 0.2773, 0.2734, 0.2695],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.self_attn.q_proj.weight tensor([[-0.0063, -0.0195, -0.0057,  ...,  0.0096, -0.0084,  0.0103],\n",
      "        [ 0.0250, -0.0228,  0.0089,  ..., -0.0014,  0.0045,  0.0058],\n",
      "        [ 0.0061, -0.0075,  0.0029,  ...,  0.0144, -0.0028, -0.0122],\n",
      "        ...,\n",
      "        [-0.0444, -0.0342,  0.0118,  ..., -0.0123, -0.0110,  0.0288],\n",
      "        [-0.0173, -0.0554,  0.0186,  ...,  0.0167, -0.0065,  0.0043],\n",
      "        [ 0.0184,  0.0193,  0.0280,  ...,  0.0007, -0.0197, -0.0150]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.self_attn.k_proj.weight tensor([[ 0.0211, -0.0065, -0.0112,  ..., -0.0051,  0.0052,  0.0024],\n",
      "        [ 0.0120, -0.0095, -0.0076,  ...,  0.0019,  0.0048, -0.0150],\n",
      "        [ 0.0048,  0.0055, -0.0051,  ..., -0.0055,  0.0124, -0.0101],\n",
      "        ...,\n",
      "        [-0.0442, -0.0479,  0.0245,  ...,  0.0110,  0.0088, -0.0096],\n",
      "        [ 0.0092,  0.0050,  0.0114,  ...,  0.0286, -0.0119,  0.0156],\n",
      "        [-0.0242, -0.0087,  0.0325,  ..., -0.0014, -0.0211, -0.0017]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.self_attn.v_proj.weight tensor([[ 0.0087, -0.0004,  0.0042,  ...,  0.0096,  0.0132,  0.0154],\n",
      "        [-0.0110, -0.0089,  0.0148,  ...,  0.0171,  0.0043,  0.0244],\n",
      "        [ 0.0054, -0.0354, -0.0071,  ..., -0.0320,  0.0059,  0.0036],\n",
      "        ...,\n",
      "        [-0.0048, -0.0034,  0.0136,  ...,  0.0101,  0.0217, -0.0153],\n",
      "        [ 0.0055,  0.0038, -0.0273,  ...,  0.0127,  0.0139, -0.0059],\n",
      "        [ 0.0232,  0.0034,  0.0193,  ..., -0.0220,  0.0023,  0.0069]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.self_attn.o_proj.weight tensor([[ 0.0019,  0.0072, -0.0153,  ...,  0.0143, -0.0089,  0.0239],\n",
      "        [-0.0056, -0.0047, -0.0244,  ...,  0.0137, -0.0325,  0.0181],\n",
      "        [ 0.0059, -0.0078, -0.0042,  ...,  0.0038, -0.0029,  0.0067],\n",
      "        ...,\n",
      "        [-0.0091, -0.0236,  0.0134,  ..., -0.0073, -0.0250, -0.0098],\n",
      "        [-0.0035, -0.0060, -0.0029,  ...,  0.0129,  0.0038, -0.0030],\n",
      "        [-0.0248,  0.0051,  0.0251,  ..., -0.0057, -0.0342, -0.0270]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.mlp.gate_proj.weight tensor([[-0.0093,  0.0287, -0.0140,  ...,  0.0094, -0.0234,  0.0278],\n",
      "        [ 0.0087, -0.0085,  0.0342,  ..., -0.0126,  0.0096,  0.0113],\n",
      "        [ 0.0065, -0.0120, -0.0104,  ...,  0.0276, -0.0503, -0.0176],\n",
      "        ...,\n",
      "        [-0.0094,  0.0026,  0.0330,  ..., -0.0081,  0.0107,  0.0096],\n",
      "        [-0.0077, -0.0165,  0.0079,  ..., -0.0089, -0.0140, -0.0371],\n",
      "        [ 0.0004, -0.0151,  0.0003,  ..., -0.0005, -0.0009,  0.0065]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.mlp.down_proj.weight tensor([[-0.0093, -0.0156, -0.0283,  ...,  0.0238,  0.0108, -0.0118],\n",
      "        [ 0.0051, -0.0148, -0.0250,  ..., -0.0118, -0.0219,  0.0187],\n",
      "        [ 0.0425,  0.0496, -0.0245,  ..., -0.0103,  0.0349,  0.0172],\n",
      "        ...,\n",
      "        [-0.0131, -0.0437,  0.0111,  ...,  0.0066, -0.0033,  0.0029],\n",
      "        [-0.0361, -0.0173, -0.0016,  ..., -0.0283, -0.0164,  0.0051],\n",
      "        [ 0.0043,  0.0302, -0.0103,  ..., -0.0130, -0.0275, -0.0232]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.15.mlp.up_proj.weight tensor([[-0.0128, -0.0123,  0.0227,  ...,  0.0204,  0.0171, -0.0476],\n",
      "        [-0.0156,  0.0349,  0.0298,  ...,  0.0045,  0.0055,  0.0036],\n",
      "        [-0.0417, -0.0006, -0.0083,  ..., -0.0042,  0.0164, -0.0093],\n",
      "        ...,\n",
      "        [-0.0115, -0.0199, -0.0087,  ...,  0.0093,  0.0024, -0.0063],\n",
      "        [ 0.0376, -0.0065,  0.0137,  ..., -0.0078,  0.0181, -0.0004],\n",
      "        [-0.0060, -0.0220, -0.0217,  ...,  0.0051, -0.0182, -0.0019]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.input_layernorm.weight tensor([0.4062, 0.4004, 0.3770,  ..., 0.3848, 0.3848, 0.3887],\n",
      "       dtype=torch.float16)\n",
      "model.layers.15.post_attention_layernorm.weight tensor([0.2852, 0.2715, 0.2715,  ..., 0.2852, 0.2812, 0.2812],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.self_attn.q_proj.weight tensor([[ 0.0108,  0.0079, -0.0220,  ...,  0.0070,  0.0250, -0.0064],\n",
      "        [ 0.0194, -0.0425,  0.0175,  ...,  0.0147,  0.0067, -0.0205],\n",
      "        [-0.0112, -0.0018,  0.0055,  ..., -0.0232, -0.0062, -0.0242],\n",
      "        ...,\n",
      "        [ 0.0229, -0.0308, -0.0017,  ..., -0.0210,  0.0238, -0.0188],\n",
      "        [-0.0146, -0.0098,  0.0262,  ...,  0.0025, -0.0203,  0.0130],\n",
      "        [ 0.0067,  0.0157,  0.0179,  ..., -0.0051, -0.0295, -0.0123]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.self_attn.k_proj.weight tensor([[-1.7822e-02,  1.9653e-02,  7.7515e-03,  ...,  6.5002e-03,\n",
      "          1.7700e-02,  2.3438e-02],\n",
      "        [ 2.2583e-02, -3.4912e-02,  9.0942e-03,  ..., -2.6822e-05,\n",
      "         -2.7344e-02, -2.0905e-03],\n",
      "        [ 2.8687e-02, -3.3417e-03, -1.3855e-02,  ..., -2.1484e-02,\n",
      "          2.8229e-03, -3.5889e-02],\n",
      "        ...,\n",
      "        [ 1.6235e-02,  1.2085e-02, -1.3611e-02,  ..., -2.2583e-02,\n",
      "         -2.4658e-02, -5.4932e-02],\n",
      "        [ 1.7700e-02,  1.0193e-02,  1.2146e-02,  ...,  2.6611e-02,\n",
      "          1.1902e-02,  1.3000e-02],\n",
      "        [-3.7354e-02,  5.3223e-02,  6.2256e-02,  ..., -1.4038e-03,\n",
      "         -1.5869e-02,  3.1250e-02]], dtype=torch.float16)\n",
      "model.layers.16.self_attn.v_proj.weight tensor([[-0.0095,  0.0023, -0.0059,  ...,  0.0078, -0.0015, -0.0060],\n",
      "        [-0.0562, -0.0061, -0.0056,  ...,  0.0058,  0.0154, -0.0281],\n",
      "        [-0.0033, -0.0048, -0.0063,  ..., -0.0254,  0.0439,  0.0150],\n",
      "        ...,\n",
      "        [ 0.0085, -0.0014,  0.0141,  ..., -0.0139,  0.0055,  0.0173],\n",
      "        [-0.0208,  0.0002,  0.0045,  ...,  0.0354, -0.0031,  0.0123],\n",
      "        [ 0.0130,  0.0107,  0.0084,  ..., -0.0102,  0.0013, -0.0309]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.self_attn.o_proj.weight tensor([[ 4.3457e-02, -1.1292e-03, -1.9165e-02,  ...,  7.4463e-03,\n",
      "         -1.5076e-02,  1.1749e-03],\n",
      "        [-2.5513e-02,  1.1292e-02, -2.3315e-02,  ..., -5.4016e-03,\n",
      "         -1.0729e-06, -1.2665e-03],\n",
      "        [-2.8564e-02, -1.0498e-02, -5.7983e-03,  ...,  2.6398e-03,\n",
      "          2.8076e-02,  1.3245e-02],\n",
      "        ...,\n",
      "        [-6.4087e-03, -1.7456e-02,  6.7749e-03,  ...,  5.7068e-03,\n",
      "         -2.1606e-02,  4.2534e-04],\n",
      "        [-5.1270e-03,  2.3193e-02,  3.1494e-02,  ..., -2.7161e-03,\n",
      "          8.9722e-03, -1.4893e-02],\n",
      "        [-1.8066e-02,  4.7302e-03,  4.1809e-03,  ..., -1.4038e-03,\n",
      "         -3.9978e-03,  6.6757e-04]], dtype=torch.float16)\n",
      "model.layers.16.mlp.gate_proj.weight tensor([[-0.0175, -0.0168, -0.0245,  ...,  0.0050, -0.0048,  0.0120],\n",
      "        [ 0.0317,  0.0182, -0.0081,  ...,  0.0126,  0.0067,  0.0190],\n",
      "        [-0.0075,  0.0168, -0.0031,  ...,  0.0078, -0.0020, -0.0259],\n",
      "        ...,\n",
      "        [-0.0038,  0.0199,  0.0013,  ...,  0.0232,  0.0234,  0.0060],\n",
      "        [ 0.0023, -0.0073,  0.0104,  ...,  0.0190,  0.0182,  0.0215],\n",
      "        [ 0.0344,  0.0215,  0.0364,  ..., -0.0364,  0.0233, -0.0104]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.mlp.down_proj.weight tensor([[-0.0200, -0.0022,  0.0181,  ...,  0.0291, -0.0046,  0.0026],\n",
      "        [-0.0231, -0.0025, -0.0116,  ...,  0.0278,  0.0020, -0.0006],\n",
      "        [-0.0161,  0.0045, -0.0120,  ...,  0.0228, -0.0064, -0.0060],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0225, -0.0248,  ...,  0.0097,  0.0156,  0.0016],\n",
      "        [ 0.0147,  0.0057, -0.0103,  ..., -0.0154, -0.0126, -0.0071],\n",
      "        [ 0.0117, -0.0151, -0.0251,  ...,  0.0261,  0.0031, -0.0143]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.mlp.up_proj.weight tensor([[-0.0374,  0.0118,  0.0055,  ...,  0.0177, -0.0170,  0.0046],\n",
      "        [ 0.0126, -0.0154,  0.0091,  ...,  0.0168, -0.0240, -0.0076],\n",
      "        [-0.0103, -0.0216, -0.0075,  ..., -0.0115,  0.0082,  0.0255],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0247, -0.0073,  ...,  0.0007, -0.0102, -0.0171],\n",
      "        [-0.0124,  0.0233, -0.0195,  ...,  0.0102, -0.0146,  0.0131],\n",
      "        [ 0.0034,  0.0042, -0.0022,  ...,  0.0010, -0.0240, -0.0236]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.input_layernorm.weight tensor([0.4102, 0.4160, 0.3867,  ..., 0.3867, 0.4023, 0.4004],\n",
      "       dtype=torch.float16)\n",
      "model.layers.16.post_attention_layernorm.weight tensor([0.3027, 0.2891, 0.2910,  ..., 0.3027, 0.3066, 0.2969],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.self_attn.q_proj.weight tensor([[-0.0165, -0.0136,  0.0143,  ..., -0.0067, -0.0105, -0.0038],\n",
      "        [ 0.0114, -0.0089,  0.0195,  ..., -0.0004, -0.0209,  0.0013],\n",
      "        [-0.0137, -0.0018, -0.0114,  ...,  0.0179, -0.0093, -0.0012],\n",
      "        ...,\n",
      "        [ 0.0376, -0.0283,  0.0559,  ..., -0.0143, -0.0177,  0.0105],\n",
      "        [ 0.0223, -0.0226,  0.0459,  ...,  0.0121, -0.0164, -0.0601],\n",
      "        [ 0.0189, -0.0096,  0.0142,  ...,  0.0288,  0.0029,  0.0579]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.self_attn.k_proj.weight tensor([[-0.0148,  0.0040,  0.0059,  ..., -0.0050,  0.0142,  0.0076],\n",
      "        [ 0.0059,  0.0067,  0.0073,  ...,  0.0112, -0.0017, -0.0085],\n",
      "        [ 0.0027, -0.0106,  0.0031,  ...,  0.0188, -0.0097, -0.0111],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0615, -0.0119,  ..., -0.0150,  0.0525, -0.0679],\n",
      "        [ 0.0254,  0.0217,  0.0138,  ..., -0.0154, -0.0219, -0.0199],\n",
      "        [-0.0400, -0.0454, -0.0031,  ...,  0.0320,  0.0234, -0.0060]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.self_attn.v_proj.weight tensor([[-0.0143,  0.0087,  0.0063,  ...,  0.0236, -0.0082, -0.0036],\n",
      "        [-0.0024, -0.0125, -0.0090,  ...,  0.0087, -0.0247, -0.0172],\n",
      "        [ 0.0130,  0.0090, -0.0019,  ...,  0.0056, -0.0085, -0.0223],\n",
      "        ...,\n",
      "        [ 0.0093, -0.0044,  0.0146,  ...,  0.0090,  0.0052, -0.0154],\n",
      "        [ 0.0173,  0.0033, -0.0403,  ..., -0.0226,  0.0018, -0.0315],\n",
      "        [-0.0087, -0.0079, -0.0013,  ...,  0.0090,  0.0107,  0.0079]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.self_attn.o_proj.weight tensor([[-0.0043, -0.0055,  0.0069,  ..., -0.0193,  0.0032, -0.0109],\n",
      "        [-0.0146, -0.0232,  0.0240,  ...,  0.0265, -0.0069,  0.0203],\n",
      "        [ 0.0118, -0.0135, -0.0056,  ..., -0.0153, -0.0030, -0.0168],\n",
      "        ...,\n",
      "        [-0.0084,  0.0123, -0.0182,  ..., -0.0280, -0.0299, -0.0193],\n",
      "        [-0.0099,  0.0115, -0.0057,  ..., -0.0215, -0.0024,  0.0092],\n",
      "        [-0.0186, -0.0095,  0.0127,  ...,  0.0317, -0.0183,  0.0143]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.mlp.gate_proj.weight tensor([[-0.0347,  0.0232,  0.0157,  ...,  0.0073, -0.0071,  0.0043],\n",
      "        [-0.0038,  0.0317,  0.0167,  ...,  0.0164, -0.0021,  0.0078],\n",
      "        [ 0.0134,  0.0122, -0.0233,  ...,  0.0057,  0.0220, -0.0052],\n",
      "        ...,\n",
      "        [-0.0125, -0.0117, -0.0162,  ..., -0.0065, -0.0137, -0.0062],\n",
      "        [ 0.0052, -0.0017,  0.0003,  ...,  0.0040,  0.0388, -0.0025],\n",
      "        [-0.0014,  0.0194,  0.0147,  ..., -0.0144,  0.0093, -0.0041]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.mlp.down_proj.weight tensor([[ 2.2217e-02, -2.1606e-02,  6.8665e-03,  ...,  5.4016e-03,\n",
      "         -7.2021e-03, -5.8899e-03],\n",
      "        [ 2.3315e-02,  2.3804e-02,  1.5625e-02,  ...,  2.9449e-03,\n",
      "         -1.0864e-02,  2.8809e-02],\n",
      "        [-2.7710e-02, -1.6968e-02, -1.1902e-02,  ...,  1.2146e-02,\n",
      "         -9.7656e-03,  5.1575e-03],\n",
      "        ...,\n",
      "        [ 6.3477e-03, -1.7944e-02, -7.3547e-03,  ..., -1.2146e-02,\n",
      "         -5.9605e-08, -2.8198e-02],\n",
      "        [ 5.7068e-03,  1.5106e-03, -7.5684e-03,  ..., -2.0020e-02,\n",
      "         -2.5024e-02,  2.0630e-02],\n",
      "        [ 2.5177e-03, -2.1118e-02,  1.7578e-02,  ..., -3.0518e-02,\n",
      "         -8.6060e-03,  3.4485e-03]], dtype=torch.float16)\n",
      "model.layers.17.mlp.up_proj.weight tensor([[-0.0072, -0.0255, -0.0077,  ...,  0.0077,  0.0146, -0.0140],\n",
      "        [-0.0211,  0.0170,  0.0176,  ..., -0.0043, -0.0018,  0.0064],\n",
      "        [ 0.0004, -0.0023, -0.0049,  ...,  0.0118, -0.0227,  0.0110],\n",
      "        ...,\n",
      "        [ 0.0117,  0.0157,  0.0062,  ...,  0.0074, -0.0133, -0.0120],\n",
      "        [-0.0084, -0.0187, -0.0120,  ..., -0.0052, -0.0203,  0.0084],\n",
      "        [ 0.0032,  0.0061, -0.0025,  ...,  0.0040, -0.0366, -0.0170]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.input_layernorm.weight tensor([0.4238, 0.4277, 0.4004,  ..., 0.4199, 0.4219, 0.4043],\n",
      "       dtype=torch.float16)\n",
      "model.layers.17.post_attention_layernorm.weight tensor([0.3203, 0.3125, 0.3105,  ..., 0.3223, 0.3242, 0.3145],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.self_attn.q_proj.weight tensor([[ 0.0089,  0.0049,  0.0028,  ..., -0.0111, -0.0413,  0.0051],\n",
      "        [ 0.0022, -0.0073,  0.0120,  ..., -0.0056,  0.0177,  0.0415],\n",
      "        [ 0.0036, -0.0190, -0.0162,  ...,  0.0002, -0.0085,  0.0102],\n",
      "        ...,\n",
      "        [-0.0310,  0.0128, -0.0073,  ...,  0.0278,  0.0056,  0.0176],\n",
      "        [ 0.0649, -0.0167, -0.0271,  ...,  0.0562,  0.0317,  0.0107],\n",
      "        [ 0.0260,  0.0195, -0.0006,  ...,  0.0211,  0.0449, -0.0476]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.self_attn.k_proj.weight tensor([[-0.0126,  0.0264, -0.0093,  ...,  0.0160, -0.0160, -0.0234],\n",
      "        [ 0.0078, -0.0172,  0.0096,  ...,  0.0189,  0.0238,  0.0003],\n",
      "        [ 0.0106, -0.0015,  0.0042,  ...,  0.0347, -0.0160,  0.0254],\n",
      "        ...,\n",
      "        [-0.1099, -0.0198, -0.0669,  ...,  0.0242, -0.0021, -0.0439],\n",
      "        [ 0.0461,  0.0400, -0.0106,  ...,  0.0393,  0.0669, -0.0344],\n",
      "        [-0.0452, -0.0435, -0.0452,  ...,  0.0309, -0.0144, -0.0292]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.18.self_attn.v_proj.weight tensor([[-0.0266,  0.0175, -0.0197,  ...,  0.0013,  0.0300,  0.0085],\n",
      "        [ 0.0115,  0.0041, -0.0059,  ..., -0.0211,  0.0386,  0.0021],\n",
      "        [-0.0024,  0.0115, -0.0053,  ...,  0.0082, -0.0016, -0.0104],\n",
      "        ...,\n",
      "        [ 0.0135,  0.0145, -0.0042,  ..., -0.0040, -0.0195, -0.0055],\n",
      "        [ 0.0029, -0.0040,  0.0017,  ...,  0.0131,  0.0098,  0.0317],\n",
      "        [-0.0015,  0.0074,  0.0070,  ..., -0.0058, -0.0106, -0.0004]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.self_attn.o_proj.weight tensor([[-0.0168,  0.0122, -0.0137,  ...,  0.0096,  0.0278,  0.0172],\n",
      "        [-0.0039, -0.0361,  0.0087,  ...,  0.0033, -0.0072, -0.0280],\n",
      "        [-0.0040, -0.0165,  0.0152,  ...,  0.0258,  0.0179,  0.0304],\n",
      "        ...,\n",
      "        [-0.0352, -0.0154, -0.0240,  ...,  0.0177,  0.0137,  0.0028],\n",
      "        [ 0.0211,  0.0535,  0.0143,  ...,  0.0099, -0.0076, -0.0339],\n",
      "        [-0.0160, -0.0466, -0.0192,  ..., -0.0121,  0.0096, -0.0160]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.mlp.gate_proj.weight tensor([[ 1.2146e-02, -6.0120e-03,  1.2329e-02,  ..., -2.2125e-03,\n",
      "          1.5869e-02,  6.5994e-04],\n",
      "        [-3.9062e-02,  7.7820e-03, -1.6602e-02,  ..., -6.1646e-03,\n",
      "         -1.4160e-02, -8.4400e-05],\n",
      "        [-2.0386e-02,  1.0742e-02,  3.7354e-02,  ..., -2.3926e-02,\n",
      "         -4.7607e-03,  3.8147e-03],\n",
      "        ...,\n",
      "        [-3.8330e-02, -2.9175e-02, -3.1250e-02,  ..., -2.8809e-02,\n",
      "         -1.6968e-02, -2.2339e-02],\n",
      "        [ 4.6997e-03,  2.0874e-02,  1.7700e-02,  ..., -1.2634e-02,\n",
      "         -1.3809e-03,  8.8501e-03],\n",
      "        [ 2.2736e-03, -4.3640e-03,  7.3547e-03,  ...,  7.6904e-03,\n",
      "         -4.4861e-03,  1.4954e-03]], dtype=torch.float16)\n",
      "model.layers.18.mlp.down_proj.weight tensor([[ 0.0242,  0.0091, -0.0304,  ...,  0.0388,  0.0132,  0.0181],\n",
      "        [ 0.0194,  0.0078,  0.0228,  ..., -0.0047, -0.0013, -0.0035],\n",
      "        [ 0.0066, -0.0204, -0.0334,  ...,  0.0175, -0.0019,  0.0152],\n",
      "        ...,\n",
      "        [-0.0109,  0.0137,  0.0162,  ...,  0.0215,  0.0025, -0.0093],\n",
      "        [-0.0203,  0.0025,  0.0142,  ...,  0.0091,  0.0352,  0.0001],\n",
      "        [-0.0002,  0.0225,  0.0182,  ..., -0.0066,  0.0211,  0.0151]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.mlp.up_proj.weight tensor([[ 0.0105,  0.0103,  0.0315,  ...,  0.0036,  0.0194, -0.0107],\n",
      "        [-0.0140,  0.0287, -0.0322,  ..., -0.0014, -0.0013,  0.0070],\n",
      "        [-0.0242, -0.0115,  0.0090,  ...,  0.0132,  0.0160,  0.0231],\n",
      "        ...,\n",
      "        [-0.0021,  0.0089,  0.0093,  ...,  0.0188, -0.0040,  0.0060],\n",
      "        [-0.0075, -0.0018, -0.0100,  ...,  0.0187,  0.0222,  0.0383],\n",
      "        [ 0.0094,  0.0231,  0.0099,  ..., -0.0147,  0.0164,  0.0151]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.input_layernorm.weight tensor([0.4473, 0.4473, 0.4277,  ..., 0.4297, 0.4414, 0.4258],\n",
      "       dtype=torch.float16)\n",
      "model.layers.18.post_attention_layernorm.weight tensor([0.3398, 0.3301, 0.3281,  ..., 0.3359, 0.3398, 0.3340],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.self_attn.q_proj.weight tensor([[ 0.0026,  0.0068, -0.0078,  ..., -0.0170, -0.0062,  0.0128],\n",
      "        [-0.0049, -0.0099,  0.0014,  ..., -0.0038,  0.0282, -0.0048],\n",
      "        [ 0.0089,  0.0244,  0.0070,  ...,  0.0094,  0.0179,  0.0021],\n",
      "        ...,\n",
      "        [-0.0254, -0.0187, -0.0038,  ..., -0.0442,  0.0009,  0.0265],\n",
      "        [ 0.0066,  0.0359,  0.0266,  ...,  0.0537,  0.0154, -0.0283],\n",
      "        [-0.0129,  0.0275,  0.0527,  ..., -0.0029, -0.0398, -0.0134]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.self_attn.k_proj.weight tensor([[ 0.0184,  0.0120,  0.0015,  ..., -0.0063, -0.0030, -0.0145],\n",
      "        [-0.0190,  0.0215, -0.0190,  ...,  0.0146,  0.0055,  0.0156],\n",
      "        [-0.0208,  0.0070, -0.0182,  ...,  0.0010,  0.0134,  0.0188],\n",
      "        ...,\n",
      "        [ 0.0011,  0.0430, -0.0135,  ..., -0.0178,  0.0226, -0.0302],\n",
      "        [-0.0708, -0.0332, -0.0026,  ...,  0.0212,  0.0128, -0.0129],\n",
      "        [-0.0186,  0.0088, -0.0212,  ..., -0.0226,  0.0140,  0.0359]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.self_attn.v_proj.weight tensor([[ 3.5248e-03, -5.3711e-03, -3.8147e-03,  ...,  7.0190e-03,\n",
      "         -3.9673e-03,  7.7209e-03],\n",
      "        [ 2.5330e-03,  2.2827e-02, -1.8978e-04,  ...,  3.7354e-02,\n",
      "         -1.6357e-02, -5.4169e-04],\n",
      "        [-4.6875e-02, -2.7222e-02, -5.2185e-03,  ..., -5.7068e-03,\n",
      "         -7.4768e-03,  1.3000e-02],\n",
      "        ...,\n",
      "        [ 3.6001e-05, -3.8086e-02, -3.4790e-03,  ...,  1.6708e-03,\n",
      "          5.8289e-03, -1.8921e-02],\n",
      "        [ 1.4099e-02, -3.9978e-03, -2.3926e-02,  ..., -9.0942e-03,\n",
      "         -9.3994e-03, -4.3945e-03],\n",
      "        [-1.5488e-03,  2.0996e-02, -1.6174e-03,  ..., -2.5513e-02,\n",
      "         -1.4160e-02,  1.3367e-02]], dtype=torch.float16)\n",
      "model.layers.19.self_attn.o_proj.weight tensor([[ 2.1851e-02, -5.5908e-02,  2.3804e-02,  ...,  5.2643e-04,\n",
      "          7.2098e-04, -2.4414e-02],\n",
      "        [ 2.5513e-02, -6.5918e-03,  3.4027e-03,  ..., -1.2573e-02,\n",
      "          7.0801e-03, -5.5237e-03],\n",
      "        [-3.6163e-03, -1.3367e-02, -2.4170e-02,  ...,  4.0894e-03,\n",
      "          9.5825e-03, -2.1210e-03],\n",
      "        ...,\n",
      "        [ 1.5198e-02, -5.5847e-03, -5.5847e-03,  ...,  9.8267e-03,\n",
      "         -1.8188e-02,  1.1047e-02],\n",
      "        [ 7.7820e-03, -1.9989e-03, -1.0925e-02,  ..., -1.0254e-02,\n",
      "         -1.7578e-02, -8.8215e-05],\n",
      "        [ 7.6904e-03,  2.0874e-02,  9.5825e-03,  ..., -1.6602e-02,\n",
      "         -1.1902e-02,  1.4282e-02]], dtype=torch.float16)\n",
      "model.layers.19.mlp.gate_proj.weight tensor([[-0.0118,  0.0045,  0.0065,  ...,  0.0045,  0.0039, -0.0023],\n",
      "        [ 0.0089,  0.0206,  0.0045,  ...,  0.0056,  0.0145, -0.0347],\n",
      "        [-0.0066, -0.0042, -0.0189,  ..., -0.0386, -0.0074,  0.0167],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0023,  0.0020,  ..., -0.0225, -0.0047,  0.0022],\n",
      "        [-0.0498,  0.0011, -0.0031,  ..., -0.0056, -0.0212,  0.0162],\n",
      "        [-0.0245, -0.0242,  0.0138,  ..., -0.0012,  0.0071,  0.0011]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.mlp.down_proj.weight tensor([[-0.0054, -0.0186,  0.0063,  ...,  0.0043, -0.0165,  0.0170],\n",
      "        [-0.0091, -0.0415, -0.0259,  ...,  0.0098,  0.0092, -0.0090],\n",
      "        [-0.0214, -0.0190,  0.0242,  ...,  0.0046, -0.0074,  0.0069],\n",
      "        ...,\n",
      "        [-0.0087, -0.0026,  0.0186,  ...,  0.0142,  0.0172, -0.0013],\n",
      "        [ 0.0057, -0.0148, -0.0427,  ...,  0.0271, -0.0083, -0.0152],\n",
      "        [ 0.0084, -0.0053, -0.0282,  ...,  0.0014,  0.0079,  0.0070]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.mlp.up_proj.weight tensor([[ 0.0006, -0.0093,  0.0062,  ...,  0.0041,  0.0105,  0.0070],\n",
      "        [-0.0087, -0.0007, -0.0101,  ...,  0.0231, -0.0209, -0.0125],\n",
      "        [ 0.0156,  0.0067,  0.0183,  ..., -0.0162, -0.0117,  0.0500],\n",
      "        ...,\n",
      "        [ 0.0113,  0.0153, -0.0063,  ...,  0.0205, -0.0149,  0.0361],\n",
      "        [ 0.0065,  0.0251,  0.0349,  ..., -0.0160,  0.0325, -0.0137],\n",
      "        [ 0.0039, -0.0184, -0.0135,  ..., -0.0038,  0.0007,  0.0097]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.input_layernorm.weight tensor([0.4512, 0.4570, 0.4375,  ..., 0.4258, 0.4336, 0.4375],\n",
      "       dtype=torch.float16)\n",
      "model.layers.19.post_attention_layernorm.weight tensor([0.3535, 0.3398, 0.3418,  ..., 0.3496, 0.3496, 0.3477],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.self_attn.q_proj.weight tensor([[ 0.0013, -0.0057,  0.0166,  ...,  0.0120,  0.0022, -0.0220],\n",
      "        [ 0.0060, -0.0201,  0.0096,  ..., -0.0084, -0.0090,  0.0098],\n",
      "        [ 0.0035, -0.0061, -0.0084,  ...,  0.0157, -0.0017, -0.0048],\n",
      "        ...,\n",
      "        [-0.0222, -0.0430,  0.0110,  ..., -0.0036,  0.0203,  0.0048],\n",
      "        [-0.0703, -0.0173,  0.0114,  ...,  0.0081,  0.0192,  0.0074],\n",
      "        [ 0.0179, -0.0014, -0.0059,  ..., -0.0151, -0.0094, -0.0021]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.self_attn.k_proj.weight tensor([[-3.2349e-03,  4.8218e-03,  8.1177e-03,  ..., -2.1820e-03,\n",
      "          2.3193e-03,  4.4861e-03],\n",
      "        [ 2.8839e-03,  8.9722e-03,  3.5858e-03,  ...,  8.7738e-05,\n",
      "         -1.8692e-03,  1.1108e-02],\n",
      "        [ 4.0894e-03,  5.6152e-03, -6.1646e-03,  ..., -3.8910e-03,\n",
      "          4.3030e-03,  1.9409e-02],\n",
      "        ...,\n",
      "        [-2.2221e-04, -3.0975e-03, -2.2949e-02,  ..., -1.9653e-02,\n",
      "          2.3804e-02,  2.6855e-02],\n",
      "        [-1.7456e-02,  1.9531e-02, -2.4414e-03,  ...,  2.0874e-02,\n",
      "          2.1118e-02,  5.9570e-02],\n",
      "        [-1.6357e-02, -2.6855e-02, -1.1597e-02,  ...,  2.5757e-02,\n",
      "         -2.6611e-02,  4.7607e-02]], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.20.self_attn.v_proj.weight tensor([[-0.0041, -0.0126,  0.0147,  ..., -0.0042,  0.0112,  0.0117],\n",
      "        [-0.0070, -0.0413, -0.0003,  ..., -0.0106, -0.0010,  0.0099],\n",
      "        [-0.0078, -0.0134,  0.0005,  ..., -0.0186,  0.0003, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0066, -0.0083, -0.0044,  ...,  0.0203, -0.0052,  0.0068],\n",
      "        [ 0.0173,  0.0101, -0.0096,  ...,  0.0023,  0.0131, -0.0084],\n",
      "        [ 0.0025, -0.0056,  0.0258,  ..., -0.0067, -0.0107, -0.0403]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.self_attn.o_proj.weight tensor([[ 0.0066,  0.0041,  0.0305,  ..., -0.0042,  0.0164, -0.0100],\n",
      "        [ 0.0102, -0.0131, -0.0087,  ...,  0.0014, -0.0116,  0.0215],\n",
      "        [ 0.0128, -0.0036, -0.0118,  ..., -0.0269,  0.0015,  0.0092],\n",
      "        ...,\n",
      "        [-0.0120,  0.0014,  0.0056,  ...,  0.0137, -0.0151,  0.0108],\n",
      "        [ 0.0033,  0.0149, -0.0145,  ...,  0.0121, -0.0256,  0.0024],\n",
      "        [ 0.0178, -0.0248, -0.0046,  ...,  0.0143, -0.0215,  0.0116]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.mlp.gate_proj.weight tensor([[ 0.0014, -0.0047, -0.0222,  ...,  0.0206, -0.0102,  0.0140],\n",
      "        [-0.0186,  0.0049, -0.0103,  ..., -0.0047, -0.0106,  0.0188],\n",
      "        [-0.0110, -0.0080,  0.0586,  ..., -0.0209,  0.0211, -0.0026],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0063,  0.0250,  ..., -0.0217,  0.0067, -0.0029],\n",
      "        [ 0.0123,  0.0062,  0.0104,  ...,  0.0201, -0.0026,  0.0135],\n",
      "        [-0.0071, -0.0245, -0.0148,  ...,  0.0012, -0.0281, -0.0055]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.mlp.down_proj.weight tensor([[-0.0026,  0.0260, -0.0023,  ..., -0.0089, -0.0112, -0.0303],\n",
      "        [ 0.0023,  0.0165, -0.0080,  ...,  0.0078, -0.0189,  0.0233],\n",
      "        [-0.0081,  0.0115, -0.0212,  ..., -0.0058,  0.0225, -0.0186],\n",
      "        ...,\n",
      "        [-0.0327,  0.0297,  0.0033,  ...,  0.0108, -0.0019, -0.0008],\n",
      "        [-0.0115,  0.0059, -0.0166,  ..., -0.0099,  0.0171, -0.0109],\n",
      "        [ 0.0120, -0.0032,  0.0025,  ..., -0.0010, -0.0090, -0.0044]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.mlp.up_proj.weight tensor([[-0.0596, -0.0013,  0.0010,  ...,  0.0130, -0.0123, -0.0101],\n",
      "        [-0.0098,  0.0043, -0.0099,  ..., -0.0444, -0.0171,  0.0366],\n",
      "        [ 0.0129,  0.0342,  0.0165,  ...,  0.0094, -0.0226,  0.0071],\n",
      "        ...,\n",
      "        [-0.0131,  0.0253, -0.0092,  ..., -0.0175, -0.0034,  0.0008],\n",
      "        [ 0.0153, -0.0267, -0.0171,  ...,  0.0027, -0.0244, -0.0359],\n",
      "        [ 0.0137, -0.0229,  0.0284,  ..., -0.0107, -0.0198, -0.0099]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.input_layernorm.weight tensor([0.4531, 0.4668, 0.4375,  ..., 0.4336, 0.4336, 0.4473],\n",
      "       dtype=torch.float16)\n",
      "model.layers.20.post_attention_layernorm.weight tensor([0.3633, 0.3574, 0.3516,  ..., 0.3633, 0.3613, 0.3574],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.self_attn.q_proj.weight tensor([[-0.0181, -0.0054,  0.0166,  ..., -0.0060,  0.0056, -0.0052],\n",
      "        [ 0.0154, -0.0021, -0.0043,  ...,  0.0091,  0.0021,  0.0187],\n",
      "        [-0.0095, -0.0070,  0.0008,  ...,  0.0283, -0.0069,  0.0022],\n",
      "        ...,\n",
      "        [ 0.0024, -0.0082,  0.0295,  ..., -0.0110, -0.0496,  0.0114],\n",
      "        [-0.0126,  0.0168,  0.0197,  ...,  0.0253,  0.0359, -0.0052],\n",
      "        [-0.0747, -0.0110, -0.0092,  ..., -0.0090,  0.0164, -0.0413]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.self_attn.k_proj.weight tensor([[ 0.0041,  0.0107,  0.0046,  ...,  0.0119, -0.0004,  0.0045],\n",
      "        [-0.0022,  0.0109,  0.0141,  ...,  0.0078,  0.0065,  0.0147],\n",
      "        [ 0.0093,  0.0031,  0.0312,  ...,  0.0154,  0.0009,  0.0053],\n",
      "        ...,\n",
      "        [-0.0183,  0.0200,  0.0057,  ...,  0.0215, -0.0352,  0.0452],\n",
      "        [ 0.0005,  0.0038, -0.0138,  ..., -0.0168,  0.0284, -0.0089],\n",
      "        [-0.0232,  0.0054, -0.0214,  ...,  0.0004, -0.0201,  0.0623]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.self_attn.v_proj.weight tensor([[ 0.0011,  0.0074, -0.0023,  ..., -0.0052,  0.0197, -0.0261],\n",
      "        [-0.0276,  0.0260, -0.0068,  ...,  0.0211, -0.0211,  0.0265],\n",
      "        [ 0.0223,  0.0024, -0.0042,  ..., -0.0225,  0.0195, -0.0077],\n",
      "        ...,\n",
      "        [-0.0056, -0.0248,  0.0337,  ...,  0.0014, -0.0022, -0.0167],\n",
      "        [-0.0182, -0.0118,  0.0134,  ...,  0.0066, -0.0009,  0.0171],\n",
      "        [-0.0091,  0.0187, -0.0131,  ...,  0.0055, -0.0154,  0.0002]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.self_attn.o_proj.weight tensor([[-0.0128, -0.0006,  0.0127,  ..., -0.0088,  0.0266,  0.0012],\n",
      "        [-0.0044,  0.0231, -0.0125,  ..., -0.0302,  0.0093, -0.0074],\n",
      "        [-0.0183, -0.0121,  0.0055,  ...,  0.0266,  0.0309,  0.0156],\n",
      "        ...,\n",
      "        [ 0.0087, -0.0079, -0.0139,  ...,  0.0215,  0.0082,  0.0190],\n",
      "        [ 0.0217, -0.0080,  0.0143,  ..., -0.0083,  0.0100, -0.0184],\n",
      "        [-0.0200,  0.0164, -0.0134,  ...,  0.0322, -0.0190,  0.0045]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.mlp.gate_proj.weight tensor([[-0.0027, -0.0205,  0.0111,  ..., -0.0300,  0.0410,  0.0151],\n",
      "        [ 0.0141,  0.0108,  0.0090,  ..., -0.0359,  0.0496,  0.0058],\n",
      "        [ 0.0145,  0.0069,  0.0024,  ..., -0.0425, -0.0048, -0.0198],\n",
      "        ...,\n",
      "        [ 0.0122, -0.0266, -0.0291,  ...,  0.0344, -0.0254,  0.0005],\n",
      "        [-0.0023,  0.0312,  0.0081,  ...,  0.0096, -0.0219,  0.0052],\n",
      "        [ 0.0015, -0.0108, -0.0025,  ..., -0.0159,  0.0013,  0.0072]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.mlp.down_proj.weight tensor([[ 0.0121,  0.0166, -0.0031,  ...,  0.0172,  0.0194,  0.0071],\n",
      "        [ 0.0168, -0.0045, -0.0144,  ...,  0.0018, -0.0131, -0.0132],\n",
      "        [-0.0083,  0.0009,  0.0117,  ..., -0.0012,  0.0354,  0.0206],\n",
      "        ...,\n",
      "        [-0.0126,  0.0378,  0.0234,  ..., -0.0027,  0.0017,  0.0078],\n",
      "        [ 0.0156, -0.0011, -0.0032,  ..., -0.0496, -0.0132,  0.0092],\n",
      "        [ 0.0070,  0.0038,  0.0228,  ..., -0.0135,  0.0181, -0.0216]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.mlp.up_proj.weight tensor([[ 0.0259, -0.0302, -0.0189,  ...,  0.0215,  0.0398, -0.0119],\n",
      "        [-0.0247, -0.0142,  0.0004,  ...,  0.0356,  0.0435,  0.0046],\n",
      "        [ 0.0160, -0.0049, -0.0011,  ...,  0.0045, -0.0132, -0.0073],\n",
      "        ...,\n",
      "        [-0.0457, -0.0049,  0.0178,  ...,  0.0159, -0.0091, -0.0107],\n",
      "        [-0.0024, -0.0131,  0.0079,  ..., -0.0231, -0.0067,  0.0036],\n",
      "        [-0.0101, -0.0262, -0.0184,  ..., -0.0076, -0.0041, -0.0286]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.input_layernorm.weight tensor([0.4785, 0.4863, 0.4688,  ..., 0.4551, 0.4707, 0.4766],\n",
      "       dtype=torch.float16)\n",
      "model.layers.21.post_attention_layernorm.weight tensor([0.3730, 0.3691, 0.3633,  ..., 0.3789, 0.3711, 0.3730],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.self_attn.q_proj.weight tensor([[-0.0236, -0.0150, -0.0093,  ...,  0.0442, -0.0364,  0.0275],\n",
      "        [-0.0413, -0.0171, -0.0032,  ..., -0.0400,  0.0137, -0.0400],\n",
      "        [-0.0315,  0.0245, -0.0159,  ...,  0.0231,  0.0139, -0.0212],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0160,  0.0110,  ...,  0.0146,  0.0542, -0.0046],\n",
      "        [-0.0277, -0.0037, -0.0259,  ...,  0.0493,  0.0035, -0.0220],\n",
      "        [ 0.0277, -0.0121,  0.0116,  ..., -0.0530,  0.0114,  0.0238]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.self_attn.k_proj.weight tensor([[-0.0193, -0.0015, -0.0067,  ...,  0.0145, -0.0149, -0.0197],\n",
      "        [ 0.0036, -0.0147,  0.0298,  ..., -0.0233,  0.0339, -0.0152],\n",
      "        [-0.0183,  0.0332, -0.0449,  ...,  0.0601, -0.0060, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0219,  0.0078, -0.0272,  ..., -0.0221, -0.0150,  0.0164],\n",
      "        [-0.0128, -0.0461, -0.0068,  ...,  0.0007, -0.0081, -0.0026],\n",
      "        [ 0.0277,  0.0071,  0.0332,  ...,  0.0299, -0.0344,  0.0168]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.self_attn.v_proj.weight tensor([[-1.8799e-02,  5.4626e-03, -2.4872e-03,  ..., -4.6387e-02,\n",
      "         -2.5024e-02,  2.1851e-02],\n",
      "        [-3.1738e-02, -1.3123e-02,  7.8125e-03,  ..., -2.4658e-02,\n",
      "         -3.2715e-02, -7.2937e-03],\n",
      "        [ 2.6733e-02, -2.5024e-02, -8.4839e-03,  ...,  4.6875e-02,\n",
      "          6.7520e-04, -1.5320e-02],\n",
      "        ...,\n",
      "        [-1.0742e-02, -4.5410e-02, -2.5269e-02,  ...,  1.4343e-02,\n",
      "         -6.3477e-03,  1.4221e-02],\n",
      "        [-3.6865e-02, -1.2695e-02,  2.3315e-02,  ..., -1.6968e-02,\n",
      "          8.4229e-03, -1.0498e-02],\n",
      "        [-2.9602e-03,  2.0752e-02, -3.9307e-02,  ...,  6.7444e-03,\n",
      "          3.4094e-05,  2.4170e-02]], dtype=torch.float16)\n",
      "model.layers.22.self_attn.o_proj.weight tensor([[ 0.0237,  0.0162, -0.0214,  ..., -0.0029, -0.0264,  0.0089],\n",
      "        [ 0.0334,  0.0133,  0.0033,  ...,  0.0050, -0.0131, -0.0189],\n",
      "        [ 0.0137,  0.0061, -0.0092,  ..., -0.0275,  0.0062, -0.0029],\n",
      "        ...,\n",
      "        [-0.0027, -0.0121, -0.0102,  ..., -0.0161, -0.0183,  0.0039],\n",
      "        [ 0.0209, -0.0097, -0.0203,  ..., -0.0192, -0.0085,  0.0118],\n",
      "        [ 0.0049, -0.0237,  0.0258,  ...,  0.0101,  0.0162,  0.0146]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.mlp.gate_proj.weight tensor([[-0.0173, -0.0155,  0.0254,  ...,  0.0137,  0.0227, -0.0349],\n",
      "        [ 0.0096, -0.0035,  0.0022,  ...,  0.0013,  0.0040,  0.0151],\n",
      "        [ 0.0050, -0.0128,  0.0066,  ...,  0.0121,  0.0081, -0.0070],\n",
      "        ...,\n",
      "        [-0.0030, -0.0011,  0.0026,  ..., -0.0131,  0.0050, -0.0036],\n",
      "        [ 0.0006,  0.0256, -0.0259,  ..., -0.0029,  0.0140, -0.0237],\n",
      "        [-0.0079,  0.0043,  0.0018,  ..., -0.0128,  0.0212,  0.0170]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.22.mlp.down_proj.weight tensor([[ 0.0121, -0.0103,  0.0140,  ..., -0.0135,  0.0057, -0.0420],\n",
      "        [ 0.0037,  0.0197, -0.0092,  ..., -0.0103,  0.0118, -0.0264],\n",
      "        [-0.0064, -0.0072,  0.0069,  ...,  0.0192,  0.0129,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0068, -0.0452,  ..., -0.0053, -0.0150, -0.0201],\n",
      "        [-0.0135, -0.0471,  0.0123,  ...,  0.0198, -0.0262,  0.0334],\n",
      "        [ 0.0118,  0.0003, -0.0347,  ..., -0.0026, -0.0189,  0.0075]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.mlp.up_proj.weight tensor([[ 0.0315,  0.0210,  0.0026,  ...,  0.0108,  0.0322, -0.0037],\n",
      "        [ 0.0104,  0.0082,  0.0188,  ..., -0.0129, -0.0056,  0.0009],\n",
      "        [ 0.0070, -0.0317, -0.0417,  ...,  0.0261,  0.0096,  0.0057],\n",
      "        ...,\n",
      "        [-0.0104,  0.0008, -0.0192,  ..., -0.0031,  0.0075,  0.0046],\n",
      "        [ 0.0165, -0.0130, -0.0203,  ..., -0.0325,  0.0092, -0.0186],\n",
      "        [-0.0022,  0.0008, -0.0041,  ..., -0.0425, -0.0581,  0.0146]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.input_layernorm.weight tensor([0.4863, 0.4863, 0.4746,  ..., 0.4688, 0.4863, 0.4863],\n",
      "       dtype=torch.float16)\n",
      "model.layers.22.post_attention_layernorm.weight tensor([0.3848, 0.3809, 0.3828,  ..., 0.3926, 0.3867, 0.3887],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.self_attn.q_proj.weight tensor([[ 3.8452e-03, -1.5015e-02,  8.4305e-04,  ..., -1.5625e-02,\n",
      "         -6.1035e-04,  2.2411e-05],\n",
      "        [ 1.0071e-02, -2.9907e-03,  1.0803e-02,  ..., -1.7212e-02,\n",
      "         -2.6398e-03,  7.8125e-03],\n",
      "        [-5.4932e-03,  3.0518e-03,  1.8921e-02,  ..., -1.2817e-03,\n",
      "         -1.4893e-02,  6.4087e-03],\n",
      "        ...,\n",
      "        [-2.0874e-02,  6.2500e-02,  1.0742e-02,  ..., -3.7842e-02,\n",
      "         -6.1035e-02, -2.8809e-02],\n",
      "        [-3.6865e-02, -3.2471e-02,  2.2705e-02,  ..., -1.6846e-02,\n",
      "         -2.1667e-03,  1.0254e-02],\n",
      "        [ 1.0376e-02, -5.8838e-02, -1.3351e-03,  ...,  1.0193e-02,\n",
      "          4.0283e-02, -9.0942e-03]], dtype=torch.float16)\n",
      "model.layers.23.self_attn.k_proj.weight tensor([[-0.0019,  0.0060,  0.0042,  ...,  0.0011, -0.0176,  0.0024],\n",
      "        [-0.0159, -0.0014, -0.0143,  ...,  0.0171,  0.0151, -0.0026],\n",
      "        [ 0.0094,  0.0040, -0.0129,  ...,  0.0170,  0.0135, -0.0083],\n",
      "        ...,\n",
      "        [ 0.0059,  0.0295,  0.0537,  ..., -0.0308, -0.0149, -0.0183],\n",
      "        [ 0.0007, -0.0315, -0.0118,  ..., -0.0212, -0.0222,  0.0137],\n",
      "        [-0.0145, -0.0076, -0.0221,  ...,  0.0060,  0.0021, -0.0139]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.self_attn.v_proj.weight tensor([[ 0.0038,  0.0334, -0.0049,  ..., -0.0091, -0.0080, -0.0161],\n",
      "        [-0.0002, -0.0126, -0.0200,  ...,  0.0045, -0.0125,  0.0036],\n",
      "        [ 0.0096, -0.0162, -0.0265,  ...,  0.0126, -0.0476, -0.0288],\n",
      "        ...,\n",
      "        [ 0.0097,  0.0065,  0.0050,  ..., -0.0029,  0.0275,  0.0113],\n",
      "        [ 0.0198,  0.0273,  0.0070,  ..., -0.0009, -0.0079, -0.0143],\n",
      "        [-0.0179, -0.0132, -0.0566,  ...,  0.0097,  0.0081, -0.0317]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.self_attn.o_proj.weight tensor([[-0.0066, -0.0047, -0.0164,  ..., -0.0171,  0.0137, -0.0072],\n",
      "        [-0.0102, -0.0179, -0.0043,  ..., -0.0029,  0.0126, -0.0031],\n",
      "        [ 0.0175,  0.0047,  0.0309,  ..., -0.0140,  0.0050, -0.0217],\n",
      "        ...,\n",
      "        [ 0.0334, -0.0056,  0.0376,  ..., -0.0193,  0.0127,  0.0033],\n",
      "        [-0.0070,  0.0220, -0.0142,  ..., -0.0058, -0.0140, -0.0063],\n",
      "        [ 0.0094, -0.0052, -0.0118,  ..., -0.0058, -0.0001, -0.0286]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.mlp.gate_proj.weight tensor([[-0.0103,  0.0278, -0.0004,  ..., -0.0008,  0.0364,  0.0110],\n",
      "        [-0.0084,  0.0474, -0.0121,  ...,  0.0049, -0.0208,  0.0181],\n",
      "        [-0.0337,  0.0210,  0.0033,  ...,  0.0183, -0.0593,  0.0198],\n",
      "        ...,\n",
      "        [-0.0068,  0.0099,  0.0004,  ..., -0.0098,  0.0104,  0.0020],\n",
      "        [-0.0248,  0.0325, -0.0023,  ...,  0.0013,  0.0140, -0.0271],\n",
      "        [ 0.0023,  0.0097,  0.0128,  ..., -0.0071,  0.0042, -0.0092]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.mlp.down_proj.weight tensor([[-1.9264e-04,  8.9722e-03, -1.7456e-02,  ..., -3.3936e-02,\n",
      "         -1.4648e-02,  2.8687e-02],\n",
      "        [ 6.7749e-03,  1.9897e-02, -6.1035e-03,  ...,  2.6611e-02,\n",
      "          2.6611e-02,  4.6492e-05],\n",
      "        [-4.8523e-03, -1.3245e-02, -2.0264e-02,  ...,  6.7444e-03,\n",
      "          3.5645e-02, -1.2573e-02],\n",
      "        ...,\n",
      "        [ 5.4626e-03, -5.2185e-03,  1.0437e-02,  ...,  1.7822e-02,\n",
      "         -8.6594e-04, -1.6479e-02],\n",
      "        [ 2.5757e-02, -1.0071e-02, -1.3638e-04,  ..., -5.2490e-03,\n",
      "          4.5776e-03, -8.2397e-04],\n",
      "        [-2.5482e-03, -3.0273e-02, -1.2573e-02,  ..., -8.6060e-03,\n",
      "          1.2939e-02, -8.3008e-03]], dtype=torch.float16)\n",
      "model.layers.23.mlp.up_proj.weight tensor([[ 0.0309,  0.0145,  0.0325,  ...,  0.0029, -0.0292, -0.0084],\n",
      "        [ 0.0014,  0.0029, -0.0157,  ..., -0.0046, -0.0159,  0.0106],\n",
      "        [-0.0161,  0.0053, -0.0242,  ...,  0.0156,  0.0115, -0.0055],\n",
      "        ...,\n",
      "        [-0.0103, -0.0142,  0.0177,  ...,  0.0085,  0.0422,  0.0057],\n",
      "        [-0.0006,  0.0112, -0.0388,  ...,  0.0181,  0.0049,  0.0142],\n",
      "        [ 0.0074,  0.0131, -0.0040,  ..., -0.0010, -0.0194,  0.0194]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.input_layernorm.weight tensor([0.5117, 0.5234, 0.5078,  ..., 0.5039, 0.5195, 0.5234],\n",
      "       dtype=torch.float16)\n",
      "model.layers.23.post_attention_layernorm.weight tensor([0.4004, 0.3926, 0.3945,  ..., 0.3984, 0.4004, 0.4023],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.self_attn.q_proj.weight tensor([[ 0.0209, -0.0162,  0.0126,  ...,  0.0403, -0.0186,  0.0014],\n",
      "        [ 0.0245,  0.0129,  0.0054,  ..., -0.0110,  0.0164, -0.0078],\n",
      "        [-0.0041, -0.0082, -0.0214,  ...,  0.0088, -0.0104,  0.0170],\n",
      "        ...,\n",
      "        [-0.0056,  0.0010, -0.0205,  ...,  0.0123, -0.0006, -0.0054],\n",
      "        [-0.0203,  0.0104,  0.0067,  ...,  0.0203, -0.0371,  0.0038],\n",
      "        [-0.0229, -0.0361, -0.0197,  ..., -0.0104,  0.0270,  0.0674]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.self_attn.k_proj.weight tensor([[ 0.0229,  0.0013, -0.0093,  ...,  0.0121, -0.0054,  0.0022],\n",
      "        [ 0.0130, -0.0182,  0.0087,  ..., -0.0238,  0.0227,  0.0087],\n",
      "        [ 0.0092,  0.0041, -0.0442,  ...,  0.0177,  0.0160, -0.0148],\n",
      "        ...,\n",
      "        [ 0.0295,  0.0065,  0.0403,  ...,  0.0122,  0.0209, -0.0435],\n",
      "        [-0.0046,  0.0215,  0.0181,  ...,  0.0344, -0.0157,  0.0010],\n",
      "        [-0.0231, -0.0188, -0.0061,  ...,  0.0029, -0.0069,  0.0591]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.self_attn.v_proj.weight tensor([[ 0.0271, -0.0201, -0.0048,  ..., -0.0022, -0.0016, -0.0023],\n",
      "        [ 0.0315, -0.0227, -0.0099,  ..., -0.0645, -0.0427, -0.0194],\n",
      "        [-0.0522, -0.0167, -0.0164,  ..., -0.0082, -0.0131,  0.0056],\n",
      "        ...,\n",
      "        [-0.0011, -0.0179,  0.0154,  ...,  0.0248, -0.0032,  0.0275],\n",
      "        [-0.0024, -0.0122, -0.0154,  ...,  0.0311, -0.0053,  0.0095],\n",
      "        [ 0.0142,  0.0192,  0.0229,  ...,  0.0234, -0.0157, -0.0087]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.self_attn.o_proj.weight tensor([[-0.0115,  0.0172,  0.0261,  ...,  0.0067,  0.0014, -0.0165],\n",
      "        [-0.0102,  0.0286,  0.0135,  ...,  0.0189,  0.0273, -0.0264],\n",
      "        [-0.0085, -0.0019,  0.0107,  ..., -0.0234,  0.0022, -0.0068],\n",
      "        ...,\n",
      "        [-0.0020,  0.0151,  0.0154,  ..., -0.0075, -0.0080,  0.0019],\n",
      "        [ 0.0505,  0.0058, -0.0052,  ...,  0.0039, -0.0205, -0.0041],\n",
      "        [ 0.0039,  0.0143,  0.0101,  ...,  0.0057, -0.0206,  0.0148]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.mlp.gate_proj.weight tensor([[-2.9663e-02, -1.2939e-02,  1.3000e-02,  ..., -1.0498e-02,\n",
      "         -3.1738e-03, -1.1902e-03],\n",
      "        [-4.0054e-04, -1.3489e-02, -2.7100e-02,  ...,  8.6670e-03,\n",
      "          2.3079e-04, -1.1780e-02],\n",
      "        [-5.9891e-04, -3.3691e-02,  2.5391e-02,  ..., -6.4392e-03,\n",
      "         -4.7119e-02, -1.6846e-02],\n",
      "        ...,\n",
      "        [-5.9891e-04, -3.0151e-02,  4.4556e-03,  ..., -3.9062e-03,\n",
      "          1.1780e-02, -7.1106e-03],\n",
      "        [ 1.6602e-02,  1.7090e-02,  3.3722e-03,  ...,  4.4861e-03,\n",
      "         -4.4441e-04, -3.4332e-03],\n",
      "        [-3.4668e-02,  3.1250e-02, -4.1127e-06,  ...,  3.2471e-02,\n",
      "         -1.4954e-02, -4.4678e-02]], dtype=torch.float16)\n",
      "model.layers.24.mlp.down_proj.weight tensor([[-0.0057, -0.0125, -0.0320,  ..., -0.0003,  0.0039, -0.0205],\n",
      "        [ 0.0270, -0.0087,  0.0153,  ..., -0.0034,  0.0103, -0.0486],\n",
      "        [ 0.0214,  0.0172, -0.0320,  ...,  0.0010,  0.0200,  0.0058],\n",
      "        ...,\n",
      "        [-0.0090,  0.0077, -0.0192,  ..., -0.0344, -0.0126, -0.0220],\n",
      "        [ 0.0101,  0.0253,  0.0059,  ...,  0.0228, -0.0248,  0.0181],\n",
      "        [-0.0203, -0.0069,  0.0055,  ...,  0.0124, -0.0075, -0.0315]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.24.mlp.up_proj.weight tensor([[ 0.0031, -0.0019,  0.0197,  ...,  0.0239,  0.0005,  0.0276],\n",
      "        [-0.0039,  0.0149, -0.0273,  ..., -0.0102,  0.0011, -0.0197],\n",
      "        [ 0.0106, -0.0491,  0.0131,  ...,  0.0417,  0.0175, -0.0327],\n",
      "        ...,\n",
      "        [ 0.0143, -0.0303, -0.0086,  ..., -0.0018,  0.0171,  0.0120],\n",
      "        [ 0.0349, -0.0071,  0.0140,  ...,  0.0048,  0.0133, -0.0095],\n",
      "        [-0.0043, -0.0162,  0.0223,  ...,  0.0255,  0.0349,  0.0187]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.input_layernorm.weight tensor([0.4941, 0.5195, 0.5117,  ..., 0.4863, 0.5156, 0.5078],\n",
      "       dtype=torch.float16)\n",
      "model.layers.24.post_attention_layernorm.weight tensor([0.4102, 0.4062, 0.4082,  ..., 0.4121, 0.4160, 0.4102],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.self_attn.q_proj.weight tensor([[ 0.0033, -0.0132, -0.0197,  ...,  0.0061, -0.0141,  0.0042],\n",
      "        [-0.0126,  0.0050,  0.0128,  ..., -0.0135, -0.0002, -0.0071],\n",
      "        [-0.0013,  0.0056,  0.0026,  ..., -0.0042, -0.0184, -0.0179],\n",
      "        ...,\n",
      "        [ 0.0447,  0.0466,  0.0055,  ...,  0.0027,  0.0149, -0.0288],\n",
      "        [-0.0564, -0.0574, -0.0081,  ..., -0.0150, -0.0236,  0.0055],\n",
      "        [ 0.0403, -0.0422, -0.0243,  ...,  0.0023,  0.0172, -0.0312]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.self_attn.k_proj.weight tensor([[-0.0102, -0.0134, -0.0058,  ..., -0.0006,  0.0050, -0.0020],\n",
      "        [-0.0187, -0.0085,  0.0071,  ..., -0.0064, -0.0214,  0.0106],\n",
      "        [-0.0087,  0.0019, -0.0017,  ...,  0.0092, -0.0080,  0.0074],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0525, -0.0117,  ..., -0.0562, -0.0552,  0.0334],\n",
      "        [ 0.0371, -0.0386,  0.0012,  ..., -0.0030,  0.0082,  0.0150],\n",
      "        [-0.0107, -0.0232,  0.0024,  ...,  0.0364,  0.0094, -0.0208]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.self_attn.v_proj.weight tensor([[ 0.0085,  0.0110,  0.0059,  ...,  0.0216, -0.0171, -0.0115],\n",
      "        [-0.0056, -0.0072,  0.0046,  ...,  0.0077, -0.0332, -0.0095],\n",
      "        [-0.0155,  0.0088, -0.0167,  ..., -0.0070, -0.0043, -0.0040],\n",
      "        ...,\n",
      "        [-0.0179, -0.0311, -0.0070,  ...,  0.0147,  0.0055,  0.0107],\n",
      "        [-0.0135, -0.0003,  0.0200,  ..., -0.0010,  0.0070, -0.0211],\n",
      "        [-0.0334, -0.0140, -0.0216,  ...,  0.0016,  0.0204,  0.0211]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.self_attn.o_proj.weight tensor([[ 0.0182,  0.0139,  0.0011,  ...,  0.0137,  0.0024,  0.0126],\n",
      "        [-0.0245, -0.0183, -0.0175,  ...,  0.0248,  0.0022, -0.0046],\n",
      "        [-0.0054, -0.0107,  0.0182,  ..., -0.0105, -0.0157,  0.0025],\n",
      "        ...,\n",
      "        [-0.0170,  0.0136, -0.0032,  ...,  0.0124,  0.0131,  0.0276],\n",
      "        [-0.0093, -0.0238, -0.0294,  ..., -0.0219, -0.0104,  0.0078],\n",
      "        [-0.0151, -0.0086, -0.0178,  ..., -0.0430, -0.0240, -0.0152]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.mlp.gate_proj.weight tensor([[-0.0020, -0.0383,  0.0425,  ..., -0.0061, -0.0118,  0.0128],\n",
      "        [-0.0203, -0.0024, -0.0084,  ...,  0.0232,  0.0366,  0.0150],\n",
      "        [-0.0112,  0.0254,  0.0063,  ...,  0.0140,  0.0090,  0.0259],\n",
      "        ...,\n",
      "        [-0.0400,  0.0023,  0.0098,  ..., -0.0035, -0.0187,  0.0049],\n",
      "        [ 0.0199, -0.0047,  0.0588,  ...,  0.0236,  0.0244,  0.0054],\n",
      "        [-0.0086, -0.0029, -0.0116,  ...,  0.0469,  0.0037, -0.0069]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.mlp.down_proj.weight tensor([[ 0.0292, -0.0101,  0.0130,  ...,  0.0181, -0.0054,  0.0299],\n",
      "        [-0.0173,  0.0011, -0.0005,  ...,  0.0136, -0.0031,  0.0236],\n",
      "        [ 0.0161,  0.0101, -0.0018,  ..., -0.0198,  0.0071, -0.0157],\n",
      "        ...,\n",
      "        [ 0.0135, -0.0166, -0.0280,  ..., -0.0449,  0.0017,  0.0240],\n",
      "        [ 0.0021, -0.0063,  0.0047,  ...,  0.0061,  0.0048,  0.0008],\n",
      "        [-0.0088, -0.0092,  0.0100,  ..., -0.0334, -0.0033, -0.0066]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.mlp.up_proj.weight tensor([[-3.0762e-02, -6.5308e-03,  1.6235e-02,  ...,  6.6528e-03,\n",
      "          9.7046e-03, -1.7944e-02],\n",
      "        [-1.9531e-02,  1.4877e-03,  9.8877e-03,  ..., -4.6997e-03,\n",
      "          1.1475e-02, -5.8746e-04],\n",
      "        [ 2.7222e-02,  1.0834e-03,  2.9297e-02,  ..., -1.3428e-02,\n",
      "         -2.1606e-02, -4.6692e-03],\n",
      "        ...,\n",
      "        [ 6.6833e-03,  6.5308e-03,  4.0039e-02,  ...,  7.0801e-03,\n",
      "         -3.0396e-02, -1.9165e-02],\n",
      "        [-2.9419e-02,  1.1536e-02, -3.1982e-02,  ..., -4.1504e-02,\n",
      "         -2.8320e-02,  1.6174e-03],\n",
      "        [-1.4465e-02,  2.7180e-05,  2.9175e-02,  ...,  2.1240e-02,\n",
      "          7.1106e-03,  2.3193e-02]], dtype=torch.float16)\n",
      "model.layers.25.input_layernorm.weight tensor([0.5469, 0.5547, 0.5430,  ..., 0.5508, 0.5625, 0.5508],\n",
      "       dtype=torch.float16)\n",
      "model.layers.25.post_attention_layernorm.weight tensor([0.4180, 0.4160, 0.4199,  ..., 0.4277, 0.4238, 0.4238],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.self_attn.q_proj.weight tensor([[ 0.0332, -0.0053, -0.0087,  ..., -0.0061,  0.0381,  0.0014],\n",
      "        [-0.0084, -0.0168,  0.0106,  ..., -0.0024, -0.0214,  0.0116],\n",
      "        [-0.0004,  0.0044,  0.0103,  ...,  0.0036, -0.0405, -0.0176],\n",
      "        ...,\n",
      "        [-0.0012, -0.0110, -0.0078,  ...,  0.0141,  0.0197,  0.0026],\n",
      "        [ 0.0039, -0.0232, -0.0276,  ...,  0.0156, -0.0049, -0.0104],\n",
      "        [-0.0156,  0.0525,  0.0278,  ..., -0.0108, -0.0605, -0.0457]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.self_attn.k_proj.weight tensor([[ 0.0251,  0.0190, -0.0135,  ..., -0.0229,  0.0347, -0.0006],\n",
      "        [-0.0267,  0.0085,  0.0142,  ..., -0.0012,  0.0204,  0.0315],\n",
      "        [ 0.0109,  0.0278, -0.0044,  ..., -0.0245, -0.0623, -0.0060],\n",
      "        ...,\n",
      "        [ 0.0255,  0.0212,  0.0018,  ..., -0.0442, -0.0513,  0.0244],\n",
      "        [-0.0312, -0.0347, -0.0092,  ...,  0.0447,  0.0249, -0.0010],\n",
      "        [-0.0151,  0.0237, -0.0139,  ..., -0.0003, -0.0183, -0.0114]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.self_attn.v_proj.weight tensor([[ 0.0574, -0.0261, -0.0253,  ..., -0.0148,  0.0084, -0.0139],\n",
      "        [-0.0547, -0.0214,  0.0258,  ...,  0.0052, -0.0073,  0.0069],\n",
      "        [ 0.0021,  0.0018, -0.0278,  ..., -0.0082, -0.0054, -0.0311],\n",
      "        ...,\n",
      "        [-0.0228, -0.0145,  0.0035,  ...,  0.0461, -0.0045,  0.0087],\n",
      "        [ 0.0081, -0.0325, -0.0052,  ..., -0.0160, -0.0171, -0.0005],\n",
      "        [ 0.0061,  0.0223, -0.0110,  ...,  0.0097,  0.0007,  0.0209]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.self_attn.o_proj.weight tensor([[ 0.0117,  0.0023, -0.0294,  ..., -0.0017,  0.0150,  0.0302],\n",
      "        [ 0.0080,  0.0491,  0.0084,  ..., -0.0040,  0.0486, -0.0066],\n",
      "        [-0.0078,  0.0150, -0.0208,  ..., -0.0160,  0.0038, -0.0042],\n",
      "        ...,\n",
      "        [ 0.0294,  0.0029,  0.0126,  ...,  0.0009,  0.0146,  0.0034],\n",
      "        [-0.0126,  0.0076, -0.0090,  ...,  0.0132, -0.0061, -0.0481],\n",
      "        [ 0.0162,  0.0013,  0.0033,  ..., -0.0226,  0.0225, -0.0189]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.mlp.gate_proj.weight tensor([[ 0.0134, -0.0079,  0.0133,  ...,  0.0262,  0.0280, -0.0408],\n",
      "        [-0.0084,  0.0131, -0.0182,  ..., -0.0282, -0.0267,  0.0192],\n",
      "        [ 0.0042,  0.0033,  0.0010,  ...,  0.0068, -0.0073, -0.0179],\n",
      "        ...,\n",
      "        [-0.0051,  0.0354, -0.0089,  ..., -0.0009, -0.0342, -0.0189],\n",
      "        [-0.0206,  0.0139,  0.0247,  ...,  0.0179,  0.0181,  0.0044],\n",
      "        [ 0.0182, -0.0011, -0.0049,  ..., -0.0253, -0.0034, -0.0047]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.mlp.down_proj.weight tensor([[-0.0112, -0.0126, -0.0132,  ...,  0.0309,  0.0142,  0.0060],\n",
      "        [ 0.0124,  0.0139,  0.0047,  ...,  0.0574,  0.0219, -0.0259],\n",
      "        [-0.0187,  0.0093, -0.0267,  ..., -0.0109,  0.0199,  0.0109],\n",
      "        ...,\n",
      "        [-0.0148, -0.0071, -0.0160,  ...,  0.0093,  0.0388,  0.0008],\n",
      "        [-0.0186, -0.0147, -0.0247,  ...,  0.0081,  0.0096,  0.0166],\n",
      "        [ 0.0054, -0.0071, -0.0089,  ...,  0.0120,  0.0172,  0.0121]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.mlp.up_proj.weight tensor([[-0.0152,  0.0396, -0.0024,  ...,  0.0120, -0.0087, -0.0096],\n",
      "        [-0.0364, -0.0137,  0.0024,  ...,  0.0293, -0.0024,  0.0114],\n",
      "        [ 0.0039, -0.0124, -0.0280,  ...,  0.0135, -0.0167, -0.0078],\n",
      "        ...,\n",
      "        [ 0.0084, -0.0249,  0.0027,  ...,  0.0130,  0.0359, -0.0110],\n",
      "        [-0.0437, -0.0025,  0.0547,  ...,  0.0309,  0.0012,  0.0053],\n",
      "        [ 0.0017, -0.0188,  0.0188,  ..., -0.0139,  0.0106, -0.0050]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.input_layernorm.weight tensor([0.5156, 0.5352, 0.5352,  ..., 0.5195, 0.5430, 0.5312],\n",
      "       dtype=torch.float16)\n",
      "model.layers.26.post_attention_layernorm.weight tensor([0.4355, 0.4316, 0.4336,  ..., 0.4434, 0.4414, 0.4395],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.self_attn.q_proj.weight tensor([[-0.0310,  0.0189,  0.0073,  ..., -0.0041,  0.0129,  0.0391],\n",
      "        [-0.0190,  0.0408,  0.0251,  ..., -0.0206,  0.0065,  0.0047],\n",
      "        [-0.0148, -0.0249,  0.0076,  ...,  0.0038, -0.0276,  0.0332],\n",
      "        ...,\n",
      "        [-0.0530,  0.0292, -0.0069,  ...,  0.0231, -0.0172, -0.0179],\n",
      "        [ 0.0026,  0.0317,  0.0159,  ...,  0.0074,  0.0032,  0.0256],\n",
      "        [-0.0243, -0.0134,  0.0077,  ...,  0.0193,  0.0148, -0.0085]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.self_attn.k_proj.weight tensor([[-0.0067, -0.0066,  0.0090,  ..., -0.0039, -0.0143,  0.0099],\n",
      "        [-0.0007,  0.0055,  0.0173,  ...,  0.0435, -0.0088, -0.0099],\n",
      "        [ 0.0093, -0.0166,  0.0051,  ...,  0.0005, -0.0061, -0.0037],\n",
      "        ...,\n",
      "        [-0.0060, -0.0146,  0.0339,  ...,  0.0071, -0.0269, -0.0004],\n",
      "        [ 0.0262, -0.0187, -0.0442,  ...,  0.0610, -0.0104,  0.0471],\n",
      "        [ 0.0300,  0.0145, -0.0045,  ..., -0.0325, -0.0378,  0.0111]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.self_attn.v_proj.weight tensor([[ 0.0211, -0.0057, -0.0374,  ...,  0.0322,  0.0376, -0.0109],\n",
      "        [ 0.0194,  0.0173,  0.0092,  ...,  0.0009, -0.0166,  0.0148],\n",
      "        [-0.0277,  0.0038,  0.0231,  ..., -0.0115,  0.0496, -0.0269],\n",
      "        ...,\n",
      "        [ 0.0082, -0.0199, -0.0210,  ...,  0.0193, -0.0145, -0.0109],\n",
      "        [ 0.0008, -0.0227, -0.0303,  ..., -0.0128, -0.0082, -0.0160],\n",
      "        [ 0.0208, -0.0237, -0.0189,  ..., -0.0280, -0.0305,  0.0200]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.self_attn.o_proj.weight tensor([[ 0.0039, -0.0092,  0.0057,  ...,  0.0317,  0.0099, -0.0283],\n",
      "        [ 0.0036,  0.0315, -0.0222,  ..., -0.0125, -0.0101,  0.0011],\n",
      "        [-0.0013, -0.0139, -0.0164,  ...,  0.0165,  0.0072, -0.0056],\n",
      "        ...,\n",
      "        [ 0.0231, -0.0079, -0.0124,  ...,  0.0143,  0.0469, -0.0023],\n",
      "        [ 0.0327, -0.0221, -0.0104,  ..., -0.0002, -0.0052,  0.0229],\n",
      "        [ 0.0413,  0.0176,  0.0269,  ..., -0.0131,  0.0027, -0.0199]],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.27.mlp.gate_proj.weight tensor([[ 0.0396,  0.0150, -0.0113,  ...,  0.0110,  0.0031, -0.0147],\n",
      "        [-0.0222,  0.0084, -0.0200,  ...,  0.0298,  0.0305,  0.0339],\n",
      "        [ 0.0090,  0.0146, -0.0077,  ..., -0.0162, -0.0242,  0.0302],\n",
      "        ...,\n",
      "        [ 0.0289,  0.0170,  0.0033,  ..., -0.0063,  0.0177, -0.0102],\n",
      "        [-0.0178,  0.0287, -0.0255,  ..., -0.0026, -0.0152,  0.0078],\n",
      "        [ 0.0078, -0.0183, -0.0098,  ..., -0.0067, -0.0145, -0.0007]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.mlp.down_proj.weight tensor([[ 9.0332e-03, -1.9897e-02,  2.9907e-02,  ..., -1.7944e-02,\n",
      "          1.4160e-02,  9.2983e-05],\n",
      "        [-7.4463e-03,  1.2207e-02, -1.4404e-02,  ..., -1.9165e-02,\n",
      "         -2.5146e-02,  7.3242e-03],\n",
      "        [ 3.2471e-02,  1.1902e-02,  5.7678e-03,  ..., -3.2715e-02,\n",
      "          4.1809e-03,  2.8076e-02],\n",
      "        ...,\n",
      "        [-6.3782e-03,  1.1230e-02,  1.3428e-02,  ...,  1.1597e-02,\n",
      "         -5.1025e-02,  3.1006e-02],\n",
      "        [-2.1362e-02, -4.2419e-03, -1.5747e-02,  ...,  1.9409e-02,\n",
      "         -3.3722e-03,  2.3438e-02],\n",
      "        [ 5.3711e-03,  1.4465e-02, -5.1575e-03,  ...,  2.7222e-02,\n",
      "         -7.3547e-03,  2.6398e-03]], dtype=torch.float16)\n",
      "model.layers.27.mlp.up_proj.weight tensor([[-0.0053,  0.0160,  0.0284,  ..., -0.0013,  0.0102, -0.0386],\n",
      "        [-0.0161,  0.0013, -0.0264,  ...,  0.0200, -0.0157, -0.0228],\n",
      "        [ 0.0283,  0.0099,  0.0417,  ...,  0.0293, -0.0056, -0.0159],\n",
      "        ...,\n",
      "        [-0.0068,  0.0289,  0.0294,  ..., -0.0179,  0.0193, -0.0359],\n",
      "        [ 0.0134,  0.0304,  0.0098,  ...,  0.0148, -0.0139, -0.0136],\n",
      "        [-0.0051, -0.0090, -0.0097,  ..., -0.0082, -0.0166,  0.0013]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.input_layernorm.weight tensor([0.5430, 0.5508, 0.5508,  ..., 0.5508, 0.5508, 0.5547],\n",
      "       dtype=torch.float16)\n",
      "model.layers.27.post_attention_layernorm.weight tensor([0.4551, 0.4453, 0.4434,  ..., 0.4512, 0.4551, 0.4492],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.self_attn.q_proj.weight tensor([[-0.0011, -0.0182,  0.0071,  ..., -0.0114, -0.0115, -0.0049],\n",
      "        [ 0.0124, -0.0061, -0.0055,  ...,  0.0125,  0.0011, -0.0008],\n",
      "        [ 0.0131,  0.0055, -0.0079,  ..., -0.0007, -0.0222,  0.0199],\n",
      "        ...,\n",
      "        [-0.0049,  0.0332, -0.0522,  ...,  0.0107, -0.0012, -0.0452],\n",
      "        [ 0.0179, -0.0315, -0.0143,  ...,  0.0254,  0.0413, -0.0608],\n",
      "        [ 0.0108,  0.0154, -0.0493,  ...,  0.0476,  0.0197, -0.0339]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.self_attn.k_proj.weight tensor([[-0.0008, -0.0014,  0.0147,  ..., -0.0046,  0.0093, -0.0211],\n",
      "        [-0.0145,  0.0022, -0.0209,  ..., -0.0136,  0.0036,  0.0079],\n",
      "        [ 0.0072,  0.0085, -0.0178,  ..., -0.0164, -0.0069, -0.0037],\n",
      "        ...,\n",
      "        [-0.0330, -0.0109,  0.0023,  ..., -0.0093, -0.0466, -0.0028],\n",
      "        [ 0.0435, -0.0223,  0.0131,  ...,  0.0146,  0.0371,  0.0327],\n",
      "        [ 0.0193, -0.0123,  0.0041,  ..., -0.0019, -0.0215, -0.0134]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.self_attn.v_proj.weight tensor([[-0.0256,  0.0081,  0.0265,  ..., -0.0178, -0.0276,  0.0178],\n",
      "        [ 0.0206, -0.0146, -0.0082,  ..., -0.0469,  0.0325, -0.0129],\n",
      "        [-0.0064,  0.0178, -0.0040,  ...,  0.0049, -0.0435,  0.0129],\n",
      "        ...,\n",
      "        [ 0.0276,  0.0053, -0.0042,  ...,  0.0142, -0.0013, -0.0249],\n",
      "        [ 0.0439,  0.0076,  0.0408,  ...,  0.0079,  0.0112, -0.0038],\n",
      "        [ 0.0147,  0.0177, -0.0471,  ...,  0.0281,  0.0021,  0.0010]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.self_attn.o_proj.weight tensor([[ 0.0101,  0.0243, -0.0454,  ..., -0.0469,  0.0193,  0.0017],\n",
      "        [-0.0157, -0.0215,  0.0021,  ..., -0.0131,  0.0204,  0.0300],\n",
      "        [ 0.0284,  0.0131, -0.0542,  ...,  0.0260,  0.0238, -0.0192],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0104,  0.0332,  ..., -0.0122, -0.0413, -0.0074],\n",
      "        [-0.0089, -0.0552, -0.0157,  ...,  0.0087, -0.0254, -0.0280],\n",
      "        [-0.0461, -0.0054, -0.0112,  ..., -0.0225, -0.0023,  0.0046]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.mlp.gate_proj.weight tensor([[-0.0013,  0.0034, -0.0035,  ...,  0.0294, -0.0095,  0.0347],\n",
      "        [-0.0243,  0.0200, -0.0121,  ...,  0.0121,  0.0069,  0.0267],\n",
      "        [ 0.0156,  0.0306,  0.0232,  ...,  0.0217,  0.0029,  0.0085],\n",
      "        ...,\n",
      "        [-0.0120,  0.0306,  0.0070,  ...,  0.0146,  0.0126, -0.0112],\n",
      "        [ 0.0092, -0.0325, -0.0146,  ..., -0.0156, -0.0150,  0.0030],\n",
      "        [-0.0288,  0.0165,  0.0139,  ..., -0.0087, -0.0139,  0.0192]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.mlp.down_proj.weight tensor([[ 0.0078,  0.0393, -0.0098,  ...,  0.0085, -0.0061, -0.0260],\n",
      "        [ 0.0022, -0.0320,  0.0171,  ..., -0.0330, -0.0072, -0.0271],\n",
      "        [-0.0164,  0.0259, -0.0085,  ...,  0.0054,  0.0012,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0085, -0.0115,  0.0028,  ..., -0.0247, -0.0265,  0.0113],\n",
      "        [-0.0140, -0.0171,  0.0569,  ..., -0.0203, -0.0009,  0.0097],\n",
      "        [-0.0265,  0.0099, -0.0124,  ...,  0.0242,  0.0342, -0.0160]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.mlp.up_proj.weight tensor([[ 0.0206,  0.0036,  0.0238,  ...,  0.0286,  0.0181,  0.0172],\n",
      "        [-0.0154, -0.0251, -0.0020,  ..., -0.0155, -0.0297, -0.0223],\n",
      "        [ 0.0128,  0.0147, -0.0058,  ...,  0.0096,  0.0029, -0.0228],\n",
      "        ...,\n",
      "        [-0.0139, -0.0048,  0.0265,  ..., -0.0045,  0.0038,  0.0040],\n",
      "        [-0.0231,  0.0046,  0.0001,  ...,  0.0132, -0.0054, -0.0505],\n",
      "        [-0.0085, -0.0270, -0.0046,  ..., -0.0083,  0.0070, -0.0055]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.input_layernorm.weight tensor([0.5625, 0.5664, 0.5586,  ..., 0.5469, 0.5664, 0.5586],\n",
      "       dtype=torch.float16)\n",
      "model.layers.28.post_attention_layernorm.weight tensor([0.4629, 0.4629, 0.4551,  ..., 0.4668, 0.4590, 0.4570],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.self_attn.q_proj.weight tensor([[ 2.7924e-03,  6.8359e-03, -5.4626e-03,  ..., -3.3379e-06,\n",
      "         -4.1809e-03,  5.2795e-03],\n",
      "        [-2.4170e-02, -2.1118e-02, -2.5635e-02,  ..., -1.3062e-02,\n",
      "          6.8665e-04, -4.9133e-03],\n",
      "        [-1.8921e-02,  3.3936e-02, -1.3123e-03,  ..., -7.7248e-05,\n",
      "         -8.9722e-03,  1.4587e-02],\n",
      "        ...,\n",
      "        [-7.6904e-03,  6.0059e-02, -3.4180e-02,  ...,  9.7046e-03,\n",
      "         -1.0010e-02, -3.2471e-02],\n",
      "        [-2.3560e-02, -2.3438e-02,  1.9775e-02,  ..., -8.6060e-03,\n",
      "         -2.2339e-02,  2.1973e-02],\n",
      "        [ 4.8256e-04, -4.1260e-02, -1.1047e-02,  ...,  1.1597e-02,\n",
      "          6.8970e-03, -3.9795e-02]], dtype=torch.float16)\n",
      "model.layers.29.self_attn.k_proj.weight tensor([[ 0.0381, -0.0104, -0.0168,  ..., -0.0020,  0.0068,  0.0026],\n",
      "        [-0.0161,  0.0216, -0.0023,  ..., -0.0219,  0.0087, -0.0275],\n",
      "        [ 0.0183,  0.0089,  0.0188,  ...,  0.0181,  0.0250,  0.0014],\n",
      "        ...,\n",
      "        [-0.0347,  0.0378,  0.0057,  ..., -0.0067, -0.0040, -0.0106],\n",
      "        [ 0.0031, -0.0171, -0.0688,  ..., -0.0106, -0.0110, -0.0391],\n",
      "        [ 0.0160, -0.0095, -0.0562,  ..., -0.0596, -0.0239,  0.0049]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.self_attn.v_proj.weight tensor([[ 1.1841e-02,  5.0354e-03,  2.9907e-02,  ...,  2.3682e-02,\n",
      "          1.6098e-03,  1.9775e-02],\n",
      "        [-3.0365e-03,  2.4170e-02,  4.0771e-02,  ..., -2.0386e-02,\n",
      "          3.9978e-03, -5.8899e-03],\n",
      "        [-1.4404e-02,  5.7373e-03, -8.3618e-03,  ..., -6.2943e-04,\n",
      "          2.8931e-02,  1.2268e-02],\n",
      "        ...,\n",
      "        [ 8.7280e-03,  2.2217e-02,  1.2146e-02,  ..., -2.5513e-02,\n",
      "         -7.1716e-03, -1.5015e-02],\n",
      "        [ 4.3631e-05,  3.4668e-02, -2.4292e-02,  ...,  1.8188e-02,\n",
      "          3.0151e-02, -1.1841e-02],\n",
      "        [ 2.1076e-04, -1.6968e-02,  6.9580e-03,  ...,  3.5706e-03,\n",
      "         -1.4771e-02,  2.5146e-02]], dtype=torch.float16)\n",
      "model.layers.29.self_attn.o_proj.weight tensor([[-0.0046,  0.0344,  0.0322,  ...,  0.0020, -0.0175, -0.0074],\n",
      "        [-0.0203, -0.0217, -0.0094,  ...,  0.0117, -0.0267,  0.0142],\n",
      "        [ 0.0229,  0.0262, -0.0123,  ..., -0.0371,  0.0162, -0.0183],\n",
      "        ...,\n",
      "        [-0.0294, -0.0157,  0.0223,  ..., -0.0081, -0.0092, -0.0176],\n",
      "        [-0.0110, -0.0070,  0.0332,  ..., -0.0014, -0.0153,  0.0065],\n",
      "        [ 0.0120,  0.0110,  0.0045,  ...,  0.0063,  0.0157, -0.0282]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.mlp.gate_proj.weight tensor([[ 0.0090,  0.0262, -0.0103,  ..., -0.0060, -0.0286,  0.0327],\n",
      "        [ 0.0074,  0.0233, -0.0110,  ..., -0.0232,  0.0112,  0.0042],\n",
      "        [ 0.0381,  0.0012, -0.0120,  ..., -0.0113,  0.0267,  0.0381],\n",
      "        ...,\n",
      "        [-0.0144,  0.0075, -0.0151,  ..., -0.0050,  0.0106,  0.0082],\n",
      "        [ 0.0132,  0.0138, -0.0327,  ..., -0.0179, -0.0065,  0.0179],\n",
      "        [ 0.0021,  0.0145,  0.0015,  ...,  0.0215, -0.0197, -0.0144]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.mlp.down_proj.weight tensor([[ 0.0461,  0.0077, -0.0003,  ...,  0.0145, -0.0166, -0.0150],\n",
      "        [ 0.0187,  0.0112,  0.0248,  ...,  0.0299,  0.0075,  0.0018],\n",
      "        [ 0.0019, -0.0186, -0.0121,  ...,  0.0042,  0.0076, -0.0272],\n",
      "        ...,\n",
      "        [ 0.0138, -0.0114,  0.0088,  ..., -0.0159, -0.0051, -0.0209],\n",
      "        [ 0.0284,  0.0188,  0.0310,  ...,  0.0106, -0.0405,  0.0030],\n",
      "        [ 0.0121, -0.0197, -0.0077,  ..., -0.0145, -0.0067,  0.0015]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.mlp.up_proj.weight tensor([[-0.0027, -0.0015,  0.0074,  ...,  0.0162,  0.0053,  0.0095],\n",
      "        [-0.0055,  0.0070,  0.0334,  ...,  0.0036, -0.0216,  0.0342],\n",
      "        [-0.0165, -0.0255, -0.0034,  ..., -0.0459,  0.0023, -0.0371],\n",
      "        ...,\n",
      "        [ 0.0327,  0.0118,  0.0104,  ...,  0.0106,  0.0251,  0.0386],\n",
      "        [-0.0107, -0.0154, -0.0051,  ...,  0.0171,  0.0150, -0.0037],\n",
      "        [ 0.0104, -0.0023,  0.0131,  ..., -0.0057,  0.0171,  0.0045]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.input_layernorm.weight tensor([0.5273, 0.5391, 0.5312,  ..., 0.5273, 0.5352, 0.5547],\n",
      "       dtype=torch.float16)\n",
      "model.layers.29.post_attention_layernorm.weight tensor([0.4688, 0.4707, 0.4668,  ..., 0.4727, 0.4746, 0.4727],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.30.self_attn.q_proj.weight tensor([[-3.7003e-04, -1.4038e-02, -1.3428e-03,  ..., -3.1586e-03,\n",
      "         -5.0964e-03, -2.3804e-03],\n",
      "        [ 1.6357e-02,  1.1597e-02,  1.0986e-02,  ..., -2.5940e-03,\n",
      "         -9.5215e-03, -1.8921e-02],\n",
      "        [-6.1035e-03, -2.1973e-02,  4.6143e-02,  ...,  1.6113e-02,\n",
      "          4.1809e-03, -3.1494e-02],\n",
      "        ...,\n",
      "        [-8.6060e-03, -4.0588e-03, -3.4485e-03,  ...,  1.4465e-02,\n",
      "         -1.0071e-02,  4.6387e-03],\n",
      "        [-1.3065e-04, -1.9775e-02, -1.6113e-02,  ...,  1.7944e-02,\n",
      "          1.0559e-02, -3.2471e-02],\n",
      "        [-6.9336e-02, -1.3550e-02,  3.9795e-02,  ..., -7.2327e-03,\n",
      "         -5.9605e-05, -5.8899e-03]], dtype=torch.float16)\n",
      "model.layers.30.self_attn.k_proj.weight tensor([[ 2.6367e-02, -2.2339e-02,  3.7766e-04,  ..., -9.5825e-03,\n",
      "          1.3184e-02,  8.3618e-03],\n",
      "        [ 3.8086e-02,  1.5747e-02,  4.5166e-03,  ..., -3.7384e-03,\n",
      "         -7.3547e-03, -3.6716e-05],\n",
      "        [-5.4016e-03, -1.1719e-02,  2.4902e-02,  ..., -1.1673e-03,\n",
      "         -8.9111e-03, -1.2589e-03],\n",
      "        ...,\n",
      "        [ 7.4158e-03,  2.6123e-02, -3.2425e-04,  ...,  5.5176e-02,\n",
      "          4.6082e-03, -2.6093e-03],\n",
      "        [ 1.1169e-02, -6.5430e-02,  3.6621e-02,  ...,  6.2256e-03,\n",
      "         -9.2773e-03, -2.3926e-02],\n",
      "        [-3.9062e-02, -1.3428e-02,  1.8311e-02,  ...,  3.2349e-03,\n",
      "          1.3672e-02, -2.0386e-02]], dtype=torch.float16)\n",
      "model.layers.30.self_attn.v_proj.weight tensor([[-0.0024, -0.0354,  0.0256,  ..., -0.0308, -0.0339,  0.0315],\n",
      "        [ 0.0148,  0.0251,  0.0302,  ..., -0.0118,  0.0374,  0.0049],\n",
      "        [-0.0184,  0.0133, -0.0171,  ...,  0.0058, -0.0260, -0.0126],\n",
      "        ...,\n",
      "        [ 0.0187, -0.0058,  0.0007,  ..., -0.0258, -0.0022,  0.0383],\n",
      "        [-0.0018, -0.0469,  0.0219,  ...,  0.0317, -0.0040,  0.0254],\n",
      "        [-0.0011,  0.0054,  0.0161,  ..., -0.0107, -0.0562, -0.0242]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.30.self_attn.o_proj.weight tensor([[-0.0067,  0.0102, -0.0112,  ...,  0.0078, -0.0172, -0.0222],\n",
      "        [-0.0203, -0.0199, -0.0007,  ...,  0.0168, -0.0211, -0.0209],\n",
      "        [-0.0188, -0.0032,  0.0178,  ..., -0.0518, -0.0151, -0.0117],\n",
      "        ...,\n",
      "        [ 0.0066,  0.0045,  0.0034,  ..., -0.0258,  0.0084,  0.0143],\n",
      "        [ 0.0271, -0.0123,  0.0130,  ...,  0.0305, -0.0024,  0.0081],\n",
      "        [-0.0054,  0.0114,  0.0043,  ..., -0.0027,  0.0132, -0.0165]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.30.mlp.gate_proj.weight tensor([[-0.0334, -0.0315, -0.0045,  ...,  0.0122, -0.0076,  0.0036],\n",
      "        [-0.0192, -0.0035,  0.0322,  ..., -0.0036, -0.0026,  0.0315],\n",
      "        [-0.0219, -0.0107, -0.0303,  ...,  0.0140,  0.0087, -0.0057],\n",
      "        ...,\n",
      "        [-0.0244, -0.0114,  0.0197,  ..., -0.0139, -0.0229, -0.0153],\n",
      "        [ 0.0315,  0.0250, -0.0129,  ...,  0.0139, -0.0396,  0.0027],\n",
      "        [-0.0085, -0.0242, -0.0216,  ..., -0.0016,  0.0103, -0.0222]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.30.mlp.down_proj.weight tensor([[ 1.9531e-02, -1.4648e-02, -3.1738e-02,  ...,  2.8076e-02,\n",
      "         -5.0659e-03,  6.6528e-03],\n",
      "        [ 2.5757e-02, -7.3242e-04,  4.3701e-02,  ...,  1.4648e-02,\n",
      "          4.4861e-03, -7.6294e-03],\n",
      "        [ 2.2583e-03, -7.8735e-03, -1.9897e-02,  ..., -7.5912e-04,\n",
      "          1.2207e-02,  1.9165e-02],\n",
      "        ...,\n",
      "        [-3.1738e-02,  4.3640e-03, -2.3315e-02,  ...,  2.0874e-02,\n",
      "         -2.1729e-02, -1.1719e-02],\n",
      "        [-5.8746e-04, -3.1982e-02,  3.5095e-03,  ...,  2.0752e-02,\n",
      "          2.4414e-03,  2.3723e-05],\n",
      "        [-6.3782e-03, -1.3306e-02, -2.6367e-02,  ...,  1.0254e-02,\n",
      "         -2.9144e-03, -1.7822e-02]], dtype=torch.float16)\n",
      "model.layers.30.mlp.up_proj.weight tensor([[-0.0010,  0.0090,  0.0110,  ...,  0.0164,  0.0219,  0.0026],\n",
      "        [-0.0286, -0.0287,  0.0354,  ...,  0.0002, -0.0106, -0.0159],\n",
      "        [-0.0337, -0.0217, -0.0386,  ..., -0.0170, -0.0074,  0.0157],\n",
      "        ...,\n",
      "        [-0.0206, -0.0125,  0.0146,  ...,  0.0168, -0.0134, -0.0216],\n",
      "        [-0.0087,  0.0043, -0.0137,  ...,  0.0093, -0.0146,  0.0172],\n",
      "        [-0.0059, -0.0228,  0.0013,  ..., -0.0141,  0.0018,  0.0176]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.30.input_layernorm.weight tensor([0.5742, 0.5820, 0.5625,  ..., 0.5508, 0.5625, 0.5820],\n",
      "       dtype=torch.float16)\n",
      "model.layers.30.post_attention_layernorm.weight tensor([0.4785, 0.4883, 0.4785,  ..., 0.4805, 0.4824, 0.4785],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.self_attn.q_proj.weight tensor([[-0.0332,  0.0133,  0.0212,  ...,  0.0168, -0.0008, -0.0210],\n",
      "        [-0.0082, -0.0215, -0.0337,  ...,  0.0026, -0.0034, -0.0074],\n",
      "        [-0.0126,  0.0199,  0.0170,  ...,  0.0017,  0.0026,  0.0017],\n",
      "        ...,\n",
      "        [ 0.0161, -0.0076, -0.0208,  ..., -0.0126,  0.0242, -0.0234],\n",
      "        [-0.0194,  0.0040, -0.0098,  ..., -0.0021,  0.0012,  0.0034],\n",
      "        [-0.0549, -0.0082,  0.0303,  ...,  0.0410,  0.0121,  0.0300]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.self_attn.k_proj.weight tensor([[-0.0126, -0.0219,  0.0136,  ...,  0.0066, -0.0023, -0.0223],\n",
      "        [ 0.0250, -0.0125,  0.0127,  ...,  0.0085, -0.0003, -0.0071],\n",
      "        [ 0.0091,  0.0018, -0.0088,  ..., -0.0028,  0.0084, -0.0276],\n",
      "        ...,\n",
      "        [ 0.0237, -0.0330, -0.0233,  ..., -0.0121, -0.0133, -0.0488],\n",
      "        [ 0.0159,  0.0172,  0.0170,  ..., -0.0229, -0.0216,  0.0198],\n",
      "        [-0.0771, -0.0320,  0.0109,  ..., -0.0105, -0.0004,  0.0040]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.self_attn.v_proj.weight tensor([[ 0.0130, -0.0278, -0.0049,  ...,  0.0087,  0.0006,  0.0131],\n",
      "        [ 0.0214,  0.0077,  0.0175,  ...,  0.0057,  0.0364,  0.0131],\n",
      "        [ 0.0121, -0.0034, -0.0160,  ...,  0.0140,  0.0001,  0.0315],\n",
      "        ...,\n",
      "        [-0.0028, -0.0085, -0.0140,  ..., -0.0178, -0.0027,  0.0260],\n",
      "        [ 0.0109, -0.0242,  0.0165,  ..., -0.0097, -0.0292,  0.0100],\n",
      "        [-0.0118, -0.0255,  0.0275,  ...,  0.0206, -0.0052,  0.0398]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.self_attn.o_proj.weight tensor([[ 0.0067,  0.0277, -0.0039,  ..., -0.0045, -0.0116, -0.0491],\n",
      "        [-0.0078,  0.0040, -0.0267,  ...,  0.0262, -0.0190, -0.0151],\n",
      "        [ 0.0113, -0.0002,  0.0024,  ..., -0.0153,  0.0254,  0.0088],\n",
      "        ...,\n",
      "        [ 0.0075, -0.0164,  0.0272,  ..., -0.0198, -0.0108, -0.0162],\n",
      "        [ 0.0018,  0.0203, -0.0212,  ..., -0.0223, -0.0140, -0.0187],\n",
      "        [ 0.0081,  0.0160, -0.0103,  ...,  0.0334, -0.0190,  0.0063]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.mlp.gate_proj.weight tensor([[ 0.0024, -0.0095,  0.0304,  ...,  0.0154,  0.0179,  0.0029],\n",
      "        [-0.0728, -0.0264,  0.0028,  ...,  0.0093, -0.0172,  0.0178],\n",
      "        [ 0.0179,  0.0030, -0.0118,  ..., -0.0040, -0.0151, -0.0076],\n",
      "        ...,\n",
      "        [-0.0151, -0.0254,  0.0099,  ...,  0.0036,  0.0184,  0.0312],\n",
      "        [ 0.0337,  0.0165, -0.0330,  ...,  0.0364, -0.0136, -0.0089],\n",
      "        [ 0.0236, -0.0325, -0.0312,  ..., -0.0147,  0.0137,  0.0058]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.mlp.down_proj.weight tensor([[-0.0022, -0.0276, -0.0073,  ..., -0.0371, -0.0150,  0.0234],\n",
      "        [ 0.0422,  0.0020, -0.0052,  ..., -0.0020, -0.0156,  0.0132],\n",
      "        [-0.0141, -0.0110,  0.0250,  ...,  0.0312, -0.0022,  0.0238],\n",
      "        ...,\n",
      "        [ 0.0028,  0.0325, -0.0007,  ...,  0.0107,  0.0059,  0.0212],\n",
      "        [ 0.0006,  0.0476, -0.0081,  ..., -0.0082,  0.0215,  0.0221],\n",
      "        [ 0.0253, -0.0361,  0.0171,  ...,  0.0058,  0.0018,  0.0265]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.mlp.up_proj.weight tensor([[-0.0413, -0.0168,  0.0293,  ...,  0.0072, -0.0114,  0.0031],\n",
      "        [ 0.0223,  0.0320,  0.0063,  ..., -0.0188,  0.0012, -0.0009],\n",
      "        [-0.0066,  0.0075, -0.0190,  ...,  0.0106,  0.0021, -0.0102],\n",
      "        ...,\n",
      "        [ 0.0129, -0.0109, -0.0007,  ...,  0.0014,  0.0349, -0.0176],\n",
      "        [ 0.0056,  0.0025, -0.0227,  ...,  0.0256,  0.0122, -0.0005],\n",
      "        [ 0.0420, -0.0381, -0.0028,  ..., -0.0070,  0.0036,  0.0253]],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.input_layernorm.weight tensor([0.4863, 0.4844, 0.4355,  ..., 0.4316, 0.4551, 0.4805],\n",
      "       dtype=torch.float16)\n",
      "model.layers.31.post_attention_layernorm.weight tensor([0.4336, 0.4375, 0.4414,  ..., 0.4238, 0.4102, 0.4277],\n",
      "       dtype=torch.float16)\n",
      "model.norm.weight tensor([1.8672, 1.8672, 1.8047,  ..., 1.7188, 1.8281, 1.6016],\n",
      "       dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
      "        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],\n",
      "        [-0.0125,  0.0036,  0.0195,  ..., -0.0271,  0.0143, -0.0082],\n",
      "        ...,\n",
      "        [-0.0281, -0.0195, -0.0024,  ...,  0.0123, -0.0117, -0.0237],\n",
      "        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],\n",
      "        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Importing libraries for dataframe creation\n",
    "# and graph plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Creating our own dataframe\n",
    "data = {\"Name\": [\"embed_tokens\", \"self_attn\", \"mlp\", \"input_layernorm\", \"post_attn_layernorm\", \"norm\",\"lm_head\"],\n",
    "        \"Marks\": [131_072_000/6738415616, 32*67_108_864/6738415616, 32*135_266_304/6738415616, 32*4_096/6738415616, 32*4_096/6738415616, 4_096/6738415616, 131_072_000/6738415616]}\n",
    " \n",
    "# Now convert this dictionary type data into a pandas dataframe\n",
    "# specifying what are the column names\n",
    "df = pd.DataFrame(data, columns=['Name', 'Marks'])\n",
    "\n",
    "# Defining the plot size\n",
    "plt.figure(figsize=(6,6))\n",
    " \n",
    "# Defining the values for x-axis, y-axis\n",
    "# and from which dataframe the values are to be picked\n",
    "plots = sns.barplot(x=\"Name\", y=\"Marks\", data=df)\n",
    " \n",
    "# Iterating over the bars one-by-one\n",
    "for bar in plots.patches:\n",
    "   \n",
    "  # Using Matplotlib's annotate function and\n",
    "  # passing the coordinates where the annotation shall be done\n",
    "  # x-coordinate: bar.get_x() + bar.get_width() / 2\n",
    "  # y-coordinate: bar.get_height()\n",
    "  # free space to be left to make graph pleasing: (0, 8)\n",
    "  # ha and va stand for the horizontal and vertical alignment\n",
    "    plots.annotate(format(bar.get_height()*100, '.4f')+'%', \n",
    "                   (bar.get_x() + bar.get_width() / 2, \n",
    "                    bar.get_height()), ha='center', va='center',\n",
    "                   size=10, xytext=(0, 8),\n",
    "                   textcoords='offset points')\n",
    " \n",
    "# Setting the label for x-axis\n",
    "plt.xlabel(\"Layer Name\", size=14)\n",
    "\n",
    "plt.xticks(rotation = 50)\n",
    " \n",
    "# Setting the label for y-axis\n",
    "plt.ylabel(\"Parameter Percentage\", size=14)\n",
    " \n",
    "# Setting the title for the graph\n",
    "plt.title(\"Llama-2-7b-hf\")\n",
    " \n",
    "# Finally showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load the Phi-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_auth_token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                              use_auth_token\u001b[38;5;241m=\u001b[39mtoken,  torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", use_auth_token=token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\",\n",
    "                                             use_auth_token=token,  torch_dtype=torch.float16, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (4.28.1)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/85/f6/c5065913119c41ecad148c34e3a861f719e16b89a522287213698da911fc/transformers-4.37.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/be/72/1be614ce1e7da45d7bddaa50f5f491bc56c586d0acd2e30e4575c4b42aa3/tokenizers-0.15.1-cp311-none-win_amd64.whl.metadata\n",
      "  Using cached tokenizers-0.15.1-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bdeng2\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Using cached tokenizers-0.15.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\bdeng2\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-40m33q9i\\\\tokenizers.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in Phi-2: 2779683840\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in Phi-2: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2779683840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "131_072_000 + 32 * ( (6_553_600+2_560)*4  + (26_214_400*2 + 10_240 + 2_560) + 2_560 * 2) + 2_560*2 + 131_072_000 + 51200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing libraries for dataframe creation\n",
    "# and graph plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Creating our own dataframe\n",
    "data = {\"Name\": [\"embed_tokens\", \"self_attn\", \"mlp\", \"input_layernorm\", \"final_layernorm\",\"lm_head\"],\n",
    "        \"Marks\": [131_072_000/2779683840, (6_553_600+2_560)*4*32/2779683840,(26_214_400*2 + 10_240 + 2_560)*32/2779683840, 2_560*2*32/2779683840, 2_560*2/2779683840,(131_072_000 + 51200)/2779683840]}\n",
    " \n",
    "# Now convert this dictionary type data into a pandas dataframe\n",
    "# specifying what are the column names\n",
    "df = pd.DataFrame(data, columns=['Name', 'Marks'])\n",
    "\n",
    "# Defining the plot size\n",
    "plt.figure(figsize=(6,6))\n",
    " \n",
    "# Defining the values for x-axis, y-axis\n",
    "# and from which dataframe the values are to be picked\n",
    "plots = sns.barplot(x=\"Name\", y=\"Marks\", data=df)\n",
    " \n",
    "# Iterating over the bars one-by-one\n",
    "for bar in plots.patches:\n",
    "   \n",
    "  # Using Matplotlib's annotate function and\n",
    "  # passing the coordinates where the annotation shall be done\n",
    "  # x-coordinate: bar.get_x() + bar.get_width() / 2\n",
    "  # y-coordinate: bar.get_height()\n",
    "  # free space to be left to make graph pleasing: (0, 8)\n",
    "  # ha and va stand for the horizontal and vertical alignment\n",
    "    plots.annotate(format(bar.get_height()*100, '.4f')+'%', \n",
    "                   (bar.get_x() + bar.get_width() / 2, \n",
    "                    bar.get_height()), ha='center', va='center',\n",
    "                   size=10, xytext=(0, 8),\n",
    "                   textcoords='offset points')\n",
    " \n",
    "# Setting the label for x-axis\n",
    "plt.xlabel(\"Layer Name\", size=14)\n",
    "\n",
    "plt.xticks(rotation = 50)\n",
    " \n",
    "# Setting the label for y-axis\n",
    "plt.ylabel(\"Parameter Percentage\", size=14)\n",
    " \n",
    "# Setting the title for the graph\n",
    "plt.title(\"phi-2\")\n",
    " \n",
    "# Finally showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001DC75E426C0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.6998e-02, -1.3275e-02,  2.0309e-02,  ...,  1.7822e-02,\n",
      "          5.1346e-03, -7.7343e-04],\n",
      "        [ 9.7351e-03,  5.1636e-02,  1.5656e-02,  ..., -6.7329e-03,\n",
      "          6.9389e-03, -1.1322e-02],\n",
      "        [-4.6387e-02, -9.0942e-03, -1.1349e-03,  ..., -3.0945e-02,\n",
      "          3.8940e-02,  1.3847e-02],\n",
      "        ...,\n",
      "        [-5.9605e-08,  5.9605e-08, -5.9605e-08,  ...,  1.5140e-05,\n",
      "         -1.1206e-05,  1.7762e-05],\n",
      "        [-0.0000e+00, -5.9605e-08, -1.1921e-07,  ..., -2.5094e-05,\n",
      "          3.7730e-05,  2.0683e-05],\n",
      "        [ 0.0000e+00, -5.9605e-08,  5.9605e-08,  ..., -1.6034e-05,\n",
      "         -1.5676e-05, -3.5882e-05]], dtype=torch.float16, requires_grad=True) 131072000\n",
      "Parameter containing:\n",
      "tensor([[ 0.0465, -0.0080, -0.0411,  ..., -0.0539,  0.0262,  0.0199],\n",
      "        [-0.0557, -0.0238, -0.0144,  ...,  0.0227,  0.0007, -0.0386],\n",
      "        [-0.0320, -0.0162, -0.0062,  ..., -0.0003,  0.0136,  0.0128],\n",
      "        ...,\n",
      "        [-0.0551, -0.0406,  0.0073,  ...,  0.0312,  0.0184,  0.0532],\n",
      "        [-0.0391,  0.0524, -0.0017,  ...,  0.0612, -0.0225,  0.0071],\n",
      "        [ 0.0375, -0.0342, -0.0867,  ..., -0.0035, -0.0148,  0.0370]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0104, -0.1927, -0.1383,  ..., -0.0007,  0.0056, -0.0108],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0429,  0.0030,  0.0584,  ..., -0.0086,  0.0076, -0.0075],\n",
      "        [ 0.0677, -0.0022,  0.0099,  ...,  0.0177,  0.0190,  0.0215],\n",
      "        [ 0.0529,  0.0881, -0.0202,  ...,  0.0277,  0.0374,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0030,  0.0136,  0.0154,  ...,  0.0297,  0.0058,  0.0361],\n",
      "        [ 0.1032, -0.0361, -0.0049,  ..., -0.0484, -0.0207, -0.0285],\n",
      "        [-0.0304,  0.0423,  0.0893,  ..., -0.0007, -0.0163, -0.0179]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0904, -0.2859, -0.1951,  ..., -0.0099,  0.0070,  0.0075],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-4.0192e-02, -3.0022e-03, -1.0452e-02,  ...,  1.3588e-02,\n",
      "          6.4507e-03, -5.3436e-02],\n",
      "        [-4.5280e-03, -8.2474e-03,  2.3727e-02,  ...,  7.0724e-03,\n",
      "          2.4826e-02,  7.3120e-02],\n",
      "        [ 1.4107e-02,  1.9226e-02, -3.7365e-03,  ..., -1.8372e-02,\n",
      "         -1.8021e-02, -5.3177e-03],\n",
      "        ...,\n",
      "        [-5.0934e-02,  7.0076e-03,  1.0803e-02,  ..., -3.7354e-02,\n",
      "         -2.5269e-02, -5.4626e-03],\n",
      "        [ 1.3550e-02, -2.4292e-02, -1.2405e-02,  ...,  3.8025e-02,\n",
      "         -3.4424e-02, -3.5217e-02],\n",
      "        [-3.7964e-02,  2.6474e-02, -6.3133e-03,  ..., -1.2054e-02,\n",
      "         -2.7130e-02,  3.4809e-05]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0001,  0.0042, -0.0034,  ..., -0.0035, -0.0052,  0.0022],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 7.4005e-03,  6.9275e-03, -2.6108e-02,  ...,  5.9128e-03,\n",
      "          2.4475e-02, -2.4719e-02],\n",
      "        [ 2.9236e-02,  8.4257e-04, -1.1093e-02,  ..., -1.7303e-02,\n",
      "         -2.2629e-02, -6.9499e-05],\n",
      "        [ 6.9351e-03,  2.3087e-02,  2.5063e-03,  ..., -8.8806e-03,\n",
      "          1.9196e-02, -1.5236e-02],\n",
      "        ...,\n",
      "        [ 2.0294e-03, -1.7138e-03,  2.8091e-02,  ...,  6.6147e-03,\n",
      "         -1.8299e-05,  1.3466e-02],\n",
      "        [ 1.0735e-02,  5.3772e-02,  1.0033e-02,  ...,  1.1055e-02,\n",
      "          1.1742e-02,  2.3499e-02],\n",
      "        [ 3.3379e-03,  8.2626e-03,  2.3865e-02,  ...,  3.4302e-02,\n",
      "         -3.8795e-03,  3.0197e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0291,  0.0069,  0.0309,  ..., -0.0225, -0.0260,  0.0126],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-3.9520e-02,  2.5146e-02,  2.8549e-02,  ...,  8.4915e-03,\n",
      "          3.6240e-03, -3.0701e-02],\n",
      "        [ 1.8740e-03,  6.1378e-03, -1.4717e-02,  ..., -2.8748e-02,\n",
      "          3.6097e-04, -1.1101e-03],\n",
      "        [-2.9488e-03,  2.2873e-02, -4.3945e-02,  ...,  2.8782e-03,\n",
      "         -2.1561e-02,  1.5556e-02],\n",
      "        ...,\n",
      "        [-2.7728e-04, -4.5240e-05, -5.2643e-04,  ..., -6.5947e-04,\n",
      "          1.3151e-03,  8.2302e-04],\n",
      "        [-4.6420e-04, -5.1308e-04,  1.2708e-04,  ...,  8.3351e-04,\n",
      "          2.2185e-04,  5.7602e-04],\n",
      "        [-1.7624e-03, -5.2691e-04,  4.9305e-04,  ...,  8.6498e-04,\n",
      "         -4.1032e-04, -5.0688e-04]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0089, -0.0109, -0.0138,  ..., -0.0003, -0.0011, -0.0001],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 2.2308e-02, -5.2605e-03,  3.6697e-03,  ...,  3.4070e-04,\n",
      "         -2.3019e-04, -8.0156e-04],\n",
      "        [ 1.9714e-02,  4.9171e-03,  3.3813e-02,  ...,  2.2030e-04,\n",
      "         -4.4227e-04,  1.4544e-03],\n",
      "        [-2.7828e-03, -2.3010e-02, -5.4810e-02,  ...,  5.7983e-04,\n",
      "         -6.9237e-04, -5.9843e-04],\n",
      "        ...,\n",
      "        [ 1.7944e-02, -1.6647e-02, -3.0945e-02,  ..., -2.1803e-04,\n",
      "          1.4353e-04,  1.8907e-04],\n",
      "        [ 1.2102e-03, -5.3291e-03,  1.3626e-02,  ...,  1.5497e-06,\n",
      "         -3.4165e-04,  5.3310e-04],\n",
      "        [-1.6190e-02, -1.0010e-02, -8.0109e-03,  ..., -4.9305e-04,\n",
      "         -8.4925e-04,  3.1972e-04]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0034, -0.0016,  0.0121,  ..., -0.0100, -0.0020,  0.0042],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.2576, 0.2559, 0.2637,  ..., 0.2571, 0.2544, 0.2524],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0062,  0.0076,  0.0087,  ..., -0.0238, -0.0254, -0.0201],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-2.0218e-03,  1.1345e-02,  3.1586e-02,  ..., -1.3412e-02,\n",
      "          5.0293e-02,  7.0877e-03],\n",
      "        [-1.5366e-02, -2.9099e-02,  3.4790e-02,  ..., -5.3009e-02,\n",
      "         -5.6366e-02,  3.0151e-02],\n",
      "        [-2.3239e-02,  2.5955e-02,  1.3924e-02,  ...,  7.8278e-03,\n",
      "         -4.0412e-05, -2.8336e-02],\n",
      "        ...,\n",
      "        [-1.0933e-02, -6.6711e-02, -2.5085e-02,  ...,  1.4328e-02,\n",
      "          2.8210e-03,  5.1270e-02],\n",
      "        [-6.0303e-02,  3.2715e-02,  2.1114e-03,  ..., -5.1544e-02,\n",
      "         -8.1711e-03, -5.4245e-03],\n",
      "        [-4.5959e-02,  3.1555e-02, -5.7907e-03,  ..., -4.8096e-02,\n",
      "          6.1646e-03,  2.4841e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0260, -0.0136,  0.0460,  ..., -0.0315,  0.0221, -0.0188],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0084,  0.0179, -0.0504,  ...,  0.0111, -0.0127, -0.0345],\n",
      "        [ 0.0349,  0.0383,  0.0444,  ...,  0.0472,  0.0125, -0.0153],\n",
      "        [-0.0442, -0.0472,  0.0364,  ...,  0.0113,  0.0319,  0.0153],\n",
      "        ...,\n",
      "        [ 0.0345, -0.0038,  0.0418,  ..., -0.0025, -0.0600, -0.0061],\n",
      "        [ 0.0098, -0.0190, -0.0034,  ..., -0.0869,  0.0569,  0.0242],\n",
      "        [ 0.0213, -0.0421, -0.0175,  ...,  0.0676,  0.0601, -0.0167]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0420, -0.0201, -0.0484,  ..., -0.0077,  0.0058, -0.0015],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0016, -0.0093,  0.0304,  ..., -0.0376, -0.0155,  0.0004],\n",
      "        [-0.0067,  0.0289,  0.0080,  ...,  0.0197, -0.0317,  0.0019],\n",
      "        [ 0.0262, -0.0093, -0.0365,  ..., -0.0200, -0.0039,  0.0089],\n",
      "        ...,\n",
      "        [-0.0278, -0.0402,  0.0039,  ...,  0.0092, -0.0044,  0.0070],\n",
      "        [ 0.0149, -0.0132,  0.0078,  ...,  0.0055,  0.0032,  0.0069],\n",
      "        [ 0.0013,  0.0516, -0.0078,  ...,  0.0126,  0.0002, -0.0133]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0025,  0.0007,  0.0035,  ...,  0.0012, -0.0008, -0.0002],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0065,  0.0210,  0.0005,  ...,  0.0022,  0.0194,  0.0019],\n",
      "        [ 0.0071,  0.0123, -0.0010,  ..., -0.0343, -0.0212,  0.0255],\n",
      "        [-0.0197, -0.0363,  0.0338,  ...,  0.0112, -0.0016, -0.0487],\n",
      "        ...,\n",
      "        [ 0.0026,  0.0116, -0.0202,  ..., -0.0114,  0.0029,  0.0021],\n",
      "        [ 0.0037,  0.0038, -0.0147,  ..., -0.0022,  0.0201,  0.0117],\n",
      "        [-0.0224,  0.0134,  0.0204,  ...,  0.0121,  0.0024,  0.0111]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0011,  0.0139,  0.0414,  ...,  0.0316,  0.0585,  0.0575],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0145,  0.0457, -0.0235,  ..., -0.0323,  0.0764,  0.0311],\n",
      "        [ 0.0192,  0.0392, -0.0286,  ...,  0.0237,  0.0087,  0.0057],\n",
      "        [ 0.0122,  0.0378,  0.0417,  ...,  0.0300,  0.0389,  0.0215],\n",
      "        ...,\n",
      "        [ 0.0146, -0.0099,  0.0065,  ..., -0.0194,  0.0074, -0.0074],\n",
      "        [ 0.0091, -0.0005, -0.0054,  ..., -0.0112, -0.0044, -0.0148],\n",
      "        [-0.0257,  0.0262, -0.0061,  ..., -0.0200, -0.0267,  0.0157]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0759, -0.0588, -0.0675,  ...,  0.0042, -0.0092, -0.0007],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0034,  0.0057,  0.0014,  ..., -0.0075, -0.0038,  0.0181],\n",
      "        [-0.0262, -0.0056,  0.0222,  ...,  0.0089,  0.0019, -0.0166],\n",
      "        [-0.0616, -0.0048,  0.0175,  ..., -0.0107,  0.0098,  0.0038],\n",
      "        ...,\n",
      "        [-0.0261, -0.0081,  0.0220,  ...,  0.0141,  0.0078,  0.0123],\n",
      "        [ 0.0418,  0.0533, -0.0064,  ...,  0.0037, -0.0015,  0.0095],\n",
      "        [-0.0383, -0.0220, -0.0223,  ..., -0.0018,  0.0096, -0.0063]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0166, -0.0105,  0.0282,  ...,  0.0156, -0.0084,  0.0034],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.3474, 0.3384, 0.2546,  ..., 0.4014, 0.4192, 0.3977],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0181, -0.0461, -0.0308,  ..., -0.0376, -0.0509, -0.0167],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0059,  0.0290,  0.0005,  ...,  0.0329, -0.0093, -0.0257],\n",
      "        [ 0.0027, -0.0081,  0.0143,  ..., -0.0427,  0.0196, -0.0190],\n",
      "        [-0.0030, -0.0220, -0.0092,  ..., -0.0033, -0.0189,  0.0076],\n",
      "        ...,\n",
      "        [-0.0119, -0.0231,  0.0090,  ..., -0.0061,  0.0257,  0.0077],\n",
      "        [ 0.0129, -0.0267, -0.0479,  ...,  0.0257,  0.0108, -0.0038],\n",
      "        [-0.0892,  0.0178, -0.0159,  ...,  0.0544, -0.0253, -0.0024]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0044,  0.0092, -0.0077,  ..., -0.0303,  0.0091,  0.0315],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0168, -0.0074, -0.0275,  ..., -0.0101, -0.0129, -0.0201],\n",
      "        [ 0.0204,  0.0087,  0.0083,  ...,  0.0291, -0.0171, -0.0007],\n",
      "        [ 0.0058,  0.0183,  0.0105,  ...,  0.0226, -0.0091, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0152, -0.0304, -0.0336,  ...,  0.0378, -0.1079, -0.0229],\n",
      "        [-0.0047,  0.0394,  0.0458,  ..., -0.0076,  0.0421, -0.0174],\n",
      "        [-0.0023,  0.0335, -0.0276,  ...,  0.0041,  0.0065,  0.0205]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0317, -0.0086, -0.0254,  ..., -0.0008,  0.0010, -0.0060],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0119, -0.0371,  0.0272,  ...,  0.0022, -0.0358, -0.0155],\n",
      "        [-0.0088, -0.0061,  0.0057,  ...,  0.0170, -0.0213,  0.0120],\n",
      "        [-0.0172, -0.0281, -0.0233,  ...,  0.0166,  0.0042,  0.0055],\n",
      "        ...,\n",
      "        [-0.0168, -0.0180, -0.0102,  ..., -0.0099,  0.0030,  0.0164],\n",
      "        [-0.0347,  0.0075,  0.0238,  ..., -0.0318, -0.0630,  0.0424],\n",
      "        [-0.0156, -0.0423, -0.0294,  ..., -0.0275, -0.0024,  0.0270]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-9.8884e-05,  2.2602e-03, -6.3801e-04,  ...,  1.6441e-03,\n",
      "         1.1044e-03,  2.1982e-04], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0277,  0.0078,  0.0099,  ...,  0.0012, -0.0095,  0.0255],\n",
      "        [ 0.0012,  0.0010, -0.0092,  ..., -0.0061,  0.0192,  0.0274],\n",
      "        [-0.0385,  0.0118, -0.0293,  ...,  0.0194,  0.0013,  0.0235],\n",
      "        ...,\n",
      "        [-0.0319,  0.0134,  0.0456,  ..., -0.0054,  0.0178,  0.0536],\n",
      "        [ 0.0559,  0.0130, -0.0181,  ..., -0.0196,  0.0445,  0.0125],\n",
      "        [ 0.0259,  0.0085, -0.0142,  ..., -0.0070,  0.0021, -0.0440]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0322, -0.0098,  0.0215,  ..., -0.0195, -0.0062,  0.0864],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0128, -0.0001,  0.0581,  ..., -0.0303,  0.0591,  0.0010],\n",
      "        [ 0.0196, -0.0512, -0.0053,  ...,  0.0108,  0.0132,  0.0587],\n",
      "        [-0.0120, -0.0115, -0.0007,  ..., -0.0604,  0.0105,  0.0763],\n",
      "        ...,\n",
      "        [-0.0143,  0.0084,  0.0149,  ...,  0.0118,  0.0342,  0.0061],\n",
      "        [-0.0007,  0.0019,  0.0168,  ...,  0.0002,  0.0137,  0.0178],\n",
      "        [-0.0089, -0.0052,  0.0162,  ...,  0.0095,  0.0110,  0.0101]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0568, -0.0500, -0.0627,  ...,  0.0137,  0.0066, -0.0010],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0344,  0.0292,  0.0428,  ...,  0.0097,  0.0007,  0.0100],\n",
      "        [-0.0337, -0.0170, -0.0245,  ..., -0.0078, -0.0090,  0.0019],\n",
      "        [-0.0640, -0.0199, -0.0328,  ..., -0.0223, -0.0111, -0.0091],\n",
      "        ...,\n",
      "        [-0.0391, -0.0067, -0.0190,  ...,  0.0003,  0.0050, -0.0005],\n",
      "        [-0.0439,  0.0329,  0.0529,  ..., -0.0179, -0.0156, -0.0078],\n",
      "        [ 0.0080,  0.0169,  0.0174,  ..., -0.0132, -0.0107,  0.0036]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0198, -0.0131,  0.0277,  ...,  0.0086,  0.0075, -0.0081],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4011, 0.3906, 0.3223,  ..., 0.4294, 0.4387, 0.4277],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0210, -0.0227,  0.0398,  ..., -0.0077, -0.0091, -0.0241],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0140, -0.0157,  0.0354,  ...,  0.0073, -0.0033, -0.0120],\n",
      "        [ 0.0357, -0.0437,  0.1436,  ...,  0.0121,  0.0238,  0.0027],\n",
      "        [ 0.0014, -0.0251,  0.0682,  ..., -0.0064,  0.0024,  0.0169],\n",
      "        ...,\n",
      "        [ 0.0205, -0.0113,  0.0017,  ..., -0.0106,  0.0075,  0.0310],\n",
      "        [-0.0090,  0.0009,  0.0377,  ...,  0.0195, -0.0143, -0.0031],\n",
      "        [ 0.0159, -0.0270, -0.0011,  ..., -0.0011, -0.0005,  0.0162]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0484,  0.1321,  0.0542,  ...,  0.0470,  0.0821,  0.0778],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0084,  0.0373, -0.0330,  ...,  0.0201, -0.0270,  0.0445],\n",
      "        [-0.0126,  0.0252,  0.0113,  ...,  0.0091,  0.0102, -0.0390],\n",
      "        [-0.0004, -0.0375,  0.0267,  ..., -0.0025,  0.0191, -0.0151],\n",
      "        ...,\n",
      "        [ 0.0048,  0.0133, -0.0431,  ...,  0.0035,  0.0025, -0.0028],\n",
      "        [-0.0022,  0.0030, -0.0002,  ..., -0.0315,  0.0106,  0.0045],\n",
      "        [ 0.0118,  0.0062, -0.0208,  ..., -0.0187,  0.0135, -0.0030]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0340, -0.1097, -0.0490,  ..., -0.0017,  0.0052,  0.0001],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0189, -0.0165,  0.0168,  ..., -0.0310, -0.0303,  0.0003],\n",
      "        [ 0.0006, -0.0093,  0.0018,  ...,  0.0113,  0.0223,  0.0131],\n",
      "        [-0.0206,  0.0291, -0.0163,  ...,  0.0474, -0.0339,  0.0086],\n",
      "        ...,\n",
      "        [-0.0423,  0.0015, -0.0057,  ...,  0.0208, -0.0121, -0.0003],\n",
      "        [ 0.0135,  0.0623, -0.0122,  ...,  0.0017, -0.0131, -0.0177],\n",
      "        [-0.0478, -0.0331,  0.0035,  ..., -0.0045,  0.0060,  0.0166]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0010, -0.0014,  0.0021,  ...,  0.0026, -0.0019, -0.0017],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0069, -0.0212,  0.0083,  ...,  0.0098,  0.0467,  0.0296],\n",
      "        [ 0.0129,  0.0085, -0.0088,  ...,  0.0091, -0.0355, -0.0037],\n",
      "        [-0.0012, -0.0166, -0.0031,  ..., -0.0145,  0.0844, -0.0088],\n",
      "        ...,\n",
      "        [ 0.0324, -0.0210,  0.0197,  ..., -0.0058, -0.0108, -0.0087],\n",
      "        [ 0.0278, -0.0111, -0.0240,  ...,  0.0191,  0.0360, -0.0418],\n",
      "        [ 0.0324,  0.0125,  0.0234,  ..., -0.0016, -0.0232, -0.0089]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0014,  0.0163,  0.0272,  ..., -0.0083, -0.0203,  0.0158],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0076, -0.0584,  ..., -0.0197,  0.0048, -0.0054],\n",
      "        [ 0.0501, -0.0139, -0.0247,  ..., -0.0360,  0.0375,  0.0296],\n",
      "        [ 0.0187, -0.0413, -0.0185,  ...,  0.0214,  0.0075, -0.0038],\n",
      "        ...,\n",
      "        [ 0.0065, -0.0221,  0.0188,  ...,  0.0159,  0.0660, -0.0392],\n",
      "        [-0.0259,  0.0197, -0.0145,  ...,  0.0296,  0.0262, -0.0142],\n",
      "        [-0.0240,  0.0072,  0.0245,  ...,  0.0115,  0.0207, -0.0254]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0497, -0.0173, -0.0459,  ..., -0.0139, -0.0004, -0.0074],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0097, -0.0242, -0.0200,  ...,  0.0008,  0.0061, -0.0003],\n",
      "        [ 0.0173, -0.0153, -0.0400,  ..., -0.0327,  0.0004, -0.0182],\n",
      "        [-0.0437,  0.0236,  0.0344,  ...,  0.0258,  0.0094, -0.0120],\n",
      "        ...,\n",
      "        [ 0.0142,  0.0021,  0.0077,  ...,  0.0037, -0.0016,  0.0104],\n",
      "        [ 0.0040, -0.0160,  0.0112,  ...,  0.0119, -0.0133,  0.0349],\n",
      "        [-0.0221, -0.0046,  0.0350,  ...,  0.0164, -0.0116, -0.0266]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0010, -0.0224,  0.0431,  ..., -0.0035,  0.0067,  0.0150],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4709, 0.4551, 0.4177,  ..., 0.4868, 0.4917, 0.4888],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0213, -0.0114,  0.0718,  ..., -0.0094, -0.0103, -0.0165],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 2.0599e-02, -1.3092e-02,  6.7810e-02,  ..., -1.2886e-02,\n",
      "          1.6165e-03, -4.5586e-03],\n",
      "        [-1.0996e-03,  2.1469e-02, -4.3457e-02,  ..., -1.9516e-02,\n",
      "         -2.2400e-02,  2.4536e-02],\n",
      "        [-1.2482e-02, -2.8809e-02,  2.8595e-02,  ..., -3.8700e-03,\n",
      "         -8.2910e-05,  2.7252e-02],\n",
      "        ...,\n",
      "        [ 2.2507e-02, -6.5384e-03, -1.5640e-02,  ..., -2.6276e-02,\n",
      "         -3.7708e-03, -1.4534e-02],\n",
      "        [ 1.5221e-02,  1.0109e-02,  2.2812e-03,  ..., -1.7578e-02,\n",
      "          1.7746e-02, -6.6719e-03],\n",
      "        [-1.9302e-02, -4.5109e-04,  2.4796e-02,  ...,  1.4710e-04,\n",
      "         -2.1248e-03,  3.2135e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0419, -0.0155,  0.0351,  ..., -0.0059,  0.0024, -0.0299],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-4.2343e-03,  6.9084e-03,  1.5850e-03,  ...,  3.3630e-02,\n",
      "          3.9307e-02, -1.8387e-02],\n",
      "        [-2.0645e-02, -7.1955e-04, -3.0411e-02,  ..., -7.9775e-04,\n",
      "          9.6512e-03, -2.2247e-02],\n",
      "        [ 3.4363e-02, -7.0953e-04, -6.4735e-03,  ...,  4.7112e-03,\n",
      "          1.0704e-02, -2.3743e-02],\n",
      "        ...,\n",
      "        [ 2.3422e-02,  1.5579e-02,  1.1284e-02,  ..., -1.1627e-02,\n",
      "         -1.0269e-02,  2.0050e-02],\n",
      "        [ 1.4984e-02, -5.5428e-03, -7.2670e-03,  ..., -5.9853e-03,\n",
      "         -2.7969e-02,  1.1284e-02],\n",
      "        [ 1.2917e-02,  3.3478e-02, -2.0721e-02,  ..., -7.6408e-03,\n",
      "         -1.4961e-05,  3.1490e-03]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0301,  0.0121, -0.0194,  ...,  0.0117,  0.0052, -0.0046],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0060, -0.0508,  0.0144,  ..., -0.0068, -0.0241,  0.0256],\n",
      "        [-0.0394,  0.0085,  0.0272,  ..., -0.0205,  0.0297, -0.0499],\n",
      "        [-0.0408,  0.0513, -0.0056,  ..., -0.0440,  0.0152,  0.0068],\n",
      "        ...,\n",
      "        [-0.0315, -0.0016,  0.0269,  ...,  0.0338, -0.0335, -0.0428],\n",
      "        [ 0.0381,  0.0415,  0.0288,  ...,  0.0284,  0.0101, -0.0194],\n",
      "        [-0.0289,  0.0161,  0.0050,  ...,  0.0099,  0.0493,  0.0282]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0026,  0.0036, -0.0013,  ..., -0.0044,  0.0006, -0.0013],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0406, -0.0245,  0.0026,  ..., -0.0192,  0.0125, -0.0326],\n",
      "        [-0.0013, -0.0036,  0.0081,  ...,  0.0562, -0.0142, -0.0290],\n",
      "        [ 0.0072,  0.0330, -0.0114,  ...,  0.0099,  0.0288,  0.0043],\n",
      "        ...,\n",
      "        [ 0.0117, -0.0167,  0.0097,  ...,  0.0034,  0.0222,  0.0093],\n",
      "        [-0.0118, -0.0006, -0.0161,  ..., -0.0091, -0.0087,  0.0190],\n",
      "        [ 0.0119, -0.0100, -0.0350,  ..., -0.0270,  0.0062,  0.0164]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0271, -0.0133,  0.0334,  ..., -0.0179,  0.0064, -0.0078],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0197, -0.0309,  0.0465,  ...,  0.0697, -0.0143,  0.0396],\n",
      "        [-0.0229,  0.0220, -0.0351,  ..., -0.0112,  0.0383, -0.0137],\n",
      "        [ 0.0039, -0.0175, -0.0025,  ...,  0.0352, -0.0125, -0.0265],\n",
      "        ...,\n",
      "        [ 0.0241, -0.0073, -0.0147,  ...,  0.0187, -0.0236, -0.0667],\n",
      "        [ 0.0208, -0.0173, -0.0120,  ...,  0.0388, -0.0185, -0.0017],\n",
      "        [ 0.0290,  0.0087, -0.0171,  ..., -0.0148, -0.0143, -0.0223]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0415, -0.0056, -0.0331,  ...,  0.0166, -0.0050, -0.0006],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 8.7509e-03,  2.5955e-02,  1.0033e-02,  ..., -1.3786e-02,\n",
      "         -1.4122e-02, -8.2245e-03],\n",
      "        [-1.0727e-02, -3.3752e-02, -6.6147e-03,  ..., -1.4877e-02,\n",
      "         -1.1581e-02,  1.3405e-02],\n",
      "        [ 2.1591e-02,  1.1543e-02, -2.6901e-02,  ..., -3.5496e-03,\n",
      "         -2.2797e-02,  4.0779e-03],\n",
      "        ...,\n",
      "        [-1.6632e-02,  2.0645e-02,  1.5732e-02,  ..., -1.2161e-02,\n",
      "         -2.3636e-02, -6.0081e-04],\n",
      "        [ 5.4016e-03, -9.9063e-05, -2.4368e-02,  ...,  3.4454e-02,\n",
      "          8.0719e-03,  2.3376e-02],\n",
      "        [ 3.5004e-02, -2.7649e-02, -3.1204e-03,  ...,  2.0126e-02,\n",
      "         -6.7863e-03, -9.9106e-03]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0056, -0.0193,  0.0410,  ...,  0.0058,  0.0120,  0.0101],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4580, 0.4580, 0.4241,  ..., 0.4858, 0.4751, 0.4773],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0183, -0.0162,  0.0682,  ..., -0.0079, -0.0137, -0.0155],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0018,  0.0253,  0.0180,  ..., -0.0108, -0.0040,  0.0251],\n",
      "        [ 0.0306, -0.0067, -0.0403,  ...,  0.0569, -0.0122, -0.0753],\n",
      "        [ 0.0324, -0.0477,  0.0137,  ..., -0.0025,  0.0057,  0.0318],\n",
      "        ...,\n",
      "        [-0.0016,  0.0234, -0.0051,  ..., -0.0160,  0.0202, -0.0021],\n",
      "        [ 0.0674,  0.0111,  0.0103,  ..., -0.0082, -0.0698, -0.0529],\n",
      "        [ 0.0605, -0.0348, -0.0258,  ..., -0.0128, -0.0084, -0.0481]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0545, -0.0359,  0.0290,  ...,  0.0062, -0.0049, -0.0007],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0090, -0.0304, -0.0192,  ...,  0.0090,  0.0097, -0.0141],\n",
      "        [ 0.0052,  0.0049,  0.0288,  ...,  0.0099, -0.0668,  0.0697],\n",
      "        [-0.0296, -0.0022,  0.0285,  ..., -0.0029,  0.0312, -0.0150],\n",
      "        ...,\n",
      "        [ 0.0497,  0.0219,  0.0831,  ..., -0.0066, -0.0244,  0.0070],\n",
      "        [-0.0829, -0.0312, -0.0615,  ..., -0.0032,  0.0793, -0.0446],\n",
      "        [-0.0411,  0.0725,  0.0584,  ...,  0.0323, -0.0188,  0.0788]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0312,  0.0304, -0.0128,  ...,  0.0103, -0.0034,  0.0107],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0114,  0.0437, -0.0008,  ...,  0.0047,  0.0100, -0.0141],\n",
      "        [-0.0114, -0.0366,  0.0015,  ..., -0.0415, -0.0244, -0.0204],\n",
      "        [-0.0138,  0.0201, -0.0282,  ...,  0.0212,  0.0158,  0.0294],\n",
      "        ...,\n",
      "        [-0.0109, -0.0197, -0.0114,  ..., -0.0142,  0.0158,  0.0638],\n",
      "        [-0.0038,  0.0335,  0.0095,  ..., -0.0053, -0.0357, -0.0155],\n",
      "        [-0.0304, -0.0089,  0.0308,  ...,  0.0354, -0.0157, -0.0215]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 5.1765e-03,  2.6941e-04, -1.9169e-03,  ...,  2.0981e-05,\n",
      "        -6.9618e-04,  3.2496e-04], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0008,  0.0495, -0.0135,  ..., -0.0310,  0.0086, -0.0061],\n",
      "        [ 0.0007, -0.0524, -0.0187,  ..., -0.0385,  0.0170, -0.0356],\n",
      "        [-0.0059,  0.0424, -0.0439,  ...,  0.0005, -0.0045,  0.0147],\n",
      "        ...,\n",
      "        [-0.0045,  0.0019,  0.0201,  ..., -0.0061,  0.0462, -0.0208],\n",
      "        [ 0.0095, -0.0186, -0.0509,  ...,  0.0029,  0.0341,  0.0025],\n",
      "        [ 0.0030,  0.0118,  0.0143,  ..., -0.0345,  0.0284, -0.0038]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0115, -0.0013,  0.0188,  ..., -0.0446,  0.0236,  0.0071],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0388, -0.0557, -0.0576,  ..., -0.0694, -0.0350, -0.0055],\n",
      "        [-0.0443, -0.0017, -0.0108,  ...,  0.0139,  0.0090, -0.0838],\n",
      "        [-0.0288, -0.0045,  0.0014,  ...,  0.0199,  0.0596,  0.0303],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0142, -0.0118,  ..., -0.0437,  0.0443, -0.0608],\n",
      "        [ 0.0286, -0.0262, -0.0099,  ...,  0.0542, -0.0130, -0.0206],\n",
      "        [ 0.0132,  0.0463, -0.0352,  ...,  0.0188,  0.0117, -0.0108]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-3.4729e-02, -4.4342e-02, -5.1666e-02,  ..., -3.4928e-05,\n",
      "        -3.9368e-03, -1.9608e-03], dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0151, -0.0213, -0.0046,  ..., -0.0163, -0.0030, -0.0204],\n",
      "        [-0.0275,  0.0022,  0.0123,  ...,  0.0063,  0.0065, -0.0100],\n",
      "        [ 0.0147, -0.0016,  0.0362,  ..., -0.0042, -0.0096,  0.0424],\n",
      "        ...,\n",
      "        [-0.0040, -0.0016,  0.0251,  ...,  0.0438, -0.0367, -0.0208],\n",
      "        [-0.0213,  0.0177, -0.0056,  ..., -0.0346,  0.0172, -0.0157],\n",
      "        [ 0.0085, -0.0095, -0.0093,  ...,  0.0515,  0.0029, -0.0012]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0128, -0.0145,  0.0635,  ...,  0.0260,  0.0016, -0.0041],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4224, 0.4263, 0.4116,  ..., 0.4309, 0.4329, 0.4336],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0174, -0.0139,  0.0710,  ..., -0.0078, -0.0114, -0.0136],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0081, -0.0121, -0.0513,  ..., -0.0254, -0.0712, -0.0460],\n",
      "        [ 0.0158,  0.0268, -0.0154,  ..., -0.0120,  0.0208, -0.0251],\n",
      "        [ 0.0187,  0.0198,  0.0014,  ...,  0.0133,  0.0199, -0.0462],\n",
      "        ...,\n",
      "        [ 0.0628,  0.0082,  0.0377,  ...,  0.0709, -0.0428,  0.0556],\n",
      "        [-0.0329,  0.0333, -0.0079,  ...,  0.0186,  0.0168,  0.0399],\n",
      "        [-0.0421, -0.0227,  0.0014,  ..., -0.0071, -0.0030,  0.0004]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0515, -0.0522, -0.0067,  ..., -0.0058, -0.0175,  0.0065],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0400,  0.0126, -0.0146,  ...,  0.0010, -0.0083,  0.0393],\n",
      "        [-0.0366,  0.0170,  0.0099,  ...,  0.0188, -0.0140,  0.0113],\n",
      "        [-0.0250,  0.0140,  0.0332,  ...,  0.0226, -0.0326, -0.0170],\n",
      "        ...,\n",
      "        [-0.0716,  0.0602,  0.0266,  ...,  0.0256, -0.0969,  0.0086],\n",
      "        [-0.0221,  0.0273,  0.0196,  ...,  0.0441, -0.0153, -0.0072],\n",
      "        [ 0.0466,  0.0464, -0.0581,  ..., -0.0307, -0.0217,  0.0382]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0225,  0.0105,  0.0130,  ..., -0.0177, -0.0017,  0.0040],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0018, -0.0100, -0.0164,  ...,  0.0107, -0.0397, -0.0123],\n",
      "        [-0.0051,  0.0152, -0.0170,  ...,  0.0179,  0.0190,  0.0021],\n",
      "        [-0.0099, -0.0386, -0.0242,  ..., -0.0173,  0.0038,  0.0290],\n",
      "        ...,\n",
      "        [ 0.0464, -0.0038, -0.0161,  ..., -0.0544,  0.0107, -0.0145],\n",
      "        [ 0.0266,  0.0385,  0.0281,  ..., -0.0109, -0.0432, -0.0393],\n",
      "        [-0.0232,  0.0507,  0.0087,  ...,  0.0504, -0.0362,  0.0281]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 4.4861e-03,  6.0499e-05,  4.7922e-04,  ...,  1.3609e-03,\n",
      "         9.0313e-04, -2.4681e-03], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 1.9236e-03,  1.6113e-02,  7.8659e-03,  ...,  2.7390e-02,\n",
      "         -4.2603e-02, -1.6205e-02],\n",
      "        [-3.0563e-02,  3.1921e-02, -5.8105e-02,  ...,  2.3115e-04,\n",
      "          3.2562e-02, -5.5504e-03],\n",
      "        [-2.2598e-02, -1.3268e-02, -2.5925e-02,  ..., -2.6512e-03,\n",
      "         -4.3945e-02, -1.5793e-02],\n",
      "        ...,\n",
      "        [ 3.6957e-02, -1.3947e-05, -8.4734e-04,  ...,  3.2349e-02,\n",
      "         -6.3553e-03,  1.0521e-02],\n",
      "        [ 1.6983e-02, -4.8103e-03,  1.4900e-02,  ...,  1.7487e-02,\n",
      "         -3.3569e-02,  1.9867e-02],\n",
      "        [-1.4030e-02,  7.9041e-03, -1.8677e-02,  ...,  9.1400e-03,\n",
      "          1.2337e-02, -3.7506e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0053,  0.0113, -0.0191,  ..., -0.0374,  0.0113,  0.0245],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0190, -0.0715, -0.0313,  ..., -0.0201, -0.0688, -0.0107],\n",
      "        [-0.0024, -0.0344, -0.0902,  ...,  0.0295,  0.0736, -0.0241],\n",
      "        [ 0.0358,  0.0245,  0.0428,  ..., -0.0412,  0.0325,  0.0723],\n",
      "        ...,\n",
      "        [-0.0101,  0.0156,  0.0435,  ..., -0.0037,  0.0215, -0.0364],\n",
      "        [-0.0298,  0.0501,  0.0007,  ...,  0.0122,  0.0212, -0.0046],\n",
      "        [-0.0050,  0.0150,  0.0257,  ...,  0.0030,  0.0123, -0.0359]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0321, -0.0439, -0.0569,  ..., -0.0095, -0.0272, -0.0102],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0455,  0.0121,  0.0214,  ..., -0.0043,  0.0367,  0.0103],\n",
      "        [-0.0326, -0.0286, -0.0068,  ..., -0.0044, -0.0086, -0.0025],\n",
      "        [-0.0016, -0.0052, -0.0089,  ..., -0.0214,  0.0072, -0.0249],\n",
      "        ...,\n",
      "        [-0.0032,  0.0069, -0.0168,  ..., -0.0134, -0.0363,  0.0100],\n",
      "        [ 0.0377,  0.0017, -0.0057,  ...,  0.0103,  0.0850,  0.0235],\n",
      "        [-0.0102,  0.0036,  0.0182,  ...,  0.0285, -0.0326,  0.0195]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0329, -0.0193,  0.0616,  ...,  0.0328,  0.0147,  0.0137],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4507, 0.4495, 0.4343,  ..., 0.4597, 0.4465, 0.4534],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0160, -0.0143,  0.0925,  ..., -0.0066, -0.0086, -0.0194],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0073,  0.0523,  0.0113,  ...,  0.0356,  0.0041,  0.0246],\n",
      "        [-0.0013, -0.0424, -0.0006,  ..., -0.0257, -0.0378,  0.0037],\n",
      "        [ 0.0234, -0.0251, -0.0168,  ...,  0.0125, -0.0397, -0.0094],\n",
      "        ...,\n",
      "        [-0.0105,  0.0461, -0.0006,  ..., -0.0193, -0.0529, -0.0687],\n",
      "        [-0.0038, -0.0848, -0.0010,  ..., -0.0376,  0.0498,  0.0153],\n",
      "        [-0.0181,  0.0043, -0.0085,  ...,  0.0086,  0.0050,  0.0051]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0019,  0.0036, -0.0125,  ..., -0.0049,  0.0181, -0.0079],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0026, -0.0088,  0.0244,  ...,  0.0083,  0.0101,  0.0073],\n",
      "        [ 0.0284, -0.0244, -0.0218,  ...,  0.0025, -0.0253, -0.0172],\n",
      "        [-0.0023,  0.0009, -0.0339,  ..., -0.0460,  0.0173, -0.0079],\n",
      "        ...,\n",
      "        [ 0.0216, -0.1136, -0.0135,  ...,  0.0387,  0.0198,  0.0580],\n",
      "        [-0.0370,  0.0132,  0.0266,  ...,  0.0278, -0.0107, -0.0033],\n",
      "        [-0.0138,  0.0297,  0.0339,  ...,  0.0294, -0.0310,  0.0291]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0229, -0.0227, -0.0065,  ...,  0.0050,  0.0190, -0.0527],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0418,  0.0315, -0.0298,  ...,  0.0205,  0.0013,  0.0332],\n",
      "        [ 0.0023,  0.0453, -0.0003,  ..., -0.0446,  0.0111, -0.0228],\n",
      "        [-0.0276,  0.0089, -0.0058,  ...,  0.0651, -0.0047,  0.0242],\n",
      "        ...,\n",
      "        [-0.0745, -0.0090,  0.0021,  ...,  0.0174, -0.0070, -0.0151],\n",
      "        [ 0.0170, -0.0102, -0.0035,  ...,  0.0038,  0.0371, -0.0259],\n",
      "        [-0.0089,  0.0126,  0.0074,  ..., -0.0094,  0.0234,  0.0480]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0007, -0.0004, -0.0008,  ...,  0.0017,  0.0007, -0.0015],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0241,  0.0111, -0.0249,  ...,  0.0081, -0.0104,  0.0016],\n",
      "        [ 0.0338,  0.0018,  0.0020,  ..., -0.0042, -0.0283, -0.0076],\n",
      "        [ 0.0274,  0.0153,  0.0614,  ..., -0.0059,  0.0174, -0.0210],\n",
      "        ...,\n",
      "        [ 0.0208, -0.0172,  0.0305,  ...,  0.0131,  0.0181,  0.0230],\n",
      "        [ 0.0282,  0.0016, -0.0722,  ..., -0.0038,  0.0732, -0.0049],\n",
      "        [-0.0057, -0.0543, -0.0491,  ..., -0.0030,  0.0365, -0.0090]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([0.0299, 0.0041, 0.0151,  ..., 0.0077, 0.0111, 0.0247],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0679,  0.0486, -0.0488,  ..., -0.0202,  0.0399, -0.0545],\n",
      "        [ 0.0184, -0.0481, -0.0619,  ..., -0.0064,  0.0127, -0.0004],\n",
      "        [-0.0046, -0.0300, -0.0471,  ..., -0.0363, -0.0193, -0.0454],\n",
      "        ...,\n",
      "        [ 0.0191, -0.0437, -0.0208,  ..., -0.0125, -0.0391, -0.0143],\n",
      "        [-0.0331,  0.0077, -0.0074,  ..., -0.0055, -0.0353,  0.0014],\n",
      "        [ 0.0066,  0.0231, -0.0131,  ..., -0.0057, -0.0084,  0.0338]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0374, -0.0427, -0.0417,  ...,  0.0103,  0.0071,  0.0121],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0472,  0.0248, -0.0045,  ..., -0.0181,  0.0248,  0.0019],\n",
      "        [ 0.0240, -0.0065,  0.0038,  ...,  0.0144,  0.0035, -0.0074],\n",
      "        [-0.0395, -0.0118,  0.0056,  ...,  0.0212,  0.0103,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0361,  0.0323, -0.0271,  ...,  0.0077,  0.0173,  0.0331],\n",
      "        [ 0.0135, -0.0148,  0.0162,  ...,  0.0408,  0.0317, -0.0139],\n",
      "        [ 0.0319, -0.0136, -0.0302,  ..., -0.0091,  0.0057, -0.0167]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0213, -0.0051,  0.0466,  ...,  0.0431, -0.0113, -0.0084],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4543, 0.4556, 0.4368,  ..., 0.4595, 0.4497, 0.4585],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0157, -0.0177,  0.0913,  ..., -0.0037, -0.0090, -0.0137],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0608,  0.0120,  0.0067,  ...,  0.0090,  0.0104,  0.0198],\n",
      "        [ 0.0020,  0.0385,  0.0657,  ..., -0.0201,  0.0501, -0.0306],\n",
      "        [ 0.0257, -0.0374,  0.0244,  ...,  0.0211, -0.0063, -0.0026],\n",
      "        ...,\n",
      "        [-0.0385, -0.0170,  0.0014,  ..., -0.0395,  0.0334, -0.0015],\n",
      "        [ 0.0174,  0.0346,  0.0343,  ..., -0.0116, -0.0414, -0.0055],\n",
      "        [-0.0212, -0.0572,  0.0128,  ..., -0.0061, -0.0130,  0.0179]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0179,  0.0158,  0.0170,  ...,  0.0077, -0.0018, -0.0085],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0089, -0.0502,  0.0428,  ..., -0.0323,  0.0329, -0.0061],\n",
      "        [ 0.0088,  0.0052,  0.0179,  ...,  0.0202,  0.0054, -0.0038],\n",
      "        [ 0.0054,  0.0140,  0.0067,  ...,  0.0246, -0.0133, -0.0466],\n",
      "        ...,\n",
      "        [ 0.0284,  0.0016, -0.0299,  ..., -0.0212,  0.0651,  0.0177],\n",
      "        [ 0.0029,  0.0097,  0.0099,  ...,  0.0140, -0.0033, -0.0424],\n",
      "        [-0.0515,  0.0065,  0.0089,  ...,  0.0003,  0.0039,  0.0388]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0284, -0.0167, -0.0194,  ...,  0.0068, -0.0113,  0.0036],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0220,  0.0190, -0.0096,  ...,  0.0094,  0.0177, -0.0246],\n",
      "        [-0.0192, -0.0459, -0.0113,  ...,  0.0017, -0.0238,  0.0046],\n",
      "        [ 0.0409, -0.0211, -0.0180,  ..., -0.0239,  0.0213, -0.0439],\n",
      "        ...,\n",
      "        [-0.0516, -0.0007, -0.0094,  ...,  0.0385,  0.0018, -0.0156],\n",
      "        [ 0.0278, -0.0120,  0.0280,  ...,  0.0100,  0.0369, -0.0140],\n",
      "        [-0.0421, -0.0117, -0.0056,  ..., -0.0107, -0.0341,  0.0043]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0020,  0.0024,  0.0028,  ..., -0.0003, -0.0005,  0.0018],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0443,  0.0221, -0.0510,  ...,  0.0085, -0.0150, -0.0077],\n",
      "        [-0.0072, -0.0618, -0.0271,  ..., -0.0375, -0.0024,  0.0336],\n",
      "        [-0.0335,  0.0414,  0.0008,  ...,  0.0149, -0.0173, -0.0439],\n",
      "        ...,\n",
      "        [-0.0168,  0.0233,  0.0012,  ..., -0.0012, -0.0225,  0.0136],\n",
      "        [-0.0022,  0.0287,  0.0039,  ..., -0.0615,  0.0155, -0.0057],\n",
      "        [-0.0074,  0.0031, -0.0355,  ...,  0.0041,  0.0283,  0.0193]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0104, -0.0029,  0.0104,  ..., -0.0240,  0.0224, -0.0060],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 2.1305e-03, -3.4119e-02,  2.1057e-03,  ...,  2.2293e-02,\n",
      "         -4.0619e-02,  4.5410e-02],\n",
      "        [-2.3438e-02, -4.3831e-03, -2.5387e-03,  ..., -1.5266e-02,\n",
      "         -2.9785e-02,  7.1983e-03],\n",
      "        [-2.0996e-02, -3.5339e-02, -4.2358e-02,  ..., -5.0903e-02,\n",
      "          2.8000e-02, -1.9388e-03],\n",
      "        ...,\n",
      "        [-5.4121e-04, -2.9816e-02, -2.3544e-02,  ...,  6.9336e-02,\n",
      "          3.1776e-03, -3.0487e-02],\n",
      "        [ 3.9368e-03,  8.9111e-03, -2.5650e-02,  ...,  5.5054e-02,\n",
      "          7.5111e-03,  4.9286e-03],\n",
      "        [ 5.2071e-03,  2.0096e-02,  1.6332e-05,  ..., -1.8082e-02,\n",
      "         -6.0364e-02, -1.1070e-02]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0462, -0.0233, -0.0525,  ...,  0.0061, -0.0135, -0.0105],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 1.7838e-02, -1.0582e-02,  2.7573e-02,  ..., -1.3374e-02,\n",
      "          7.9803e-03,  9.2697e-04],\n",
      "        [ 8.4305e-03, -2.4490e-02,  2.1118e-02,  ...,  2.2766e-02,\n",
      "          1.5230e-03,  2.0172e-02],\n",
      "        [ 8.9216e-04, -1.7441e-02, -2.7191e-02,  ...,  2.0569e-02,\n",
      "          3.7174e-03, -5.3215e-03],\n",
      "        ...,\n",
      "        [ 4.0100e-02, -3.6072e-02,  2.8976e-02,  ..., -3.9406e-03,\n",
      "         -2.8976e-02,  2.7481e-02],\n",
      "        [ 1.3748e-02, -1.0422e-02,  1.7300e-03,  ...,  1.0841e-02,\n",
      "         -1.3985e-02,  4.1321e-02],\n",
      "        [-7.5996e-05,  3.7323e-02, -1.5793e-02,  ..., -2.1652e-02,\n",
      "         -4.2295e-04,  3.2867e-02]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0039,  0.0040,  0.0622,  ...,  0.0499, -0.0087,  0.0111],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4629, 0.4592, 0.4199,  ..., 0.4731, 0.4595, 0.4592],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0170, -0.0168,  0.0950,  ..., -0.0042, -0.0135, -0.0182],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0337, -0.0199,  0.0519,  ...,  0.0121,  0.0228, -0.0008],\n",
      "        [-0.0251, -0.0042,  0.0137,  ..., -0.0123,  0.0026, -0.0123],\n",
      "        [-0.0182,  0.0331, -0.0022,  ...,  0.0177,  0.0017,  0.0152],\n",
      "        ...,\n",
      "        [ 0.0097,  0.0403, -0.0067,  ...,  0.0118,  0.0063, -0.0504],\n",
      "        [ 0.0490, -0.0683,  0.0108,  ...,  0.0297,  0.0386, -0.0200],\n",
      "        [ 0.0094, -0.0241, -0.0058,  ...,  0.0874,  0.0156, -0.0275]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0220,  0.0423, -0.0207,  ..., -0.0199, -0.0189,  0.0167],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0191,  0.0288, -0.0353,  ..., -0.0069,  0.0055, -0.0022],\n",
      "        [ 0.0116,  0.0115,  0.0074,  ..., -0.0069, -0.0194,  0.0131],\n",
      "        [ 0.0099,  0.0383,  0.0314,  ..., -0.0128, -0.0138,  0.0483],\n",
      "        ...,\n",
      "        [-0.0235, -0.0333,  0.0103,  ...,  0.0064,  0.0090, -0.0230],\n",
      "        [-0.0032, -0.0436,  0.0170,  ...,  0.0519, -0.0333, -0.0428],\n",
      "        [-0.0215,  0.0461,  0.0561,  ..., -0.0217, -0.0187,  0.0311]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0271,  0.0054, -0.0447,  ..., -0.0090,  0.0168, -0.0059],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0078,  0.0616,  0.0307,  ..., -0.0007,  0.0332, -0.0195],\n",
      "        [-0.0501,  0.0037,  0.0049,  ...,  0.0025, -0.0359,  0.0096],\n",
      "        [-0.0004, -0.0587,  0.0314,  ...,  0.0253,  0.0330,  0.0459],\n",
      "        ...,\n",
      "        [ 0.0040,  0.0438, -0.0030,  ...,  0.0127, -0.0030,  0.0035],\n",
      "        [ 0.0281, -0.0078,  0.0102,  ...,  0.0103,  0.0277,  0.0315],\n",
      "        [-0.0339,  0.0337, -0.0024,  ..., -0.0586, -0.0335, -0.0055]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 1.8377e-03,  1.0347e-03,  4.5002e-05,  ...,  1.4601e-03,\n",
      "        -1.0328e-03,  1.5688e-03], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0170,  0.0572, -0.0224,  ...,  0.0083,  0.0045, -0.0320],\n",
      "        [-0.0270, -0.0353,  0.0396,  ...,  0.0187, -0.0298, -0.0113],\n",
      "        [-0.0391, -0.0205, -0.0341,  ...,  0.0174, -0.0256, -0.0114],\n",
      "        ...,\n",
      "        [ 0.0020, -0.0149, -0.0589,  ..., -0.0248,  0.0269,  0.0070],\n",
      "        [-0.0420,  0.0545, -0.0345,  ..., -0.0034, -0.0334,  0.0248],\n",
      "        [ 0.0287, -0.0166,  0.0124,  ...,  0.0068, -0.0373,  0.0483]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0109,  0.0095, -0.0092,  ..., -0.0059,  0.0447,  0.0030],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0059, -0.0485, -0.0023,  ...,  0.0387, -0.0663, -0.0262],\n",
      "        [-0.0057,  0.0130,  0.0074,  ..., -0.0190,  0.0493, -0.0238],\n",
      "        [ 0.0074, -0.0286, -0.0338,  ..., -0.0032, -0.0084,  0.0184],\n",
      "        ...,\n",
      "        [ 0.0176,  0.0052, -0.0358,  ..., -0.0099, -0.0221, -0.0058],\n",
      "        [ 0.0627, -0.0076, -0.0405,  ..., -0.0042,  0.0107,  0.0267],\n",
      "        [-0.0096,  0.0177, -0.0083,  ...,  0.0029, -0.0437,  0.0623]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0136, -0.0301,  0.0002,  ...,  0.0161, -0.0032,  0.0123],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0358, -0.0038,  0.0046,  ...,  0.0212, -0.0197,  0.0217],\n",
      "        [ 0.0070,  0.0482,  0.0142,  ..., -0.0089,  0.0115, -0.0253],\n",
      "        [-0.0074,  0.0074,  0.0095,  ...,  0.0301,  0.0124, -0.0027],\n",
      "        ...,\n",
      "        [-0.0251,  0.0107,  0.0157,  ...,  0.0255, -0.0068, -0.0018],\n",
      "        [ 0.0090, -0.0088,  0.0314,  ...,  0.0060,  0.0085,  0.0468],\n",
      "        [-0.0159, -0.0109, -0.0601,  ...,  0.0387, -0.0086, -0.0249]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0072,  0.0025,  0.0485,  ...,  0.0339, -0.0624, -0.0143],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4541, 0.4553, 0.4238,  ..., 0.4729, 0.4585, 0.4590],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0178, -0.0173,  0.1076,  ...,  0.0015, -0.0130, -0.0144],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0324,  0.0672,  0.0484,  ..., -0.0353,  0.0545,  0.0357],\n",
      "        [ 0.0031,  0.0371,  0.0424,  ...,  0.0424, -0.0247, -0.0217],\n",
      "        [-0.0330,  0.0360, -0.0056,  ..., -0.0160,  0.0252, -0.0002],\n",
      "        ...,\n",
      "        [-0.0446, -0.0276,  0.0156,  ...,  0.0293,  0.0254,  0.0235],\n",
      "        [ 0.0055,  0.0561,  0.0272,  ...,  0.0103,  0.0255,  0.0152],\n",
      "        [ 0.0335, -0.0172, -0.0414,  ..., -0.0601,  0.0240,  0.0378]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0128,  0.0129, -0.0062,  ..., -0.0205, -0.0129,  0.0127],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 2.7740e-02,  3.8086e-02,  2.9358e-02,  ..., -4.6509e-02,\n",
      "          5.4932e-04,  9.1970e-05],\n",
      "        [-1.4755e-02,  2.8572e-03, -1.8301e-03,  ..., -4.1290e-02,\n",
      "          5.0873e-02,  3.7766e-03],\n",
      "        [-4.7035e-03,  8.7967e-03,  5.1117e-03,  ...,  9.4223e-03,\n",
      "          7.9422e-03,  4.0591e-05],\n",
      "        ...,\n",
      "        [ 4.7058e-02,  1.4610e-02, -9.4223e-03,  ...,  2.0737e-02,\n",
      "          7.0618e-02, -2.9984e-02],\n",
      "        [ 7.1602e-03, -1.0468e-02, -1.9577e-02,  ...,  1.1780e-02,\n",
      "          4.9896e-02, -2.8870e-02],\n",
      "        [-1.3947e-02,  7.3914e-02, -2.6627e-02,  ...,  2.0279e-02,\n",
      "         -3.2597e-03, -2.7283e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0206,  0.0066, -0.0180,  ...,  0.0083, -0.0056,  0.0065],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0252, -0.0133,  0.0032,  ..., -0.0117,  0.0111,  0.0150],\n",
      "        [ 0.0076, -0.0086, -0.0424,  ...,  0.0316,  0.0221, -0.0023],\n",
      "        [ 0.0069, -0.0014, -0.0200,  ...,  0.0326,  0.0061,  0.0078],\n",
      "        ...,\n",
      "        [-0.0390, -0.0350, -0.0188,  ...,  0.0233, -0.0300, -0.0033],\n",
      "        [-0.0321, -0.0047,  0.0005,  ..., -0.0242, -0.0132, -0.0250],\n",
      "        [ 0.0524, -0.0252,  0.0015,  ..., -0.0209, -0.0301,  0.0535]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0014,  0.0011, -0.0011,  ..., -0.0005, -0.0045,  0.0013],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0143, -0.0163, -0.0343,  ...,  0.0211, -0.0042, -0.0043],\n",
      "        [-0.0193,  0.0197,  0.0247,  ...,  0.0239,  0.0472,  0.0229],\n",
      "        [-0.0240,  0.0320,  0.0302,  ..., -0.0067,  0.0092,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0303, -0.0782,  0.0262,  ..., -0.0008,  0.0020,  0.0356],\n",
      "        [-0.0081, -0.0480, -0.0249,  ...,  0.0144, -0.0015,  0.0139],\n",
      "        [ 0.0536, -0.0083, -0.0319,  ..., -0.0026, -0.0247,  0.0323]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([0.0210, 0.0162, 0.0228,  ..., 0.0125, 0.0410, 0.0018],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0097, -0.0380, -0.0180,  ..., -0.0195,  0.0253, -0.0159],\n",
      "        [-0.0326, -0.0411, -0.0108,  ...,  0.0572,  0.0187, -0.0291],\n",
      "        [-0.0608, -0.0166, -0.0017,  ..., -0.0170,  0.0031,  0.0054],\n",
      "        ...,\n",
      "        [ 0.0086, -0.0172, -0.0028,  ..., -0.0092,  0.0582,  0.0259],\n",
      "        [ 0.0542,  0.0204,  0.0312,  ..., -0.0070, -0.0106, -0.0041],\n",
      "        [ 0.0514,  0.0245, -0.0207,  ..., -0.0062, -0.0221,  0.0380]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0075, -0.0345,  0.0022,  ..., -0.0104, -0.0084, -0.0035],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0392,  0.0084,  0.0263,  ..., -0.0100, -0.0304, -0.0179],\n",
      "        [ 0.0033,  0.0093,  0.0359,  ...,  0.0295, -0.0082, -0.0309],\n",
      "        [-0.0021, -0.0369, -0.0254,  ..., -0.0016, -0.0117,  0.0031],\n",
      "        ...,\n",
      "        [ 0.0257, -0.0131, -0.0011,  ...,  0.0095, -0.0050,  0.0112],\n",
      "        [-0.0215,  0.0002,  0.0219,  ..., -0.0365,  0.0097,  0.0070],\n",
      "        [ 0.0070,  0.0094, -0.0132,  ..., -0.0009,  0.0023, -0.0150]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0093, -0.0021,  0.0353,  ...,  0.0368, -0.0518, -0.0007],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4529, 0.4470, 0.4116,  ..., 0.4658, 0.4539, 0.4478],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0185, -0.0145,  0.0875,  ...,  0.0041, -0.0168, -0.0168],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0227, -0.0159,  0.1696,  ...,  0.0128, -0.0015,  0.0221],\n",
      "        [ 0.0004,  0.0283, -0.0908,  ...,  0.0080,  0.0121, -0.0253],\n",
      "        [-0.0005, -0.0364,  0.1267,  ...,  0.0400, -0.0008,  0.0400],\n",
      "        ...,\n",
      "        [ 0.0515,  0.0320, -0.0073,  ..., -0.0154,  0.0248, -0.0221],\n",
      "        [ 0.0728,  0.0070,  0.0478,  ...,  0.0136, -0.0299,  0.0543],\n",
      "        [ 0.0037, -0.0159,  0.0102,  ...,  0.0265,  0.0295, -0.0634]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0204,  0.0079, -0.0015,  ..., -0.0290,  0.0069, -0.0108],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0393, -0.0122, -0.0707,  ..., -0.0498,  0.0021,  0.0122],\n",
      "        [-0.0136,  0.0014,  0.0410,  ...,  0.0421, -0.0328,  0.0025],\n",
      "        [-0.0065,  0.0463, -0.0422,  ..., -0.0717,  0.0060,  0.0071],\n",
      "        ...,\n",
      "        [ 0.0266, -0.0137, -0.0146,  ...,  0.0406,  0.0001,  0.0340],\n",
      "        [-0.0297,  0.0406, -0.0225,  ..., -0.0278,  0.0227, -0.0539],\n",
      "        [-0.0570,  0.0059,  0.0615,  ...,  0.0649,  0.0073, -0.0362]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0354,  0.0035, -0.0197,  ...,  0.0696, -0.1002,  0.0058],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0186, -0.0049,  0.0135,  ...,  0.0318,  0.0138, -0.0026],\n",
      "        [ 0.0209,  0.0184, -0.0433,  ..., -0.0281,  0.0118, -0.0391],\n",
      "        [-0.0406, -0.0103,  0.0388,  ..., -0.0273,  0.0579,  0.0141],\n",
      "        ...,\n",
      "        [-0.0071,  0.0120, -0.0205,  ...,  0.0340, -0.0303,  0.0085],\n",
      "        [ 0.0141,  0.0139,  0.0136,  ..., -0.0197, -0.0455, -0.0258],\n",
      "        [ 0.0432,  0.0019,  0.0126,  ..., -0.0187,  0.0745,  0.0218]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-2.1038e-03, -2.4967e-03, -1.4658e-03,  ...,  3.4199e-03,\n",
      "        -2.4052e-03, -8.7559e-05], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0173, -0.0290,  0.0396,  ...,  0.0003, -0.0239,  0.0197],\n",
      "        [ 0.0477, -0.0388,  0.0018,  ...,  0.0399, -0.0111,  0.0309],\n",
      "        [ 0.0046,  0.0023, -0.0042,  ...,  0.0198, -0.0224, -0.0076],\n",
      "        ...,\n",
      "        [ 0.0264, -0.0487, -0.0293,  ..., -0.0199,  0.0020,  0.0298],\n",
      "        [-0.0055, -0.0473,  0.0385,  ..., -0.0621,  0.0711, -0.1102],\n",
      "        [-0.0193,  0.0330,  0.0582,  ...,  0.0223, -0.0685, -0.0234]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0110,  0.0179, -0.0231,  ...,  0.0182,  0.0482,  0.0119],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0325, -0.0302, -0.0387,  ...,  0.0107,  0.0027,  0.0156],\n",
      "        [ 0.0270,  0.0021, -0.0041,  ...,  0.0299, -0.0107,  0.0199],\n",
      "        [ 0.0311,  0.0381, -0.0733,  ...,  0.0040,  0.0085, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0290,  0.0119,  0.0139,  ...,  0.0275, -0.0060, -0.0065],\n",
      "        [-0.0199,  0.0002, -0.0098,  ..., -0.0022,  0.0120,  0.0077],\n",
      "        [ 0.0259,  0.0126, -0.0384,  ...,  0.0106,  0.0201, -0.0019]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0380, -0.0257, -0.0081,  ...,  0.0014,  0.0054, -0.0007],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0094,  0.0488, -0.0207,  ...,  0.0021,  0.0161, -0.0095],\n",
      "        [-0.0114, -0.0103,  0.0098,  ..., -0.0106,  0.0053, -0.0014],\n",
      "        [-0.0141,  0.0216,  0.0070,  ..., -0.0061,  0.0012,  0.0158],\n",
      "        ...,\n",
      "        [-0.0578, -0.0319, -0.0005,  ...,  0.0068, -0.0031, -0.0018],\n",
      "        [-0.0537, -0.0096, -0.0403,  ..., -0.0157,  0.0183, -0.0084],\n",
      "        [-0.0343, -0.0100,  0.0225,  ..., -0.0043, -0.0081,  0.0019]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0024, -0.0076,  0.0539,  ...,  0.0246, -0.0445, -0.0205],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4475, 0.4507, 0.4153,  ..., 0.4653, 0.4482, 0.4500],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0179, -0.0137,  0.0818,  ...,  0.0018, -0.0211, -0.0139],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0365, -0.0047, -0.0312,  ..., -0.0119,  0.0112, -0.0155],\n",
      "        [-0.0115, -0.0213,  0.0551,  ..., -0.0328, -0.0096, -0.0200],\n",
      "        [-0.0184, -0.0204,  0.0334,  ..., -0.0282,  0.0159,  0.0724],\n",
      "        ...,\n",
      "        [ 0.0111, -0.0291,  0.0181,  ...,  0.0396, -0.0277,  0.0547],\n",
      "        [ 0.0062, -0.0075, -0.0526,  ...,  0.0166, -0.0194, -0.0119],\n",
      "        [ 0.0635,  0.0187,  0.0128,  ..., -0.0568,  0.0315, -0.0292]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0254,  0.0124,  0.0133,  ...,  0.0144, -0.0022, -0.0039],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0083, -0.0086,  0.0059,  ..., -0.0186, -0.0244, -0.0078],\n",
      "        [-0.0136,  0.0109,  0.0326,  ..., -0.0063, -0.0016,  0.0205],\n",
      "        [-0.0447, -0.0384, -0.0090,  ...,  0.0332, -0.0125,  0.0354],\n",
      "        ...,\n",
      "        [-0.0047, -0.0646, -0.0050,  ..., -0.0424, -0.0696, -0.0688],\n",
      "        [ 0.0127,  0.0371, -0.0292,  ..., -0.0173, -0.0651, -0.0062],\n",
      "        [ 0.0070,  0.0294, -0.0016,  ..., -0.0298,  0.0377,  0.0476]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0154, -0.0199, -0.0187,  ...,  0.0121,  0.0092,  0.0014],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0224, -0.0192,  0.0198,  ..., -0.0113,  0.0043,  0.0104],\n",
      "        [ 0.0251,  0.0029, -0.0001,  ...,  0.0557,  0.0208, -0.0211],\n",
      "        [ 0.0572, -0.0521,  0.0352,  ..., -0.0345, -0.0278, -0.0970],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0114,  0.0085,  ...,  0.0363, -0.0287,  0.0082],\n",
      "        [ 0.0199,  0.0068, -0.0109,  ..., -0.0213,  0.0111,  0.0210],\n",
      "        [-0.0145, -0.0368, -0.0010,  ..., -0.0486, -0.0118,  0.0395]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0040,  0.0012, -0.0009,  ...,  0.0006,  0.0026, -0.0002],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0078, -0.0329, -0.0566,  ..., -0.0151,  0.0468,  0.0161],\n",
      "        [-0.0093,  0.0062, -0.0090,  ...,  0.0523,  0.0050, -0.0416],\n",
      "        [-0.0323, -0.0030, -0.0269,  ...,  0.0045, -0.0091, -0.0133],\n",
      "        ...,\n",
      "        [-0.0064,  0.0397,  0.0131,  ...,  0.0184, -0.0506, -0.0267],\n",
      "        [ 0.0289,  0.0707,  0.0300,  ..., -0.0277,  0.0472, -0.0045],\n",
      "        [ 0.0938, -0.0280,  0.0149,  ...,  0.0161,  0.0143,  0.0061]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0232,  0.0072, -0.0164,  ...,  0.0232,  0.0066,  0.0240],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0314, -0.0165,  0.0498,  ..., -0.0204,  0.0166, -0.0171],\n",
      "        [ 0.0469,  0.0629, -0.0076,  ..., -0.0233, -0.0500,  0.0223],\n",
      "        [ 0.0081, -0.0271, -0.0036,  ...,  0.0070,  0.0253, -0.0301],\n",
      "        ...,\n",
      "        [ 0.0266,  0.0246,  0.0124,  ...,  0.0017, -0.0143,  0.0226],\n",
      "        [ 0.0140, -0.0224, -0.0194,  ...,  0.0044, -0.0194, -0.0059],\n",
      "        [ 0.0370, -0.0110, -0.0158,  ...,  0.0149,  0.0402, -0.0430]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0209, -0.0515,  0.0085,  ...,  0.0056,  0.0047,  0.0067],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0005,  0.0258,  0.0175,  ..., -0.0410, -0.0100, -0.0255],\n",
      "        [ 0.0061,  0.0055,  0.0274,  ..., -0.0324,  0.0464, -0.0215],\n",
      "        [-0.0604, -0.0087,  0.0159,  ...,  0.0088,  0.0141,  0.0300],\n",
      "        ...,\n",
      "        [ 0.0152, -0.0039, -0.0022,  ..., -0.0137,  0.0106, -0.0026],\n",
      "        [ 0.0302,  0.0096,  0.0399,  ...,  0.0057,  0.0062, -0.0338],\n",
      "        [ 0.0227, -0.0110, -0.0115,  ..., -0.0138, -0.0006,  0.0039]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0023, -0.0037,  0.0640,  ...,  0.0093, -0.0182, -0.0168],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4500, 0.4529, 0.4165,  ..., 0.4746, 0.4551, 0.4619],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0201, -0.0125,  0.0879,  ...,  0.0022, -0.0192, -0.0152],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 3.1891e-02,  1.1406e-02,  2.5055e-02,  ...,  2.4811e-02,\n",
      "         -1.9623e-02, -5.4382e-02],\n",
      "        [-1.2695e-02, -2.7969e-02, -8.3847e-03,  ..., -2.4017e-02,\n",
      "          1.6422e-03, -4.0039e-02],\n",
      "        [-8.5449e-03,  4.6997e-02,  1.6327e-02,  ..., -8.7976e-05,\n",
      "         -5.0926e-03, -5.3711e-02],\n",
      "        ...,\n",
      "        [-3.7201e-02,  2.6993e-02, -4.3678e-03,  ...,  5.0018e-02,\n",
      "         -1.4267e-02,  6.1462e-02],\n",
      "        [ 2.3544e-02,  8.8928e-02, -3.4973e-02,  ...,  7.9269e-03,\n",
      "         -7.4097e-02, -1.7481e-03],\n",
      "        [ 1.3908e-02, -3.6163e-02, -2.4277e-02,  ..., -2.1774e-02,\n",
      "         -6.6261e-03, -1.7471e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0174,  0.0150,  0.0098,  ..., -0.0033, -0.0128,  0.0116],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0506, -0.0146,  0.0016,  ...,  0.0166,  0.0337, -0.0115],\n",
      "        [ 0.0432,  0.0079,  0.0145,  ...,  0.0424,  0.0157, -0.0218],\n",
      "        [ 0.0106, -0.0096,  0.0153,  ...,  0.0476,  0.0169,  0.0169],\n",
      "        ...,\n",
      "        [-0.0040,  0.0337,  0.0287,  ..., -0.0359,  0.0132,  0.0013],\n",
      "        [ 0.0032, -0.0199,  0.0424,  ...,  0.0599,  0.0101, -0.0168],\n",
      "        [-0.0006,  0.0551, -0.0236,  ...,  0.0309, -0.0055,  0.0096]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0379,  0.0354,  0.0424,  ..., -0.0376,  0.0102,  0.0321],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0294, -0.0575, -0.0074,  ...,  0.0239,  0.0098, -0.0151],\n",
      "        [-0.0133,  0.0190, -0.0317,  ..., -0.0190,  0.0184, -0.0155],\n",
      "        [ 0.0098,  0.0164,  0.0158,  ...,  0.0094,  0.0256, -0.0205],\n",
      "        ...,\n",
      "        [ 0.0020, -0.0489,  0.0212,  ...,  0.0124, -0.0172, -0.0300],\n",
      "        [-0.0236, -0.0069, -0.0026,  ...,  0.0062, -0.0076,  0.0711],\n",
      "        [-0.0056, -0.0222,  0.0216,  ...,  0.0123,  0.0205,  0.0091]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0017,  0.0041, -0.0017,  ..., -0.0034, -0.0018,  0.0007],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0104, -0.0206, -0.0515,  ...,  0.0265,  0.0259, -0.0499],\n",
      "        [-0.0142,  0.0309,  0.0349,  ..., -0.0362,  0.0137, -0.0101],\n",
      "        [ 0.0289,  0.0136, -0.0009,  ...,  0.0381, -0.0148,  0.0059],\n",
      "        ...,\n",
      "        [ 0.0256,  0.0333, -0.0229,  ..., -0.0411, -0.0236,  0.0558],\n",
      "        [ 0.0040, -0.0165,  0.0172,  ...,  0.0046,  0.0361, -0.0142],\n",
      "        [-0.0115, -0.0053,  0.0424,  ...,  0.0292, -0.0133,  0.0344]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0108, -0.0124, -0.0181,  ...,  0.0481, -0.0050, -0.0231],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0484, -0.0010,  0.0290,  ..., -0.0010,  0.0269, -0.0480],\n",
      "        [-0.0750,  0.0033, -0.0140,  ..., -0.0028, -0.0396,  0.0069],\n",
      "        [ 0.0362,  0.0003, -0.0905,  ...,  0.0402,  0.0081, -0.0066],\n",
      "        ...,\n",
      "        [-0.0007,  0.0034,  0.0257,  ..., -0.0062, -0.0379, -0.0027],\n",
      "        [ 0.0116, -0.0224, -0.0042,  ...,  0.0052,  0.0057, -0.0016],\n",
      "        [ 0.0058,  0.0062, -0.0288,  ...,  0.0202,  0.0316,  0.0138]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0119, -0.0477,  0.0108,  ..., -0.0139,  0.0195,  0.0170],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0135, -0.0338, -0.0189,  ...,  0.0179, -0.0068, -0.0291],\n",
      "        [-0.0182,  0.0293, -0.0034,  ..., -0.0287,  0.0169,  0.0145],\n",
      "        [-0.0126,  0.0322,  0.0341,  ..., -0.0207, -0.0040, -0.0032],\n",
      "        ...,\n",
      "        [-0.0494, -0.0335, -0.0178,  ..., -0.0094,  0.0290, -0.0158],\n",
      "        [-0.0276, -0.0292, -0.0189,  ...,  0.0267, -0.0088, -0.0155],\n",
      "        [ 0.0436,  0.0120,  0.0214,  ...,  0.0165, -0.0121, -0.0163]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0093, -0.0205,  0.0549,  ...,  0.0239, -0.0451, -0.0077],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4570, 0.4531, 0.4268,  ..., 0.4746, 0.4619, 0.4668],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0199, -0.0120,  0.0975,  ...,  0.0012, -0.0177, -0.0125],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-4.0253e-02, -8.8501e-03, -3.9558e-03,  ...,  3.9220e-05,\n",
      "         -2.5818e-02,  3.5950e-02],\n",
      "        [ 3.1555e-02, -4.1084e-03, -1.3062e-02,  ...,  4.9896e-03,\n",
      "          3.7750e-02, -2.8744e-03],\n",
      "        [ 1.9684e-02, -1.1635e-02,  5.0537e-02,  ..., -1.9440e-02,\n",
      "          1.7960e-02, -3.7933e-02],\n",
      "        ...,\n",
      "        [ 5.2826e-02, -1.2726e-02,  7.3395e-03,  ..., -6.9771e-03,\n",
      "         -2.1591e-02,  2.4536e-02],\n",
      "        [-4.8187e-02, -1.9245e-03,  2.0477e-02,  ...,  2.5833e-02,\n",
      "         -8.4167e-02,  2.7771e-02],\n",
      "        [ 2.4841e-02,  1.4297e-02, -2.6031e-02,  ...,  4.9438e-02,\n",
      "          3.4637e-02,  1.1955e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0326, -0.0154,  0.0443,  ...,  0.0006, -0.0169,  0.0024],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0322,  0.0204,  0.0564,  ...,  0.0159,  0.0168, -0.0087],\n",
      "        [-0.0285,  0.0190, -0.0079,  ..., -0.0575,  0.0248, -0.0049],\n",
      "        [-0.0044, -0.0216,  0.0144,  ...,  0.0091, -0.0058,  0.0192],\n",
      "        ...,\n",
      "        [-0.0708,  0.0167,  0.0523,  ...,  0.0166,  0.0285, -0.0165],\n",
      "        [-0.0909,  0.0351,  0.0569,  ...,  0.0472, -0.0008,  0.0081],\n",
      "        [-0.0014, -0.0302, -0.0724,  ...,  0.0538, -0.1306,  0.0148]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0633, -0.0067,  0.0330,  ...,  0.0048,  0.0894,  0.0213],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-2.4780e-02,  1.5945e-02, -1.0315e-02,  ...,  1.4641e-02,\n",
      "          1.5465e-02, -8.1482e-03],\n",
      "        [-2.6749e-02, -8.4534e-03, -1.4450e-02,  ..., -1.7105e-02,\n",
      "          1.2733e-02,  6.0883e-03],\n",
      "        [ 3.8361e-02, -5.4840e-02,  1.6022e-02,  ...,  1.0094e-02,\n",
      "          1.5564e-02, -3.1174e-02],\n",
      "        ...,\n",
      "        [-6.3354e-02, -4.7211e-02,  1.8906e-02,  ...,  1.1429e-02,\n",
      "         -1.3092e-02,  2.9583e-03],\n",
      "        [-1.4030e-02, -1.2646e-03,  1.7410e-02,  ...,  4.0741e-02,\n",
      "         -2.8091e-02,  8.9294e-02],\n",
      "        [ 1.2123e-02,  2.9968e-02,  2.0564e-05,  ...,  1.2711e-02,\n",
      "          6.1798e-03, -1.6525e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0024,  0.0005,  0.0016,  ...,  0.0037,  0.0010, -0.0010],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0322,  0.0933, -0.0191,  ...,  0.0121,  0.0271, -0.0398],\n",
      "        [-0.0146,  0.0263,  0.0542,  ...,  0.0239,  0.0031,  0.0037],\n",
      "        [ 0.0220, -0.0155, -0.0368,  ..., -0.0246,  0.0030, -0.0439],\n",
      "        ...,\n",
      "        [-0.0079,  0.0125,  0.0018,  ...,  0.0335, -0.0151, -0.0122],\n",
      "        [-0.0217, -0.0314, -0.0433,  ...,  0.0258,  0.0189,  0.0118],\n",
      "        [ 0.0191, -0.0026,  0.0267,  ...,  0.0220, -0.0032,  0.0468]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0182, -0.0003, -0.0136,  ...,  0.0406,  0.0339,  0.0035],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0557,  0.0505,  0.0184,  ...,  0.0224,  0.0042,  0.0049],\n",
      "        [ 0.0250,  0.0660, -0.0278,  ...,  0.0165, -0.0130,  0.0807],\n",
      "        [ 0.0453,  0.0067, -0.0616,  ..., -0.0032, -0.0169,  0.0453],\n",
      "        ...,\n",
      "        [-0.0192, -0.0080,  0.0085,  ...,  0.0003,  0.0534, -0.0130],\n",
      "        [-0.0038, -0.0363,  0.0163,  ..., -0.0350, -0.0268, -0.0302],\n",
      "        [ 0.0518,  0.0425,  0.0263,  ...,  0.0041, -0.0352,  0.0222]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0049, -0.0486,  0.0123,  ..., -0.0020,  0.0252, -0.0051],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0072, -0.0027, -0.0362,  ...,  0.0257,  0.0053, -0.0349],\n",
      "        [-0.0121,  0.0081,  0.0209,  ...,  0.0162, -0.0003, -0.0323],\n",
      "        [-0.0213, -0.0179,  0.0208,  ..., -0.0304, -0.0058, -0.0292],\n",
      "        ...,\n",
      "        [-0.0104,  0.0013, -0.0195,  ...,  0.0200,  0.0483,  0.0302],\n",
      "        [ 0.0097, -0.0095,  0.0105,  ..., -0.0462,  0.0250,  0.0261],\n",
      "        [ 0.0174,  0.1018, -0.0544,  ..., -0.0262,  0.0144, -0.0341]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0054, -0.0096,  0.0554,  ...,  0.0271, -0.0453,  0.0170],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4475, 0.4480, 0.4207,  ..., 0.4712, 0.4575, 0.4653],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0173, -0.0159,  0.1003,  ..., -0.0006, -0.0224, -0.0118],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0086, -0.0015,  0.0275,  ...,  0.0003,  0.0142, -0.0079],\n",
      "        [-0.0349, -0.0002, -0.0058,  ...,  0.0538, -0.0017,  0.0378],\n",
      "        [-0.0021,  0.0534,  0.0399,  ...,  0.0130,  0.0160, -0.0218],\n",
      "        ...,\n",
      "        [-0.0165, -0.0128, -0.0127,  ...,  0.0835,  0.0249,  0.0051],\n",
      "        [ 0.0228,  0.0154,  0.0306,  ..., -0.0122, -0.0198,  0.0104],\n",
      "        [ 0.0299,  0.0082, -0.0284,  ...,  0.0215, -0.0217, -0.0424]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0012, -0.0215,  0.0204,  ...,  0.0197, -0.0309, -0.0461],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0679, -0.0567, -0.0182,  ...,  0.0241,  0.0217,  0.0182],\n",
      "        [-0.0231, -0.0392, -0.0125,  ..., -0.0035,  0.0157, -0.0048],\n",
      "        [-0.0151, -0.0285,  0.0251,  ..., -0.0149,  0.0007,  0.0041],\n",
      "        ...,\n",
      "        [ 0.0032, -0.0438,  0.0112,  ..., -0.0348,  0.0136, -0.0267],\n",
      "        [-0.0152, -0.0013,  0.0230,  ...,  0.0473, -0.1060, -0.0674],\n",
      "        [ 0.0292, -0.0250,  0.0081,  ...,  0.0342,  0.0182,  0.0516]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0618,  0.0498,  0.0680,  ..., -0.0031,  0.0267,  0.1565],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0024, -0.0388, -0.0124,  ..., -0.0339, -0.0180,  0.0297],\n",
      "        [-0.0213,  0.0095,  0.0018,  ...,  0.0064,  0.0074,  0.0295],\n",
      "        [-0.0013,  0.0012,  0.0083,  ...,  0.0043,  0.0159,  0.0361],\n",
      "        ...,\n",
      "        [-0.0572,  0.0578,  0.0253,  ..., -0.0166,  0.0706,  0.0138],\n",
      "        [ 0.0460, -0.0551,  0.0352,  ...,  0.0014,  0.0753,  0.0025],\n",
      "        [-0.0476, -0.0806, -0.0016,  ...,  0.0375, -0.0010, -0.0289]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0022,  0.0006, -0.0001,  ...,  0.0019,  0.0055,  0.0062],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0253, -0.0067, -0.0066,  ...,  0.0339, -0.0096, -0.0141],\n",
      "        [-0.0305,  0.0123,  0.0119,  ...,  0.0205,  0.0298,  0.0875],\n",
      "        [ 0.0148,  0.0021,  0.0100,  ...,  0.0687,  0.0112,  0.0628],\n",
      "        ...,\n",
      "        [ 0.0332,  0.0417, -0.0311,  ...,  0.0189,  0.0076,  0.0139],\n",
      "        [-0.0086,  0.0331, -0.0229,  ..., -0.0009, -0.0045, -0.0096],\n",
      "        [-0.0236,  0.0028, -0.0071,  ..., -0.0539,  0.0264,  0.0489]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0146,  0.0167, -0.0452,  ...,  0.0113,  0.0273, -0.0203],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0093,  0.0436, -0.0588,  ..., -0.0078,  0.0108, -0.0263],\n",
      "        [-0.0343,  0.0101, -0.0005,  ..., -0.0077, -0.0320,  0.0108],\n",
      "        [ 0.0164,  0.0142, -0.0318,  ..., -0.0123,  0.0140,  0.0071],\n",
      "        ...,\n",
      "        [-0.0274,  0.0422, -0.0325,  ...,  0.0334, -0.0172, -0.0042],\n",
      "        [-0.0164,  0.0337, -0.0119,  ..., -0.0188,  0.0302,  0.0257],\n",
      "        [ 0.0354,  0.0003, -0.0097,  ..., -0.0031,  0.0279,  0.0421]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0312, -0.0079, -0.0145,  ..., -0.0066, -0.0079,  0.0009],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0217,  0.0299, -0.0157,  ...,  0.0340, -0.0217, -0.0186],\n",
      "        [-0.0322,  0.0312, -0.0170,  ..., -0.0013, -0.0046, -0.0290],\n",
      "        [-0.0329,  0.0094, -0.0285,  ..., -0.0256, -0.0030, -0.0032],\n",
      "        ...,\n",
      "        [-0.0356,  0.0516,  0.0494,  ..., -0.0247, -0.0143, -0.0120],\n",
      "        [ 0.0283,  0.0188,  0.0222,  ..., -0.0079, -0.0149, -0.0390],\n",
      "        [ 0.0162, -0.0230, -0.0360,  ...,  0.0222,  0.0245, -0.0042]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0106, -0.0278,  0.0740,  ...,  0.0080, -0.0142,  0.0239],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4451, 0.4429, 0.4114,  ..., 0.4609, 0.4492, 0.4546],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0188, -0.0124,  0.0906,  ...,  0.0012, -0.0197, -0.0080],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0038,  0.0025, -0.0258,  ...,  0.0068,  0.0129,  0.0099],\n",
      "        [ 0.0355, -0.0036, -0.0197,  ...,  0.0168,  0.0374, -0.0163],\n",
      "        [ 0.0072,  0.0135, -0.0096,  ...,  0.0051, -0.0376, -0.0035],\n",
      "        ...,\n",
      "        [ 0.0102,  0.0074, -0.0275,  ..., -0.0025,  0.0388, -0.0388],\n",
      "        [-0.0238, -0.0011, -0.0202,  ...,  0.0401,  0.0021,  0.0480],\n",
      "        [ 0.0998, -0.0588,  0.0082,  ...,  0.0078,  0.0055, -0.0015]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0208, -0.0151,  0.0017,  ..., -0.0304,  0.0094, -0.0352],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-2.5620e-02,  2.1851e-02, -7.1526e-03,  ..., -3.9368e-02,\n",
      "          6.1417e-04,  1.7197e-02],\n",
      "        [-1.4931e-02,  1.5053e-02, -1.1971e-02,  ..., -2.8320e-02,\n",
      "         -8.1329e-03,  2.1271e-02],\n",
      "        [ 1.8036e-02,  4.6134e-05, -8.3542e-03,  ...,  1.6603e-03,\n",
      "         -3.9940e-03, -3.5114e-03],\n",
      "        ...,\n",
      "        [-1.7776e-02,  7.6752e-03, -1.1187e-03,  ..., -2.5215e-03,\n",
      "         -6.6223e-02, -3.0731e-02],\n",
      "        [-1.9252e-05,  3.4729e-02,  4.9927e-02,  ...,  5.1651e-03,\n",
      "         -4.6448e-02, -1.9028e-02],\n",
      "        [ 2.8439e-03,  2.3727e-03, -4.6967e-02,  ..., -9.9335e-03,\n",
      "         -5.1231e-03,  6.4545e-03]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0080,  0.0066, -0.0204,  ...,  0.0199,  0.0261, -0.0031],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0029, -0.0194, -0.0152,  ...,  0.0058, -0.0289,  0.0137],\n",
      "        [ 0.0226,  0.0042, -0.0007,  ..., -0.0059,  0.0059,  0.0021],\n",
      "        [ 0.0344, -0.0061, -0.0260,  ..., -0.0069,  0.0244, -0.0091],\n",
      "        ...,\n",
      "        [ 0.0290,  0.0189, -0.0252,  ...,  0.0385, -0.0281, -0.0210],\n",
      "        [ 0.0076,  0.0036, -0.0174,  ...,  0.0346, -0.0573,  0.0387],\n",
      "        [-0.0206, -0.0102,  0.0005,  ...,  0.0190,  0.0406,  0.0024]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0015,  0.0009,  0.0044,  ...,  0.0006, -0.0005,  0.0008],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0385,  0.0423, -0.0086,  ..., -0.0189,  0.0366, -0.0062],\n",
      "        [ 0.0063, -0.0025,  0.0102,  ..., -0.0387, -0.0057,  0.0199],\n",
      "        [ 0.0093,  0.0356, -0.0062,  ...,  0.0482,  0.0020, -0.0163],\n",
      "        ...,\n",
      "        [ 0.0320,  0.0489,  0.0381,  ..., -0.0125, -0.0159,  0.0036],\n",
      "        [ 0.0102, -0.0022, -0.0153,  ..., -0.0389,  0.0414, -0.0276],\n",
      "        [ 0.0124, -0.0275,  0.0441,  ...,  0.0253,  0.0243,  0.0029]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0173,  0.0281, -0.0245,  ...,  0.0456,  0.0075, -0.0281],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0136, -0.0075, -0.0199,  ..., -0.0247,  0.0187, -0.0162],\n",
      "        [-0.0864,  0.0246,  0.0023,  ...,  0.0269, -0.0515, -0.0170],\n",
      "        [ 0.0642,  0.0035, -0.0235,  ..., -0.0051, -0.0569,  0.0200],\n",
      "        ...,\n",
      "        [-0.0446, -0.0311, -0.0158,  ...,  0.0238, -0.0208,  0.0107],\n",
      "        [-0.0072, -0.0047, -0.0327,  ...,  0.0411,  0.0293, -0.0018],\n",
      "        [ 0.0195, -0.0235, -0.0016,  ..., -0.0226, -0.0024,  0.0139]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0406,  0.0183, -0.0406,  ..., -0.0016,  0.0145, -0.0037],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0017,  0.0501,  0.0100,  ...,  0.0396,  0.0460, -0.0161],\n",
      "        [-0.0225, -0.0368,  0.0140,  ...,  0.0384, -0.0196, -0.0460],\n",
      "        [ 0.0437, -0.0104, -0.0067,  ..., -0.0002,  0.0201,  0.0180],\n",
      "        ...,\n",
      "        [-0.0368, -0.0280,  0.0029,  ..., -0.0416, -0.0255,  0.0399],\n",
      "        [ 0.0064,  0.0549, -0.0043,  ...,  0.0104, -0.0166,  0.0056],\n",
      "        [ 0.0202,  0.0111,  0.0343,  ..., -0.0432,  0.0410,  0.0220]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0109, -0.0118,  0.0604,  ..., -0.0211, -0.0114,  0.0089],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4514, 0.4502, 0.4150,  ..., 0.4685, 0.4590, 0.4685],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0210, -0.0183,  0.1039,  ..., -0.0012, -0.0176, -0.0060],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0183,  0.0124, -0.0665,  ...,  0.0007,  0.0135,  0.0190],\n",
      "        [-0.0319, -0.0392,  0.0050,  ..., -0.0087,  0.0216,  0.0311],\n",
      "        [-0.0155, -0.0048,  0.0445,  ..., -0.0018, -0.0232, -0.0254],\n",
      "        ...,\n",
      "        [ 0.0859,  0.0809,  0.0019,  ..., -0.0375,  0.0400, -0.0023],\n",
      "        [-0.0142,  0.0225,  0.0281,  ...,  0.0049,  0.0339, -0.0168],\n",
      "        [ 0.0226, -0.0320,  0.0381,  ..., -0.0125, -0.0070,  0.0157]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0678, -0.0031,  0.0508,  ..., -0.0100,  0.0077,  0.0132],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0037,  0.0176, -0.0323,  ...,  0.0108, -0.0047, -0.0031],\n",
      "        [-0.0309,  0.0093,  0.0100,  ..., -0.0349, -0.0202,  0.0307],\n",
      "        [ 0.0004, -0.0032,  0.0214,  ..., -0.0091, -0.0236, -0.0307],\n",
      "        ...,\n",
      "        [-0.0224,  0.0248, -0.0165,  ...,  0.0040,  0.0039, -0.0062],\n",
      "        [ 0.0088,  0.0159,  0.0366,  ..., -0.0211,  0.0054,  0.0157],\n",
      "        [ 0.0337,  0.0739,  0.0298,  ...,  0.0327, -0.0076, -0.0511]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0885,  0.0077,  0.0466,  ..., -0.0863, -0.0213,  0.0362],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0447, -0.0272,  0.0031,  ...,  0.0279, -0.0462, -0.0215],\n",
      "        [-0.0327,  0.0052,  0.0235,  ...,  0.0090, -0.0197,  0.0217],\n",
      "        [ 0.0093, -0.0188,  0.0546,  ...,  0.0077, -0.0279, -0.0038],\n",
      "        ...,\n",
      "        [-0.0199,  0.0309,  0.0437,  ..., -0.0128,  0.0463,  0.0174],\n",
      "        [ 0.0556, -0.0010,  0.0346,  ..., -0.0279,  0.0330,  0.0218],\n",
      "        [-0.0049, -0.0242, -0.0176,  ...,  0.0116,  0.0176,  0.0247]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([0.0005, 0.0037, 0.0005,  ..., 0.0010, 0.0021, 0.0025],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0156,  0.0284,  0.0212,  ...,  0.0475, -0.0919,  0.0158],\n",
      "        [ 0.0108, -0.0090, -0.0032,  ..., -0.0261,  0.0302, -0.0057],\n",
      "        [ 0.0113, -0.0478, -0.1019,  ..., -0.0228, -0.0599, -0.0078],\n",
      "        ...,\n",
      "        [-0.0243, -0.0156, -0.0044,  ...,  0.0290,  0.0365,  0.0470],\n",
      "        [ 0.0189,  0.0021,  0.0337,  ..., -0.0981, -0.0647, -0.0338],\n",
      "        [ 0.0317,  0.0117, -0.0096,  ..., -0.0357,  0.0438, -0.0009]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0070,  0.0275, -0.0242,  ...,  0.0582, -0.0170,  0.0365],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0467,  0.0025,  0.0070,  ...,  0.0085, -0.0324,  0.0058],\n",
      "        [-0.0058, -0.0057, -0.0192,  ...,  0.0102, -0.0063,  0.0054],\n",
      "        [-0.0294, -0.0387, -0.0258,  ..., -0.0051,  0.0153, -0.0083],\n",
      "        ...,\n",
      "        [ 0.0215,  0.0107, -0.0146,  ..., -0.0015, -0.0441, -0.0370],\n",
      "        [-0.0290, -0.0343,  0.0117,  ..., -0.0015, -0.0203, -0.0087],\n",
      "        [ 0.0218, -0.0128,  0.0175,  ...,  0.0356,  0.0131, -0.0175]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0379, -0.0257, -0.0060,  ..., -0.0401, -0.0081, -0.0018],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 3.0243e-02, -1.2833e-02,  6.6161e-05,  ...,  3.0106e-02,\n",
      "          2.1343e-03,  7.0877e-03],\n",
      "        [-2.4452e-03, -3.5889e-02,  1.2016e-02,  ...,  2.6459e-02,\n",
      "         -4.6448e-02,  2.1191e-03],\n",
      "        [-2.8259e-02,  2.2446e-02,  8.7967e-03,  ...,  2.5558e-02,\n",
      "         -1.6586e-02, -1.4252e-02],\n",
      "        ...,\n",
      "        [-3.4485e-02,  4.1016e-02,  4.0894e-03,  ...,  7.2021e-03,\n",
      "          9.3765e-03, -1.4389e-02],\n",
      "        [-4.0588e-02, -7.3891e-03,  1.1787e-02,  ...,  3.4698e-02,\n",
      "         -1.4557e-02, -8.4991e-03],\n",
      "        [ 1.0605e-02, -4.4403e-03,  5.1392e-02,  ...,  1.8188e-02,\n",
      "          6.9389e-03,  3.2013e-02]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0043, -0.0052,  0.0795,  ..., -0.0308, -0.0108,  0.0011],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4309, 0.4399, 0.4221,  ..., 0.4539, 0.4475, 0.4526],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0176, -0.0139,  0.1109,  ..., -0.0072, -0.0156, -0.0072],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0298,  0.0395,  0.0085,  ..., -0.0093, -0.0400,  0.0197],\n",
      "        [ 0.0308,  0.0647,  0.0273,  ..., -0.0135,  0.0039,  0.0164],\n",
      "        [ 0.0366,  0.1071,  0.0435,  ...,  0.0469, -0.0634,  0.0096],\n",
      "        ...,\n",
      "        [ 0.0271, -0.0202, -0.0270,  ..., -0.0825,  0.0080, -0.0446],\n",
      "        [ 0.0009,  0.0180, -0.0286,  ..., -0.1019,  0.0129,  0.0221],\n",
      "        [ 0.0070, -0.0077, -0.0072,  ...,  0.0182,  0.0010,  0.0108]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 3.9459e-02,  2.9068e-02, -7.5102e-05,  ...,  1.9348e-02,\n",
      "        -7.7343e-04, -7.9811e-05], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0333, -0.0462,  0.0246,  ...,  0.0085, -0.0029,  0.0318],\n",
      "        [ 0.0035, -0.0419, -0.0001,  ...,  0.0058,  0.0187, -0.0087],\n",
      "        [-0.0426,  0.0411,  0.0269,  ...,  0.0388, -0.0666, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0078,  0.0590, -0.0120,  ...,  0.0342,  0.0048,  0.0177],\n",
      "        [-0.0286, -0.0544, -0.0211,  ...,  0.0072,  0.0152,  0.0002],\n",
      "        [-0.0498,  0.0688, -0.0183,  ..., -0.0306, -0.0195, -0.0429]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0571,  0.0286,  0.0009,  ..., -0.2817, -0.0139, -0.1594],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0018,  0.0449, -0.0105,  ..., -0.0365, -0.0274,  0.0184],\n",
      "        [-0.0517,  0.0344, -0.0325,  ..., -0.0304, -0.0075,  0.0270],\n",
      "        [-0.0538,  0.0165, -0.0183,  ...,  0.0135, -0.0027, -0.0350],\n",
      "        ...,\n",
      "        [ 0.0099,  0.0181,  0.0163,  ...,  0.0318, -0.0016, -0.0215],\n",
      "        [ 0.0183, -0.0439,  0.0113,  ...,  0.0301,  0.0466, -0.0039],\n",
      "        [-0.0215, -0.0453,  0.0075,  ...,  0.0193, -0.0120, -0.0076]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0012, -0.0021, -0.0003,  ...,  0.0015,  0.0007, -0.0006],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0063,  0.0754,  0.0379,  ..., -0.0104, -0.0301,  0.0602],\n",
      "        [-0.0484, -0.0479, -0.0275,  ..., -0.0573,  0.0310,  0.0602],\n",
      "        [ 0.0209,  0.0449,  0.0145,  ..., -0.0344, -0.0262,  0.0185],\n",
      "        ...,\n",
      "        [ 0.0088,  0.0576, -0.0168,  ..., -0.0181,  0.0300,  0.0095],\n",
      "        [ 0.0393,  0.0224, -0.0266,  ...,  0.0275, -0.0238, -0.0037],\n",
      "        [-0.0227, -0.0177,  0.0337,  ..., -0.0345,  0.0173,  0.0183]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0152,  0.0083, -0.0514,  ...,  0.0526, -0.0085,  0.0039],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0892,  0.0576, -0.0513,  ..., -0.0400, -0.0086, -0.0343],\n",
      "        [ 0.0568,  0.0536,  0.0203,  ..., -0.0132,  0.0189,  0.0226],\n",
      "        [ 0.0389,  0.0471,  0.0373,  ..., -0.0334,  0.0077,  0.0408],\n",
      "        ...,\n",
      "        [-0.0403,  0.0002, -0.0135,  ...,  0.0108, -0.0135, -0.0097],\n",
      "        [-0.0081, -0.0413,  0.0019,  ..., -0.0057, -0.0040, -0.0077],\n",
      "        [ 0.0108, -0.0173, -0.0054,  ...,  0.0011, -0.0054,  0.0434]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0325, -0.0349, -0.0073,  ..., -0.0068,  0.0066, -0.0116],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0887,  0.0148, -0.0080,  ...,  0.0230,  0.0173, -0.0292],\n",
      "        [ 0.0552, -0.0216, -0.0366,  ...,  0.0029,  0.0031, -0.0024],\n",
      "        [-0.0040,  0.0453, -0.0272,  ...,  0.0145,  0.0050, -0.0302],\n",
      "        ...,\n",
      "        [-0.0065,  0.0478,  0.0329,  ..., -0.0129,  0.0119,  0.0032],\n",
      "        [ 0.0136, -0.0055,  0.0040,  ...,  0.0082,  0.0244,  0.0005],\n",
      "        [-0.0278,  0.0162, -0.0238,  ...,  0.0066,  0.0018, -0.0094]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0111, -0.0141,  0.0723,  ..., -0.0114, -0.0070, -0.0062],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4453, 0.4487, 0.4443,  ..., 0.4570, 0.4463, 0.4629],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0303, -0.0246,  0.1056,  ..., -0.0147, -0.0162, -0.0124],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0071, -0.0356, -0.0120,  ...,  0.0374,  0.0078,  0.0080],\n",
      "        [ 0.0311, -0.0367, -0.0572,  ...,  0.0139, -0.0039, -0.0128],\n",
      "        [-0.0435,  0.0085, -0.0239,  ..., -0.0062, -0.0409,  0.0540],\n",
      "        ...,\n",
      "        [ 0.0202, -0.0047, -0.0272,  ..., -0.0083,  0.0528,  0.0061],\n",
      "        [-0.0080, -0.0023, -0.0504,  ..., -0.0152,  0.1074, -0.0085],\n",
      "        [ 0.0258,  0.0575, -0.0029,  ...,  0.0100,  0.0153,  0.0173]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0145, -0.0451, -0.0053,  ..., -0.0007, -0.0063,  0.0315],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0106, -0.0585,  0.0100,  ...,  0.0400, -0.0100,  0.0266],\n",
      "        [ 0.0102, -0.0335,  0.0134,  ..., -0.0221,  0.0009, -0.0195],\n",
      "        [-0.0211,  0.0199,  0.0537,  ...,  0.0007, -0.0114, -0.0033],\n",
      "        ...,\n",
      "        [ 0.0006, -0.0427, -0.0140,  ..., -0.0285,  0.0241, -0.0057],\n",
      "        [ 0.0155, -0.0041,  0.0035,  ...,  0.0046,  0.0590,  0.0578],\n",
      "        [-0.0013, -0.0518, -0.0208,  ...,  0.0195,  0.0101,  0.0032]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0284, -0.0363, -0.0120,  ...,  0.0386,  0.0879,  0.1543],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0602, -0.0171,  0.0449,  ...,  0.0257, -0.0175, -0.0418],\n",
      "        [-0.0477, -0.0588, -0.0290,  ...,  0.0034,  0.0073, -0.0400],\n",
      "        [-0.0024,  0.0191,  0.0003,  ...,  0.0032, -0.0005, -0.0156],\n",
      "        ...,\n",
      "        [ 0.0622,  0.0211,  0.0151,  ..., -0.0623,  0.0189, -0.0364],\n",
      "        [-0.0734, -0.0856,  0.0150,  ..., -0.0511,  0.0359, -0.0453],\n",
      "        [-0.0026, -0.0432, -0.0232,  ...,  0.0050, -0.0527, -0.0024]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0003, -0.0004, -0.0016,  ...,  0.0009,  0.0002,  0.0013],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0355,  0.0627,  0.0235,  ..., -0.0348,  0.0146, -0.0095],\n",
      "        [ 0.0104, -0.0076, -0.0065,  ..., -0.0023,  0.0269,  0.0057],\n",
      "        [-0.0376, -0.0144, -0.0258,  ...,  0.0665, -0.0254,  0.0542],\n",
      "        ...,\n",
      "        [-0.0507, -0.0071, -0.0070,  ...,  0.0686,  0.0017, -0.0081],\n",
      "        [ 0.0086, -0.0324,  0.0343,  ...,  0.0080,  0.0273,  0.0431],\n",
      "        [ 0.0482, -0.0612,  0.0282,  ...,  0.0287,  0.0088,  0.0291]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0092, -0.0057, -0.0505,  ...,  0.0225, -0.0073,  0.0055],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0763, -0.0073, -0.0122,  ...,  0.0479,  0.0175,  0.0505],\n",
      "        [-0.0443,  0.0018, -0.0090,  ..., -0.0302, -0.0087, -0.0017],\n",
      "        [-0.0221,  0.0696,  0.0065,  ...,  0.0038,  0.0302,  0.0129],\n",
      "        ...,\n",
      "        [-0.0107, -0.0085, -0.0029,  ...,  0.0299,  0.0008, -0.0171],\n",
      "        [ 0.0071,  0.0179, -0.0134,  ...,  0.0042, -0.0148, -0.0155],\n",
      "        [-0.0149, -0.0030, -0.0594,  ..., -0.0181, -0.0322,  0.0199]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0364, -0.0307, -0.0108,  ...,  0.0126,  0.0052, -0.0009],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0703,  0.0019,  0.0161,  ..., -0.0195, -0.0020, -0.0083],\n",
      "        [ 0.0239, -0.0087, -0.0656,  ...,  0.0018, -0.0270, -0.0179],\n",
      "        [ 0.0195, -0.0080, -0.0111,  ..., -0.0065,  0.0034,  0.0327],\n",
      "        ...,\n",
      "        [ 0.0818,  0.0135,  0.0117,  ...,  0.0172, -0.0071, -0.0175],\n",
      "        [ 0.0861,  0.0224, -0.0556,  ..., -0.0031,  0.0286,  0.0172],\n",
      "        [ 0.0295,  0.0298,  0.0212,  ...,  0.0374,  0.0455, -0.0198]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0023, -0.0091,  0.0575,  ...,  0.0049, -0.0065, -0.0035],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4551, 0.4578, 0.4358,  ..., 0.4641, 0.4622, 0.4746],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0215, -0.0224,  0.1087,  ..., -0.0102, -0.0132, -0.0117],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0103, -0.0010,  0.0244,  ...,  0.0088,  0.0101, -0.0016],\n",
      "        [-0.0103,  0.0094,  0.0176,  ...,  0.0055,  0.0633, -0.0103],\n",
      "        [ 0.0405,  0.0076, -0.0311,  ..., -0.0336, -0.0403,  0.0172],\n",
      "        ...,\n",
      "        [ 0.0144, -0.0313,  0.0668,  ..., -0.0131,  0.0072, -0.0218],\n",
      "        [ 0.0351,  0.0087,  0.0197,  ..., -0.0083,  0.0581,  0.0343],\n",
      "        [-0.0354, -0.0152, -0.0512,  ...,  0.0264, -0.0437,  0.0478]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0012, -0.0089,  0.0100,  ...,  0.0974,  0.1038, -0.0726],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0023, -0.0058, -0.0162,  ..., -0.0082,  0.0439, -0.0253],\n",
      "        [ 0.0482,  0.0078, -0.0117,  ..., -0.0337,  0.0149,  0.0115],\n",
      "        [-0.0003,  0.0502,  0.0059,  ...,  0.0195, -0.0520, -0.0159],\n",
      "        ...,\n",
      "        [ 0.0094,  0.0251,  0.0047,  ...,  0.0079, -0.0166, -0.0137],\n",
      "        [ 0.0042,  0.0435, -0.0698,  ..., -0.0034,  0.0016,  0.0077],\n",
      "        [-0.0250, -0.0322, -0.0206,  ..., -0.0088,  0.0474,  0.0330]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0026,  0.0052, -0.0226,  ..., -1.4189, -1.4453,  1.1035],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0402,  0.0062, -0.0138,  ..., -0.0258,  0.0341, -0.0154],\n",
      "        [-0.0459,  0.0232, -0.0124,  ...,  0.0320, -0.0041, -0.0323],\n",
      "        [ 0.0274,  0.0381,  0.0072,  ...,  0.0038,  0.0084,  0.0161],\n",
      "        ...,\n",
      "        [ 0.0163,  0.0327, -0.0102,  ...,  0.0296,  0.0358, -0.0447],\n",
      "        [ 0.0126,  0.0352,  0.0660,  ...,  0.0473, -0.0378,  0.0303],\n",
      "        [-0.0148,  0.0343, -0.0071,  ..., -0.0255,  0.0053, -0.0033]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-1.0643e-03,  4.3845e-04, -2.5005e-03,  ..., -2.8825e-04,\n",
      "        -4.3333e-05,  1.7576e-03], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0107,  0.0068,  0.0249,  ..., -0.0411,  0.0547,  0.0045],\n",
      "        [ 0.0296,  0.0632,  0.0022,  ..., -0.0244, -0.0155, -0.0514],\n",
      "        [-0.0206,  0.0045, -0.0192,  ..., -0.0034, -0.0335,  0.0091],\n",
      "        ...,\n",
      "        [ 0.0246,  0.0070,  0.0521,  ..., -0.0173,  0.0039,  0.0469],\n",
      "        [ 0.0135,  0.0088,  0.0103,  ..., -0.0430,  0.0367,  0.0262],\n",
      "        [ 0.0187,  0.0198,  0.0069,  ...,  0.0475, -0.0298,  0.0043]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0254,  0.0094, -0.0605,  ..., -0.0178, -0.0482,  0.0677],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0385,  0.0222,  0.0151,  ...,  0.0227,  0.0282,  0.0084],\n",
      "        [-0.0186, -0.0013,  0.0062,  ...,  0.0209,  0.0286,  0.0145],\n",
      "        [-0.0271, -0.0149,  0.0075,  ...,  0.0266, -0.0012, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0518, -0.0195,  ...,  0.0076, -0.0598,  0.0081],\n",
      "        [ 0.0182,  0.0103,  0.0226,  ..., -0.0293, -0.0283, -0.0026],\n",
      "        [-0.0181, -0.0130,  0.0147,  ...,  0.0035, -0.0022, -0.0042]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0384, -0.0315, -0.0075,  ..., -0.0044, -0.0151, -0.0134],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0425, -0.0257,  0.0175,  ...,  0.0199, -0.0057,  0.0393],\n",
      "        [ 0.0300,  0.0027, -0.0459,  ...,  0.0117, -0.0078, -0.0195],\n",
      "        [-0.0038, -0.0098, -0.0578,  ...,  0.0343, -0.0218,  0.0008],\n",
      "        ...,\n",
      "        [ 0.0161, -0.0094,  0.0044,  ..., -0.0008,  0.0056, -0.0163],\n",
      "        [-0.0004, -0.0256,  0.0089,  ...,  0.0182,  0.0568, -0.0133],\n",
      "        [-0.0659,  0.0070, -0.0284,  ..., -0.0356,  0.0028,  0.0100]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0078, -0.0244,  0.0550,  ..., -0.0007, -0.0086, -0.0117],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4578, 0.4634, 0.4514,  ..., 0.4524, 0.4519, 0.4722],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0246, -0.0273,  0.1160,  ..., -0.0076, -0.0181, -0.0170],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0085, -0.0248,  0.0286,  ...,  0.0163,  0.0057,  0.0438],\n",
      "        [-0.0073,  0.0060, -0.0234,  ..., -0.0034, -0.0577,  0.0117],\n",
      "        [-0.0274,  0.0446, -0.0102,  ..., -0.0632, -0.0339, -0.0397],\n",
      "        ...,\n",
      "        [-0.0025, -0.0114,  0.0253,  ..., -0.0011, -0.0174, -0.0380],\n",
      "        [-0.0155,  0.0245,  0.0256,  ...,  0.0386, -0.0535,  0.0646],\n",
      "        [-0.0092,  0.0005, -0.0149,  ..., -0.0209,  0.0217, -0.0164]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0075, -0.0188, -0.0121,  ...,  0.0137,  0.0184, -0.0160],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0291,  0.0096,  0.0001,  ...,  0.0008,  0.0062, -0.0294],\n",
      "        [-0.0100, -0.0324,  0.0266,  ..., -0.0504,  0.0391,  0.0240],\n",
      "        [ 0.0124, -0.0023,  0.0294,  ...,  0.0194,  0.0282, -0.0137],\n",
      "        ...,\n",
      "        [ 0.0240, -0.0047,  0.0193,  ...,  0.0872, -0.0416,  0.0136],\n",
      "        [ 0.0028, -0.0357, -0.0069,  ..., -0.0347, -0.0338, -0.0352],\n",
      "        [-0.0567,  0.0189,  0.0082,  ..., -0.0630,  0.0142, -0.0026]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0090,  0.0095,  0.0092,  ...,  0.1039, -0.1881,  0.3750],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0372, -0.0247, -0.0399,  ..., -0.0605, -0.0140, -0.0164],\n",
      "        [ 0.0327, -0.0199, -0.0181,  ...,  0.0140,  0.0157,  0.0001],\n",
      "        [-0.0290, -0.0375, -0.0397,  ..., -0.0146, -0.0534, -0.0253],\n",
      "        ...,\n",
      "        [ 0.0117,  0.0043,  0.0221,  ...,  0.0079, -0.0406, -0.0088],\n",
      "        [-0.0208,  0.0090, -0.0031,  ...,  0.0113,  0.0109, -0.0019],\n",
      "        [-0.0385,  0.0212,  0.0154,  ...,  0.0387, -0.0003, -0.0259]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 3.1888e-05, -1.4172e-03,  1.1358e-03,  ...,  4.0436e-03,\n",
      "         1.6747e-03,  3.6669e-04], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0259, -0.0500, -0.0081,  ...,  0.0410,  0.0038, -0.0108],\n",
      "        [-0.0061,  0.0038, -0.0134,  ..., -0.0330, -0.0271,  0.0372],\n",
      "        [ 0.0002,  0.0010, -0.0355,  ...,  0.0540,  0.0724,  0.0507],\n",
      "        ...,\n",
      "        [-0.0227, -0.0294, -0.0578,  ...,  0.0059,  0.0034,  0.0037],\n",
      "        [-0.0180,  0.0083, -0.0086,  ..., -0.0545, -0.0278,  0.0155],\n",
      "        [ 0.0185, -0.0318,  0.0003,  ..., -0.0189, -0.0271,  0.0172]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0272,  0.0106, -0.0667,  ...,  0.0182, -0.0192,  0.0601],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.7069e-02, -2.4490e-02, -4.9835e-02,  ...,  2.4689e-02,\n",
      "         -8.0933e-02, -2.5894e-02],\n",
      "        [ 5.1483e-02,  5.0323e-02, -5.4291e-02,  ..., -3.2135e-02,\n",
      "         -8.6746e-03, -4.4159e-02],\n",
      "        [ 6.5804e-03, -1.9470e-02, -2.2781e-02,  ...,  6.8604e-02,\n",
      "          6.0333e-02,  6.3171e-02],\n",
      "        ...,\n",
      "        [-2.5024e-03,  4.9171e-03, -1.6470e-03,  ..., -2.2858e-02,\n",
      "          1.0193e-02,  5.9395e-03],\n",
      "        [-3.0609e-02,  2.1622e-02,  7.7367e-05,  ..., -2.2141e-02,\n",
      "         -7.9651e-02,  1.3176e-02],\n",
      "        [ 2.3232e-03,  1.5930e-02,  1.9455e-02,  ...,  9.1858e-03,\n",
      "         -7.3891e-03,  9.4833e-03]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0621, -0.0251, -0.0283,  ..., -0.0166, -0.0580, -0.0498],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0032, -0.0121, -0.0300,  ...,  0.0144,  0.0204,  0.0107],\n",
      "        [-0.0342,  0.0479, -0.0497,  ...,  0.0512, -0.0180, -0.0017],\n",
      "        [ 0.0207, -0.0103,  0.0654,  ..., -0.0165,  0.0021,  0.0128],\n",
      "        ...,\n",
      "        [ 0.0075, -0.0132, -0.0385,  ...,  0.0403,  0.0436, -0.0099],\n",
      "        [-0.0654, -0.0023, -0.0420,  ...,  0.0427,  0.0579,  0.0199],\n",
      "        [-0.0082, -0.0090,  0.0717,  ..., -0.0291, -0.0369,  0.0266]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0194, -0.0203,  0.0432,  ...,  0.0070, -0.0119, -0.0181],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4722, 0.4807, 0.4565,  ..., 0.4763, 0.4885, 0.5015],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0236, -0.0270,  0.1196,  ..., -0.0084, -0.0152, -0.0146],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0506, -0.0273, -0.0007,  ...,  0.0354,  0.0172,  0.0004],\n",
      "        [-0.0091,  0.0078,  0.0109,  ...,  0.0594, -0.0767, -0.0443],\n",
      "        [ 0.0052, -0.0421, -0.0173,  ..., -0.0286,  0.0081,  0.0209],\n",
      "        ...,\n",
      "        [-0.0334,  0.0239,  0.0353,  ..., -0.0021,  0.0123,  0.0076],\n",
      "        [-0.0002, -0.0080,  0.0400,  ..., -0.0283, -0.0038, -0.0366],\n",
      "        [-0.0178,  0.0264, -0.0461,  ..., -0.0056, -0.0332, -0.0209]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0271,  0.0036, -0.0158,  ...,  0.0358,  0.0117, -0.0049],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0377,  0.0276, -0.0228,  ..., -0.0088, -0.0184, -0.0219],\n",
      "        [-0.0012,  0.0638,  0.0457,  ...,  0.0319,  0.0195,  0.0005],\n",
      "        [-0.0016,  0.0056, -0.0268,  ...,  0.0342, -0.0034,  0.0057],\n",
      "        ...,\n",
      "        [-0.0017,  0.0385, -0.0157,  ...,  0.0409,  0.0313,  0.0018],\n",
      "        [ 0.0205, -0.0255, -0.0137,  ..., -0.0149, -0.0087,  0.0123],\n",
      "        [ 0.0045,  0.0690,  0.0770,  ...,  0.0138, -0.0253, -0.0142]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0340, -0.0146, -0.0245,  ..., -0.1035, -0.1501,  0.1511],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0121,  0.0354,  0.0011,  ..., -0.0312,  0.0155, -0.0157],\n",
      "        [ 0.0422, -0.0220, -0.0373,  ...,  0.0536,  0.0179,  0.0556],\n",
      "        [-0.0446,  0.0314,  0.0116,  ..., -0.0013, -0.0345, -0.0207],\n",
      "        ...,\n",
      "        [-0.0392, -0.0267, -0.0264,  ..., -0.0137,  0.0482, -0.0254],\n",
      "        [ 0.0056,  0.0441,  0.0251,  ..., -0.0283, -0.0177, -0.0063],\n",
      "        [ 0.0183,  0.0222, -0.0061,  ..., -0.0519, -0.0353, -0.0093]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-1.5764e-03, -2.5272e-04,  1.7059e-04,  ...,  4.5891e-03,\n",
      "        -2.2137e-04,  6.5565e-05], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0131, -0.0113,  0.0031,  ..., -0.0069, -0.0102,  0.0178],\n",
      "        [ 0.0295, -0.0180, -0.0280,  ...,  0.0174, -0.0015,  0.0132],\n",
      "        [-0.0048,  0.0508,  0.0076,  ..., -0.0157,  0.0296, -0.0239],\n",
      "        ...,\n",
      "        [-0.0051, -0.0453,  0.0225,  ...,  0.0187, -0.0516, -0.0057],\n",
      "        [ 0.0186, -0.0141,  0.0286,  ..., -0.0155,  0.0071,  0.0344],\n",
      "        [ 0.0260,  0.0008,  0.0029,  ..., -0.0002,  0.0423,  0.0341]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0040, -0.0107, -0.0611,  ...,  0.0219,  0.0132,  0.0422],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-1.2245e-02, -4.5891e-03, -3.7933e-02,  ..., -3.4698e-02,\n",
      "         -4.0359e-03,  6.0616e-03],\n",
      "        [ 1.4191e-02, -2.2842e-02, -6.7505e-02,  ...,  4.6570e-02,\n",
      "         -1.2344e-02, -1.9028e-02],\n",
      "        [ 4.5853e-03, -1.5140e-05, -3.2043e-02,  ..., -1.2718e-02,\n",
      "          5.3955e-02, -2.3972e-02],\n",
      "        ...,\n",
      "        [ 4.7852e-02, -2.7222e-02,  2.2934e-02,  ...,  6.5613e-02,\n",
      "          2.0218e-02, -3.4912e-02],\n",
      "        [-1.7761e-02, -1.1187e-03, -6.7017e-02,  ...,  4.5419e-04,\n",
      "          3.2101e-03,  2.9465e-02],\n",
      "        [-1.2579e-03, -1.5297e-02, -3.5492e-02,  ..., -1.7090e-02,\n",
      "          2.8744e-03,  1.6953e-02]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0396, -0.0310, -0.0202,  ...,  0.0098, -0.0145,  0.0016],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0344, -0.0263, -0.0256,  ...,  0.0019, -0.0301, -0.0206],\n",
      "        [-0.0206,  0.0622,  0.0086,  ...,  0.0174,  0.0241, -0.0132],\n",
      "        [-0.0053, -0.0120,  0.0265,  ..., -0.0170,  0.0272, -0.0351],\n",
      "        ...,\n",
      "        [-0.0122, -0.0195,  0.0121,  ..., -0.0221,  0.0123,  0.0216],\n",
      "        [ 0.0317, -0.0690,  0.0114,  ..., -0.0218,  0.0155,  0.0280],\n",
      "        [ 0.0333,  0.0113,  0.0176,  ...,  0.0385,  0.0424, -0.0040]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0179, -0.0172,  0.0493,  ...,  0.0059, -0.0098, -0.0451],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4797, 0.4946, 0.4717,  ..., 0.4785, 0.4912, 0.5156],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0220, -0.0267,  0.1263,  ..., -0.0115, -0.0215, -0.0168],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0093, -0.0114,  0.0121,  ...,  0.0090, -0.0174, -0.0123],\n",
      "        [ 0.0009, -0.0299,  0.0006,  ...,  0.0138, -0.0104, -0.0807],\n",
      "        [-0.0334,  0.0103,  0.0196,  ..., -0.0504, -0.0154,  0.0112],\n",
      "        ...,\n",
      "        [ 0.0331,  0.0315, -0.0025,  ...,  0.0346,  0.0528, -0.0038],\n",
      "        [ 0.0151,  0.0373, -0.0295,  ...,  0.0288, -0.0596,  0.0958],\n",
      "        [-0.0286, -0.0572,  0.0030,  ...,  0.0185,  0.0077, -0.0328]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0007,  0.0005, -0.0165,  ...,  0.0101, -0.1359, -0.0311],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0214,  0.0278,  0.0028,  ..., -0.0122,  0.0142, -0.0147],\n",
      "        [ 0.0323,  0.0151,  0.0143,  ...,  0.0588,  0.0418,  0.0194],\n",
      "        [ 0.0145, -0.0108, -0.0340,  ..., -0.0249, -0.0127, -0.0188],\n",
      "        ...,\n",
      "        [ 0.0088, -0.0095, -0.0074,  ...,  0.0117,  0.0056, -0.0207],\n",
      "        [-0.0250, -0.0025,  0.0241,  ...,  0.0143, -0.0233, -0.0163],\n",
      "        [ 0.0411,  0.0181,  0.0141,  ..., -0.0314,  0.0117, -0.0051]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 1.1909e-02, -9.8953e-03, -8.5020e-04,  ...,  1.6394e-01,\n",
      "        -1.1475e+00, -1.4050e-01], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0104, -0.0464, -0.0158,  ..., -0.0106, -0.0261,  0.0211],\n",
      "        [ 0.0215, -0.0501,  0.0127,  ...,  0.0090, -0.0843, -0.0339],\n",
      "        [ 0.0106,  0.0154, -0.0007,  ...,  0.0237, -0.0248,  0.0285],\n",
      "        ...,\n",
      "        [ 0.0428, -0.0787,  0.0317,  ...,  0.0292,  0.0151, -0.0278],\n",
      "        [ 0.0357, -0.0264, -0.0101,  ...,  0.0600, -0.0450,  0.0375],\n",
      "        [-0.0240, -0.0086, -0.0083,  ...,  0.0048, -0.0300,  0.0238]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0003,  0.0012,  0.0009,  ..., -0.0017,  0.0004, -0.0015],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0199,  0.0272,  0.0627,  ..., -0.0058,  0.0566,  0.0018],\n",
      "        [-0.0469, -0.0083, -0.0174,  ...,  0.0312,  0.0155,  0.0421],\n",
      "        [-0.0261,  0.0029, -0.0186,  ..., -0.0420, -0.0250,  0.0395],\n",
      "        ...,\n",
      "        [-0.0154, -0.0263,  0.0275,  ..., -0.0129,  0.0018,  0.0753],\n",
      "        [ 0.0216,  0.0003, -0.0151,  ..., -0.0182, -0.0231, -0.0244],\n",
      "        [ 0.0347,  0.0422,  0.0035,  ...,  0.0211, -0.0305,  0.0337]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0219,  0.0543, -0.0291,  ...,  0.0451, -0.0251,  0.0208],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0362,  0.0038,  0.0273,  ...,  0.0512, -0.0211,  0.0216],\n",
      "        [-0.0311, -0.0288, -0.0004,  ...,  0.0305,  0.0045,  0.0490],\n",
      "        [ 0.0016, -0.0026, -0.0100,  ...,  0.0029,  0.0071, -0.0189],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0689,  0.0283,  ...,  0.0086,  0.0147, -0.0010],\n",
      "        [ 0.0263, -0.0498, -0.0114,  ..., -0.0289,  0.0087,  0.0215],\n",
      "        [ 0.0226, -0.0038, -0.0625,  ..., -0.0206,  0.0282,  0.0530]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0220, -0.0258, -0.0254,  ...,  0.0008,  0.0025, -0.0040],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0129,  0.0217,  0.0092,  ...,  0.0099, -0.0199, -0.0193],\n",
      "        [ 0.0131, -0.0041, -0.0156,  ..., -0.0275, -0.0069,  0.0206],\n",
      "        [ 0.0189, -0.0210,  0.0159,  ...,  0.0043, -0.0172,  0.0042],\n",
      "        ...,\n",
      "        [-0.0274,  0.0555, -0.0119,  ..., -0.0468, -0.0472,  0.0191],\n",
      "        [-0.0073, -0.0287,  0.0227,  ..., -0.0162, -0.0340, -0.0338],\n",
      "        [ 0.0242,  0.0430, -0.0079,  ...,  0.0041,  0.0042,  0.0115]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0127, -0.0108,  0.0503,  ...,  0.0015, -0.0052, -0.0223],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4897, 0.4961, 0.4919,  ..., 0.4736, 0.4971, 0.5107],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0270, -0.0334,  0.1161,  ..., -0.0165, -0.0232, -0.0206],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0194, -0.0107, -0.0031,  ..., -0.0057, -0.0385,  0.0100],\n",
      "        [-0.0156, -0.0165, -0.0131,  ...,  0.0313,  0.0080,  0.0386],\n",
      "        [-0.0352, -0.0065,  0.0307,  ..., -0.0089,  0.0156,  0.0132],\n",
      "        ...,\n",
      "        [ 0.0010,  0.0323,  0.0319,  ..., -0.0256, -0.0088,  0.0207],\n",
      "        [ 0.0201,  0.0023,  0.0093,  ...,  0.0626,  0.0062, -0.0321],\n",
      "        [-0.0914,  0.0309,  0.0147,  ...,  0.0579, -0.0396, -0.0403]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0049, -0.0096, -0.0066,  ...,  0.0491, -0.0264, -0.0346],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0122,  0.0608, -0.0027,  ...,  0.0203, -0.0338,  0.0457],\n",
      "        [ 0.0289,  0.0114, -0.0096,  ..., -0.0374, -0.0189, -0.0136],\n",
      "        [-0.0103, -0.0183, -0.0477,  ..., -0.0081, -0.0347, -0.0281],\n",
      "        ...,\n",
      "        [-0.0147,  0.0157,  0.0279,  ...,  0.0082, -0.0324, -0.0473],\n",
      "        [ 0.0353, -0.0005,  0.0154,  ...,  0.0122,  0.0047,  0.0086],\n",
      "        [-0.0450, -0.0233, -0.0048,  ..., -0.0071, -0.0160,  0.0023]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0062,  0.0012, -0.0136,  ...,  0.3013, -0.3738, -0.1541],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0265,  0.0720, -0.0201,  ...,  0.0699,  0.0011,  0.0528],\n",
      "        [-0.0587, -0.0204,  0.0190,  ..., -0.0018,  0.0773,  0.0157],\n",
      "        [ 0.0177,  0.0194,  0.0091,  ..., -0.0069,  0.0498, -0.0044],\n",
      "        ...,\n",
      "        [-0.0379, -0.0036,  0.0145,  ...,  0.0060,  0.1051, -0.0096],\n",
      "        [-0.0248, -0.0281, -0.0413,  ..., -0.0258,  0.0531, -0.0026],\n",
      "        [ 0.0165, -0.0026, -0.0071,  ..., -0.0066,  0.0845,  0.0223]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0002,  0.0005, -0.0061,  ...,  0.0033,  0.0012, -0.0031],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0372, -0.0150, -0.0244,  ...,  0.0236,  0.0070,  0.0066],\n",
      "        [-0.0266,  0.0467,  0.0246,  ..., -0.0226, -0.0163, -0.0169],\n",
      "        [-0.0602, -0.0039, -0.0182,  ...,  0.0543,  0.0224,  0.0058],\n",
      "        ...,\n",
      "        [-0.0457,  0.0402,  0.0213,  ..., -0.0386, -0.0240,  0.0717],\n",
      "        [-0.0527,  0.0886, -0.0053,  ...,  0.0240, -0.0670, -0.0102],\n",
      "        [ 0.0216, -0.0192,  0.0765,  ..., -0.0403, -0.0320,  0.0232]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0251,  0.0080, -0.0458,  ...,  0.0392,  0.0123, -0.0061],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-1.2238e-02, -3.5954e-03, -6.0005e-03,  ..., -3.1525e-02,\n",
      "          3.6469e-02,  4.9362e-03],\n",
      "        [-1.5411e-02, -8.2016e-03, -2.1912e-02,  ..., -1.9913e-02,\n",
      "         -5.8899e-03,  2.4529e-03],\n",
      "        [-1.2383e-02, -2.0065e-02, -2.7390e-02,  ...,  6.8092e-03,\n",
      "          2.9297e-02,  2.8551e-05],\n",
      "        ...,\n",
      "        [-4.4922e-02,  1.8433e-02, -3.2959e-02,  ...,  1.5240e-03,\n",
      "          2.3926e-02,  5.6343e-03],\n",
      "        [ 1.1047e-02,  3.3203e-02,  6.0516e-02,  ..., -1.3535e-02,\n",
      "          3.5889e-02, -9.2316e-03],\n",
      "        [-1.3132e-03, -1.1597e-02, -8.1711e-03,  ...,  1.9417e-03,\n",
      "         -5.9326e-02,  7.6416e-02]], dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0183, -0.0136, -0.0282,  ..., -0.0114, -0.0182, -0.0282],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0574,  0.0071, -0.0228,  ...,  0.0414, -0.0043, -0.0008],\n",
      "        [ 0.0381, -0.0108, -0.0459,  ..., -0.0550, -0.0040, -0.0205],\n",
      "        [-0.0170, -0.0360,  0.0370,  ..., -0.0113,  0.0172,  0.0019],\n",
      "        ...,\n",
      "        [-0.0141,  0.0202,  0.0041,  ..., -0.0136, -0.0111, -0.0057],\n",
      "        [ 0.0182,  0.0116, -0.0278,  ..., -0.0192, -0.0391,  0.0249],\n",
      "        [-0.0341, -0.0173, -0.0039,  ..., -0.0227,  0.0103,  0.0025]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0174, -0.0074,  0.0462,  ...,  0.0115, -0.0104, -0.0369],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5112, 0.5200, 0.4910,  ..., 0.4983, 0.5273, 0.5435],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0174, -0.0238,  0.1125,  ..., -0.0140, -0.0161, -0.0125],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-3.5362e-03,  3.7628e-02, -1.5434e-02,  ..., -4.9667e-03,\n",
      "          2.3708e-03, -2.5196e-03],\n",
      "        [ 1.8239e-05,  6.4888e-03,  3.1433e-02,  ..., -1.6373e-02,\n",
      "         -8.5907e-03,  1.0139e-02],\n",
      "        [ 6.6605e-03, -4.9438e-03, -1.0010e-02,  ..., -2.3285e-02,\n",
      "         -2.6260e-02,  3.1036e-02],\n",
      "        ...,\n",
      "        [ 1.4534e-02,  6.2286e-02, -4.1542e-03,  ..., -5.0079e-02,\n",
      "          1.2474e-02, -5.1971e-02],\n",
      "        [-5.9479e-02,  8.3130e-02,  3.7994e-02,  ..., -3.4241e-02,\n",
      "         -2.2280e-04,  4.3854e-02],\n",
      "        [-3.7098e-03, -2.5606e-04, -3.7476e-02,  ..., -4.0100e-02,\n",
      "          2.7039e-02,  5.3528e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0081,  0.0111,  0.0015,  ...,  0.0602,  0.0552,  0.0002],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-3.2501e-02,  5.5618e-03,  2.4384e-02,  ..., -2.3785e-03,\n",
      "          4.9805e-02, -2.8397e-02],\n",
      "        [ 2.6947e-02,  1.2444e-02, -1.2794e-02,  ...,  1.0483e-02,\n",
      "         -1.2306e-02, -8.1635e-03],\n",
      "        [-6.1913e-03,  3.3691e-02, -1.1566e-02,  ..., -3.0518e-02,\n",
      "         -1.0345e-02,  4.9408e-02],\n",
      "        ...,\n",
      "        [-2.1210e-02,  7.3624e-03,  2.5803e-02,  ..., -6.5796e-02,\n",
      "          4.5807e-02,  4.1771e-03],\n",
      "        [-6.4392e-03, -4.0497e-02,  2.0401e-02,  ..., -5.8098e-03,\n",
      "         -3.8269e-02, -2.9206e-06],\n",
      "        [ 1.9882e-02,  1.9974e-02, -1.4023e-02,  ...,  1.0918e-02,\n",
      "          2.0981e-02,  2.2964e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0171, -0.0193, -0.0359,  ...,  1.0225,  1.7412, -0.1588],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0101,  0.0012, -0.0172,  ..., -0.0163, -0.0220,  0.0165],\n",
      "        [-0.0236, -0.0359, -0.0171,  ..., -0.0308,  0.0012, -0.0117],\n",
      "        [-0.0119, -0.0075, -0.0283,  ..., -0.0246, -0.0271,  0.0099],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0647, -0.0211,  ..., -0.0720, -0.0521, -0.0236],\n",
      "        [-0.0024,  0.0515, -0.0072,  ...,  0.0538, -0.0190,  0.0065],\n",
      "        [-0.0701, -0.0672,  0.0079,  ...,  0.0135, -0.0362,  0.0164]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0019,  0.0002,  0.0006,  ..., -0.0002, -0.0029, -0.0025],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0125,  0.0208, -0.0118,  ..., -0.0144,  0.0014, -0.0889],\n",
      "        [-0.0085,  0.0327, -0.0061,  ..., -0.0614,  0.0216, -0.0225],\n",
      "        [ 0.0068,  0.0217,  0.0105,  ..., -0.0043, -0.0305, -0.0137],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0063, -0.0183,  ...,  0.0054,  0.0286,  0.0153],\n",
      "        [ 0.0139, -0.0217,  0.0013,  ..., -0.0692, -0.0239, -0.0330],\n",
      "        [-0.0233, -0.0043, -0.0421,  ..., -0.0425,  0.0417, -0.0278]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0240, -0.0002, -0.0646,  ...,  0.0443, -0.0088, -0.0147],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0229, -0.0006, -0.0050,  ..., -0.0068, -0.0342,  0.0052],\n",
      "        [-0.0294,  0.0275, -0.0597,  ...,  0.0424, -0.0277, -0.0117],\n",
      "        [ 0.0457, -0.0248,  0.0370,  ...,  0.0025, -0.0022, -0.0033],\n",
      "        ...,\n",
      "        [-0.0112,  0.0274,  0.0479,  ...,  0.0078,  0.0227,  0.0286],\n",
      "        [-0.0179, -0.0317,  0.0205,  ...,  0.0129, -0.0070,  0.0131],\n",
      "        [-0.0226,  0.0119, -0.0649,  ..., -0.0272,  0.0121,  0.0055]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0275, -0.0127,  0.0005,  ..., -0.0219,  0.0028,  0.0043],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0096, -0.0693, -0.0083,  ...,  0.0157,  0.0299,  0.0119],\n",
      "        [-0.0193, -0.0234,  0.0053,  ...,  0.0005, -0.0172,  0.0114],\n",
      "        [-0.0054,  0.0132, -0.0542,  ...,  0.0244, -0.0486,  0.0179],\n",
      "        ...,\n",
      "        [ 0.0522,  0.0112,  0.0360,  ..., -0.0272,  0.0146,  0.0347],\n",
      "        [-0.0248,  0.0284,  0.0024,  ..., -0.0300,  0.0452,  0.0205],\n",
      "        [ 0.1090,  0.0334,  0.0259,  ..., -0.0279,  0.0062,  0.0074]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0094,  0.0013,  0.0413,  ..., -0.0048, -0.0298, -0.0243],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5264, 0.5371, 0.5205,  ..., 0.4990, 0.5410, 0.5654],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0242, -0.0344,  0.1235,  ..., -0.0200, -0.0275, -0.0226],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 1.4629e-03, -1.0612e-02,  5.0926e-03,  ..., -1.6373e-02,\n",
      "         -1.2636e-05, -4.8256e-03],\n",
      "        [-1.7838e-02,  1.2436e-02, -1.1147e-02,  ..., -1.2772e-02,\n",
      "          1.6571e-02, -1.1269e-02],\n",
      "        [ 4.4891e-02, -4.8599e-03, -4.5990e-02,  ...,  3.1921e-02,\n",
      "          4.7333e-02,  1.1002e-02],\n",
      "        ...,\n",
      "        [-3.2715e-02, -1.5671e-02, -5.9998e-02,  ..., -1.9547e-02,\n",
      "         -5.6580e-02, -4.4678e-02],\n",
      "        [ 1.2917e-02,  5.5008e-03,  7.7477e-03,  ..., -3.5797e-02,\n",
      "          4.6349e-03,  4.0314e-02],\n",
      "        [-4.0779e-03,  5.6610e-02, -4.5929e-02,  ...,  9.2239e-03,\n",
      "          2.1103e-02,  7.2571e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0058,  0.0040, -0.0010,  ...,  0.0087,  0.0045, -0.0118],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0417,  0.0219,  0.0035,  ..., -0.0061,  0.0038,  0.0172],\n",
      "        [ 0.0103, -0.0117, -0.0030,  ...,  0.0012,  0.0275,  0.0369],\n",
      "        [-0.0036,  0.0028, -0.0221,  ..., -0.0063, -0.0230,  0.0018],\n",
      "        ...,\n",
      "        [ 0.0149,  0.0566, -0.0839,  ...,  0.0390, -0.0118,  0.0195],\n",
      "        [-0.0401,  0.0201,  0.0133,  ...,  0.0117,  0.0256, -0.0302],\n",
      "        [-0.0347, -0.0076, -0.0685,  ..., -0.0175,  0.0287,  0.0211]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0067, -0.0040, -0.0032,  ..., -0.0931,  0.0309,  0.0334],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0032,  0.0258, -0.0113,  ..., -0.0397,  0.0103, -0.0219],\n",
      "        [-0.0792, -0.0240, -0.0202,  ..., -0.0111, -0.0006, -0.0224],\n",
      "        [-0.0082, -0.0046,  0.0400,  ..., -0.0279, -0.0554,  0.0001],\n",
      "        ...,\n",
      "        [-0.0437, -0.0098,  0.0008,  ...,  0.0151, -0.0028,  0.0403],\n",
      "        [-0.0206,  0.0408,  0.0177,  ...,  0.0097, -0.0005,  0.0279],\n",
      "        [ 0.0168,  0.0384,  0.0237,  ..., -0.0312,  0.0097, -0.0384]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([2.3689e-03, 2.0466e-03, 4.8280e-06,  ..., 3.3684e-03, 4.2892e-04,\n",
      "        1.4715e-03], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0294,  0.0087,  0.0349,  ..., -0.0413, -0.0006,  0.0052],\n",
      "        [-0.0025,  0.0247,  0.0064,  ..., -0.0096,  0.0517,  0.0385],\n",
      "        [-0.0394,  0.0193, -0.0069,  ..., -0.0216, -0.0247,  0.0427],\n",
      "        ...,\n",
      "        [ 0.0505, -0.0189,  0.0159,  ...,  0.0453,  0.0230,  0.0290],\n",
      "        [-0.0306,  0.0041, -0.0085,  ...,  0.0902,  0.0037, -0.0006],\n",
      "        [-0.0534, -0.0031, -0.0246,  ..., -0.0318, -0.0355, -0.0262]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0190, -0.0038, -0.0720,  ...,  0.0460,  0.0177, -0.0032],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0439,  0.0187,  0.0040,  ..., -0.0144, -0.0550,  0.0727],\n",
      "        [-0.0172, -0.0117, -0.0600,  ...,  0.0159, -0.0216,  0.0079],\n",
      "        [-0.0078,  0.0340,  0.0008,  ..., -0.0206,  0.0040, -0.0034],\n",
      "        ...,\n",
      "        [-0.0114, -0.0032,  0.0012,  ..., -0.0041,  0.0101,  0.0208],\n",
      "        [ 0.0673,  0.0258,  0.0024,  ..., -0.0250,  0.0162,  0.0278],\n",
      "        [ 0.0367,  0.0191, -0.0144,  ..., -0.0600,  0.0282, -0.0334]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0359, -0.0172,  0.0053,  ..., -0.0074,  0.0099, -0.0054],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0206,  0.0336, -0.0087,  ..., -0.0065, -0.0366, -0.0212],\n",
      "        [ 0.0068,  0.0369, -0.0308,  ...,  0.0854, -0.0167, -0.0438],\n",
      "        [-0.0242, -0.0398,  0.0168,  ..., -0.0144, -0.0055, -0.0349],\n",
      "        ...,\n",
      "        [ 0.0687,  0.0525,  0.0430,  ..., -0.0194,  0.0500,  0.0097],\n",
      "        [-0.1074, -0.0263, -0.0312,  ..., -0.0276, -0.0166, -0.0486],\n",
      "        [ 0.0409, -0.0209, -0.0057,  ...,  0.0311, -0.0309,  0.0074]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0044, -0.0035,  0.0583,  ...,  0.0017,  0.0029, -0.0083],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5151, 0.5312, 0.5444,  ..., 0.4846, 0.5444, 0.5640],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0662, -0.0515,  0.1157,  ..., -0.0722, -0.1097, -0.1206],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0317,  0.0079,  0.0101,  ...,  0.0058,  0.0353, -0.0019],\n",
      "        [-0.0177, -0.0221,  0.0238,  ...,  0.0010, -0.0305,  0.0247],\n",
      "        [-0.0111,  0.0012, -0.0097,  ...,  0.0341,  0.0084, -0.0103],\n",
      "        ...,\n",
      "        [-0.0192,  0.0155, -0.0191,  ..., -0.0215, -0.0446,  0.0027],\n",
      "        [ 0.0186,  0.0283, -0.0251,  ...,  0.0081, -0.0740,  0.0163],\n",
      "        [ 0.0877, -0.0378,  0.0074,  ..., -0.0292, -0.0358, -0.0045]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0713,  0.0418,  0.0420,  ..., -0.0276, -0.1603, -0.0153],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0144, -0.0158,  0.0096,  ..., -0.0047, -0.0330, -0.0024],\n",
      "        [-0.0315, -0.0197,  0.0195,  ..., -0.0181,  0.0250, -0.0148],\n",
      "        [ 0.0234,  0.0269,  0.0310,  ..., -0.0035,  0.0091, -0.0061],\n",
      "        ...,\n",
      "        [-0.0127,  0.0108, -0.0256,  ..., -0.0033, -0.0325, -0.0337],\n",
      "        [ 0.0262, -0.0098,  0.0484,  ..., -0.0239,  0.0203, -0.0234],\n",
      "        [ 0.0570, -0.0105, -0.0079,  ..., -0.0483, -0.0092, -0.0544]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([0.0331, 0.0480, 0.0573,  ..., 0.0268, 0.8477, 0.1139],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-1.3527e-02,  3.3081e-02,  2.2369e-02,  ..., -1.8478e-02,\n",
      "          1.6663e-02, -1.1017e-02],\n",
      "        [-6.7368e-03,  7.1899e-02,  1.3437e-03,  ...,  5.6458e-02,\n",
      "          4.8459e-05, -5.3177e-03],\n",
      "        [-1.8967e-02,  3.6224e-02, -2.7054e-02,  ...,  2.2293e-02,\n",
      "         -3.8971e-02, -2.8290e-02],\n",
      "        ...,\n",
      "        [-4.9683e-02, -6.3133e-03,  2.4521e-02,  ..., -6.5155e-03,\n",
      "         -1.2726e-02, -4.8920e-02],\n",
      "        [ 1.6647e-02, -2.4689e-02, -8.6288e-03,  ..., -4.1931e-02,\n",
      "         -2.4338e-02, -1.4595e-02],\n",
      "        [ 4.0710e-02, -1.4465e-02, -1.2146e-02,  ..., -3.7781e-02,\n",
      "         -8.8120e-03,  3.3234e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0012,  0.0004,  0.0017,  ...,  0.0041, -0.0060,  0.0030],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0174,  0.0209, -0.0076,  ..., -0.0006, -0.0204,  0.0572],\n",
      "        [ 0.0375,  0.0564,  0.0197,  ...,  0.0077, -0.0317,  0.0043],\n",
      "        [ 0.0271, -0.0035, -0.0418,  ..., -0.0314,  0.0275, -0.0333],\n",
      "        ...,\n",
      "        [-0.0403,  0.0652, -0.0267,  ...,  0.0824,  0.0233,  0.0285],\n",
      "        [-0.0214, -0.0095, -0.0492,  ...,  0.0500,  0.0024, -0.0038],\n",
      "        [ 0.0284,  0.0352, -0.0092,  ..., -0.0461,  0.0393, -0.0235]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0036,  0.0023, -0.0674,  ...,  0.0191,  0.0141, -0.0195],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0237, -0.0313, -0.0393,  ..., -0.0371, -0.0153,  0.0124],\n",
      "        [ 0.0264, -0.0044, -0.0324,  ..., -0.0350,  0.0205, -0.0223],\n",
      "        [-0.0150,  0.0048, -0.0242,  ..., -0.0291, -0.0655, -0.0117],\n",
      "        ...,\n",
      "        [-0.0508, -0.0274, -0.0365,  ...,  0.0026, -0.0149, -0.0053],\n",
      "        [-0.0067, -0.0092, -0.0266,  ...,  0.0306,  0.0115, -0.0019],\n",
      "        [ 0.0157, -0.0047,  0.0041,  ..., -0.0131,  0.0357,  0.0224]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0168, -0.0161, -0.0093,  ..., -0.0259, -0.0116, -0.0055],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0063,  0.0208, -0.0186,  ...,  0.0016,  0.0347,  0.0169],\n",
      "        [ 0.0422,  0.0053,  0.0068,  ..., -0.0114,  0.0187, -0.0687],\n",
      "        [ 0.0064,  0.0111, -0.0217,  ...,  0.0294, -0.0188, -0.0258],\n",
      "        ...,\n",
      "        [ 0.0212, -0.0063,  0.0681,  ...,  0.0296,  0.0477,  0.0293],\n",
      "        [ 0.0343,  0.0200, -0.0256,  ...,  0.0101, -0.0083,  0.0182],\n",
      "        [ 0.0146,  0.0125,  0.0288,  ...,  0.0188, -0.0267,  0.0402]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0061,  0.0027,  0.0692,  ...,  0.0158,  0.0001, -0.0463],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5469, 0.5557, 0.5503,  ..., 0.4814, 0.5601, 0.5630],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0294, -0.0356,  0.1141,  ..., -0.0225, -0.0189, -0.0191],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0010, -0.0198, -0.0008,  ...,  0.0426,  0.0447, -0.0289],\n",
      "        [ 0.0244,  0.0555,  0.0629,  ...,  0.0300, -0.0259, -0.0200],\n",
      "        [-0.0220, -0.0096,  0.0357,  ...,  0.0530,  0.0090,  0.0867],\n",
      "        ...,\n",
      "        [-0.0223, -0.0010,  0.0166,  ...,  0.0579,  0.0376,  0.0416],\n",
      "        [ 0.0037, -0.0326, -0.0032,  ...,  0.0155,  0.0157,  0.0030],\n",
      "        [ 0.0118,  0.0808,  0.0376,  ..., -0.0399, -0.0023, -0.0076]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0022,  0.0116,  0.0150,  ...,  0.0171,  0.0032, -0.0004],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-3.2806e-02, -6.7078e-02,  1.2550e-02,  ..., -4.8035e-02,\n",
      "          8.4937e-05,  7.8857e-02],\n",
      "        [ 9.5520e-03, -1.3596e-02, -1.3824e-02,  ..., -5.2979e-02,\n",
      "         -2.0493e-02, -2.3392e-02],\n",
      "        [ 1.5587e-02, -3.7476e-02,  1.0099e-03,  ..., -1.1452e-02,\n",
      "         -1.2489e-02,  2.8107e-02],\n",
      "        ...,\n",
      "        [ 1.9714e-02,  2.5986e-02,  3.1490e-03,  ...,  2.9373e-02,\n",
      "          8.3008e-03, -4.7241e-02],\n",
      "        [ 5.4749e-02,  1.6418e-02, -1.7958e-03,  ..., -1.6586e-02,\n",
      "         -6.4278e-04,  1.0567e-03],\n",
      "        [ 5.3883e-05, -1.2657e-02, -1.9226e-02,  ..., -3.7720e-02,\n",
      "         -1.0498e-02,  5.2979e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0056, -0.0141, -0.0139,  ..., -0.2081,  0.1104, -0.0789],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0291, -0.0045,  0.0081,  ...,  0.0028,  0.0117,  0.0300],\n",
      "        [ 0.0058,  0.0417,  0.0259,  ..., -0.0196, -0.0193, -0.0307],\n",
      "        [-0.0807,  0.0486,  0.0160,  ...,  0.0092,  0.0100,  0.0613],\n",
      "        ...,\n",
      "        [ 0.0211,  0.0041, -0.0106,  ..., -0.0251,  0.0257, -0.0369],\n",
      "        [ 0.0110,  0.0461,  0.0143,  ..., -0.0021,  0.0492,  0.0267],\n",
      "        [ 0.0027,  0.0511, -0.0016,  ..., -0.0443, -0.0215,  0.0028]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0015,  0.0005,  0.0006,  ..., -0.0014, -0.0008, -0.0014],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0229, -0.0269, -0.0414,  ..., -0.0219, -0.0231,  0.0406],\n",
      "        [-0.0228,  0.0080,  0.0798,  ..., -0.0345, -0.0020,  0.0324],\n",
      "        [-0.0259,  0.0228, -0.0011,  ..., -0.0054, -0.0002,  0.0021],\n",
      "        ...,\n",
      "        [-0.0309, -0.0162, -0.0152,  ..., -0.0229, -0.0133,  0.1040],\n",
      "        [ 0.0269, -0.0119,  0.0510,  ..., -0.0264,  0.0013, -0.0301],\n",
      "        [ 0.0398, -0.0378,  0.0366,  ..., -0.0652, -0.0022, -0.0217]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 1.0078e-02, -6.9916e-05, -9.8938e-02,  ...,  4.0588e-02,\n",
      "        -1.1673e-02,  1.2074e-03], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0569, -0.0088, -0.0117,  ...,  0.0418,  0.0018, -0.0015],\n",
      "        [-0.0053, -0.0044, -0.0014,  ...,  0.0310, -0.0442,  0.0211],\n",
      "        [-0.0325,  0.0586,  0.0163,  ...,  0.0119, -0.0260, -0.0126],\n",
      "        ...,\n",
      "        [-0.0179, -0.0495,  0.0263,  ..., -0.0391, -0.0456,  0.0195],\n",
      "        [-0.0138, -0.0241, -0.0115,  ..., -0.0484, -0.0305,  0.0150],\n",
      "        [-0.0457,  0.0068, -0.0345,  ..., -0.0706, -0.0049,  0.0389]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-2.3193e-02, -3.4760e-02, -1.7319e-02,  ..., -1.6663e-02,\n",
      "        -7.5877e-05, -3.5034e-02], dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0276, -0.0192, -0.0121,  ...,  0.0163, -0.0174, -0.0525],\n",
      "        [ 0.0229,  0.0020,  0.0107,  ...,  0.0409, -0.0141,  0.0606],\n",
      "        [ 0.0161,  0.0048,  0.0069,  ...,  0.0060,  0.0235,  0.0096],\n",
      "        ...,\n",
      "        [-0.0687, -0.0227, -0.0115,  ...,  0.0562, -0.0132, -0.0340],\n",
      "        [ 0.0670,  0.0116, -0.0235,  ...,  0.0423, -0.0404,  0.0076],\n",
      "        [-0.0131, -0.0136,  0.0299,  ..., -0.0051,  0.0305,  0.0068]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0195, -0.0298,  0.1044,  ...,  0.0280,  0.0282, -0.0365],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5537, 0.5615, 0.5767,  ..., 0.4548, 0.5659, 0.5591],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0189, -0.0367,  0.1530,  ..., -0.0134, -0.0060, -0.0136],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0633,  0.0131, -0.0549,  ..., -0.0157, -0.1014,  0.0273],\n",
      "        [-0.0199,  0.0256, -0.0076,  ..., -0.0067, -0.0077, -0.0453],\n",
      "        [ 0.0261, -0.0100,  0.0077,  ..., -0.0201,  0.0441, -0.0652],\n",
      "        ...,\n",
      "        [ 0.0916,  0.1030, -0.1277,  ..., -0.6021, -0.0377, -0.3042],\n",
      "        [ 0.0161, -0.0195, -0.0210,  ..., -0.2876, -0.0189, -0.1511],\n",
      "        [ 0.0383,  0.0265, -0.1166,  ..., -0.4502, -0.0016, -0.2639]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0280, -0.0242,  0.0195,  ..., -0.0707, -0.0325, -0.0608],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0080, -0.0151, -0.0432,  ...,  0.0103,  0.0529, -0.0362],\n",
      "        [-0.0276,  0.0426, -0.0561,  ...,  0.0019, -0.0135, -0.0223],\n",
      "        [-0.0170,  0.0158,  0.0380,  ..., -0.0231,  0.0227, -0.0163],\n",
      "        ...,\n",
      "        [ 0.0545, -0.0013,  0.0456,  ...,  0.0211, -0.0068,  0.0471],\n",
      "        [ 0.0646, -0.0506,  0.0051,  ..., -0.0054,  0.0385,  0.0482],\n",
      "        [ 0.0024,  0.0404,  0.1047,  ...,  0.0080,  0.0118,  0.0088]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 1.0290e-03, -7.3791e-05,  6.2675e-03,  ..., -1.3330e+00,\n",
      "        -6.2158e-01, -1.0225e+00], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0145, -0.0356, -0.0048,  ...,  0.0054,  0.0896, -0.0629],\n",
      "        [-0.0284, -0.0100,  0.0331,  ...,  0.0323, -0.0100,  0.0021],\n",
      "        [-0.0257, -0.0425, -0.0032,  ...,  0.0066,  0.0269, -0.0075],\n",
      "        ...,\n",
      "        [-0.0098,  0.0006, -0.0055,  ...,  0.0171, -0.0191,  0.0372],\n",
      "        [-0.0114,  0.0230, -0.0315,  ..., -0.0090,  0.0349, -0.0068],\n",
      "        [-0.0422, -0.0022,  0.0070,  ..., -0.0100,  0.0188, -0.0164]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-6.0272e-04, -7.2181e-05,  8.9741e-04,  ..., -2.7204e-04,\n",
      "         3.2783e-04, -3.7074e-05], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0345, -0.0639, -0.0070,  ..., -0.0090, -0.0172, -0.0261],\n",
      "        [ 0.0103,  0.0162, -0.0003,  ...,  0.0105,  0.0204,  0.0510],\n",
      "        [ 0.0063,  0.0378, -0.0050,  ..., -0.0151,  0.0086,  0.0211],\n",
      "        ...,\n",
      "        [-0.0038, -0.0643, -0.0189,  ..., -0.0024, -0.0765, -0.0254],\n",
      "        [-0.0432, -0.0269,  0.0546,  ..., -0.0131, -0.0093,  0.0041],\n",
      "        [-0.0154, -0.0186, -0.0042,  ..., -0.0007, -0.0173,  0.0192]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0092, -0.0228, -0.0704,  ...,  0.0194, -0.0532, -0.0037],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0117, -0.0567, -0.0193,  ...,  0.0176,  0.0312,  0.0445],\n",
      "        [-0.0220, -0.0805,  0.0071,  ...,  0.0081,  0.0282,  0.0308],\n",
      "        [-0.0024,  0.0326, -0.0049,  ...,  0.0052,  0.0103,  0.0068],\n",
      "        ...,\n",
      "        [-0.0552,  0.0044, -0.0049,  ...,  0.0075, -0.0185, -0.0148],\n",
      "        [ 0.0090,  0.0552, -0.0261,  ..., -0.0155,  0.0136, -0.0097],\n",
      "        [-0.0442, -0.0176, -0.0124,  ..., -0.0130, -0.0268, -0.0453]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0095, -0.0101, -0.0110,  ..., -0.0061,  0.0060, -0.0043],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0089, -0.0049,  0.0100,  ...,  0.0153,  0.0264,  0.0529],\n",
      "        [-0.0041,  0.0591, -0.0072,  ..., -0.0280, -0.0632,  0.0107],\n",
      "        [ 0.0269,  0.0070, -0.0302,  ...,  0.0035,  0.0410, -0.0295],\n",
      "        ...,\n",
      "        [-0.0202, -0.0877, -0.0290,  ..., -0.0009, -0.0242, -0.0204],\n",
      "        [ 0.0030,  0.0550,  0.0116,  ...,  0.0231, -0.0568,  0.0173],\n",
      "        [ 0.0050, -0.0084, -0.0018,  ..., -0.0072, -0.0452, -0.0236]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0322,  0.0006,  0.0816,  ..., -0.0012,  0.0018, -0.0859],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5171, 0.4604, 0.5869,  ..., 1.2354, 0.5566, 0.6953],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.5830, -0.7422, -0.3333,  ..., -0.4463, -0.5610, -0.5410],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 4.4098e-02,  7.6675e-03,  1.1475e-02,  ..., -2.4170e-02,\n",
      "         -2.3592e-04, -3.2623e-02],\n",
      "        [ 5.6854e-02, -1.2856e-02,  5.4749e-02,  ...,  1.1414e-02,\n",
      "          5.5817e-02, -3.2349e-02],\n",
      "        [-7.9163e-02,  1.6388e-02, -1.6815e-02,  ...,  1.7319e-02,\n",
      "          2.3148e-02,  8.4900e-02],\n",
      "        ...,\n",
      "        [-5.2155e-02,  8.4290e-02, -5.2948e-02,  ...,  5.0732e-01,\n",
      "         -2.7954e-02,  2.4609e-01],\n",
      "        [ 2.8702e-02, -5.1453e-02,  4.6692e-02,  ..., -1.2079e-01,\n",
      "          5.4413e-02, -1.6602e-01],\n",
      "        [-2.0233e-02,  1.4572e-03, -5.8746e-02,  ...,  4.0112e-01,\n",
      "         -2.1515e-02,  2.0544e-01]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0154,  0.0055, -0.0118,  ...,  0.0761, -0.0358,  0.0478],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 4.2381e-03, -6.3667e-03,  4.0627e-03,  ...,  1.1551e-02,\n",
      "          1.9684e-02,  1.3411e-05],\n",
      "        [ 1.8005e-02,  6.1859e-02, -1.4992e-02,  ..., -3.5339e-02,\n",
      "         -2.1957e-02, -9.3384e-03],\n",
      "        [-4.1199e-02,  2.2751e-02,  6.4611e-04,  ...,  5.4169e-03,\n",
      "          4.0649e-02, -1.4961e-02],\n",
      "        ...,\n",
      "        [-2.7664e-02, -1.3275e-02, -3.9001e-02,  ..., -3.2463e-03,\n",
      "          2.2141e-02, -4.1595e-02],\n",
      "        [ 3.5000e-03,  1.3252e-02,  1.0864e-01,  ..., -6.9962e-03,\n",
      "          2.9434e-02,  4.0924e-02],\n",
      "        [-7.1373e-03, -4.6844e-02, -3.5034e-02,  ..., -1.7029e-02,\n",
      "         -4.0070e-02, -1.0582e-02]], dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-2.8095e-03, -6.0730e-03, -4.0770e-04,  ...,  1.8555e+00,\n",
      "        -6.6504e-01,  1.3516e+00], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0093, -0.0233, -0.0020,  ..., -0.0065,  0.0262,  0.0035],\n",
      "        [-0.0209,  0.0474,  0.0209,  ...,  0.0018, -0.0418, -0.0141],\n",
      "        [ 0.0270, -0.0242, -0.0199,  ...,  0.0090, -0.0348,  0.0153],\n",
      "        ...,\n",
      "        [-0.0012, -0.0085,  0.0139,  ..., -0.0055,  0.0232,  0.0127],\n",
      "        [ 0.0160,  0.0041, -0.0262,  ..., -0.0101, -0.0200,  0.0067],\n",
      "        [-0.0114,  0.0207,  0.0103,  ..., -0.0006,  0.0285,  0.0234]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 2.4605e-04, -5.7638e-05,  7.7200e-04,  ..., -1.9443e-04,\n",
      "        -4.0102e-04, -5.0354e-04], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0211, -0.0038,  0.0162,  ..., -0.0005,  0.0116,  0.0099],\n",
      "        [-0.0111,  0.0039, -0.0285,  ..., -0.0184, -0.0134, -0.0205],\n",
      "        [-0.0028,  0.0366,  0.0042,  ...,  0.0055, -0.0125,  0.0254],\n",
      "        ...,\n",
      "        [-0.0015, -0.0151,  0.0089,  ...,  0.0034, -0.0687, -0.0463],\n",
      "        [-0.0146,  0.0082, -0.0142,  ..., -0.0038, -0.0067,  0.0255],\n",
      "        [ 0.0212, -0.0325, -0.0026,  ...,  0.0070, -0.0133,  0.0099]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0007, -0.0060, -0.0281,  ...,  0.0326, -0.0261,  0.0015],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0652, -0.0258, -0.0278,  ..., -0.0052,  0.0308, -0.0095],\n",
      "        [-0.0523, -0.0300,  0.0227,  ...,  0.0139,  0.0078,  0.0058],\n",
      "        [-0.0228, -0.0143, -0.0321,  ..., -0.0046,  0.0115, -0.0121],\n",
      "        ...,\n",
      "        [ 0.0350, -0.0071,  0.0371,  ..., -0.0020,  0.0339,  0.0225],\n",
      "        [-0.0437,  0.0081, -0.0205,  ..., -0.0129,  0.0054,  0.0120],\n",
      "        [-0.0036, -0.0159, -0.0026,  ..., -0.0211, -0.0083,  0.0128]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([-0.0041, -0.0025, -0.0058,  ...,  0.0047,  0.0010,  0.0008],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[-0.0240,  0.0597,  0.0398,  ..., -0.0717,  0.0172, -0.0242],\n",
      "        [-0.0149,  0.0197, -0.0657,  ...,  0.0134,  0.0112,  0.0334],\n",
      "        [ 0.0206,  0.0190, -0.0232,  ..., -0.0177,  0.0155, -0.0641],\n",
      "        ...,\n",
      "        [-0.0348, -0.0967, -0.0206,  ...,  0.0283, -0.0143, -0.0281],\n",
      "        [-0.0490, -0.0558,  0.0017,  ...,  0.0178, -0.0163,  0.0398],\n",
      "        [-0.0096,  0.0133, -0.0162,  ...,  0.0283, -0.0757, -0.0625]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0250, -0.0088,  0.0503,  ...,  0.0091,  0.0423, -0.0527],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.5039, 0.4917, 0.5337,  ..., 1.0439, 0.4871, 0.5776],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.1458, -0.1716, -0.0774,  ..., -0.2529, -0.1372, -0.1197],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0172,  0.0003,  0.0192,  ...,  0.0044,  0.0064, -0.0092],\n",
      "        [ 0.0299, -0.0183, -0.0450,  ...,  0.0274, -0.0311,  0.0124],\n",
      "        [-0.0238, -0.0221, -0.0004,  ..., -0.0108, -0.0384,  0.0001],\n",
      "        ...,\n",
      "        [-0.0216,  0.0149,  0.0361,  ...,  0.0007, -0.0059,  0.0823],\n",
      "        [ 0.0216, -0.0143, -0.0320,  ...,  0.0022,  0.0065, -0.0695],\n",
      "        [ 0.0214, -0.0143, -0.0369,  ..., -0.0010,  0.0047, -0.0858]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0589, -0.0167,  0.0082,  ..., -0.0707,  0.0670,  0.0690],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[ 0.0410, -0.0031,  0.0098,  ...,  0.0614,  0.0077, -0.0218],\n",
      "        [-0.0031, -0.0116, -0.0077,  ..., -0.0034, -0.0228, -0.0161],\n",
      "        [-0.0089,  0.0221, -0.0568,  ..., -0.0236, -0.0119, -0.0087],\n",
      "        ...,\n",
      "        [ 0.0110,  0.0320,  0.0060,  ...,  0.0142,  0.0077,  0.0110],\n",
      "        [-0.0094, -0.0291, -0.0085,  ..., -0.0113, -0.0069, -0.0093],\n",
      "        [-0.0115, -0.0323, -0.0050,  ..., -0.0155, -0.0082, -0.0113]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([-0.0376, -0.0465, -0.0056,  ..., -0.3926,  0.3521,  0.4011],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0069, -0.0009,  0.0060,  ...,  0.0196, -0.0077, -0.0009],\n",
      "        [ 0.0003, -0.0011, -0.0115,  ...,  0.0067,  0.0010, -0.0037],\n",
      "        [ 0.0109, -0.0073,  0.0088,  ...,  0.0011,  0.0180, -0.0017],\n",
      "        ...,\n",
      "        [ 0.0194,  0.0114, -0.0041,  ...,  0.0046,  0.0002,  0.0151],\n",
      "        [ 0.0044,  0.0090, -0.0322,  ...,  0.0012,  0.0003, -0.0049],\n",
      "        [ 0.0142,  0.0109, -0.0032,  ..., -0.0142,  0.0095, -0.0222]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 2.3823e-03, -2.6093e-03, -2.4376e-03,  ...,  2.3663e-05,\n",
      "        -9.3317e-04, -4.5300e-06], dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0097,  0.0008,  0.0088,  ..., -0.0186, -0.0112,  0.0226],\n",
      "        [-0.0119, -0.0012,  0.0019,  ..., -0.0188, -0.0213, -0.0042],\n",
      "        [-0.0095, -0.0304,  0.0102,  ..., -0.0149,  0.0051,  0.0159],\n",
      "        ...,\n",
      "        [ 0.0251,  0.0119, -0.0174,  ..., -0.0041, -0.0237,  0.0158],\n",
      "        [ 0.0149,  0.0131,  0.0024,  ..., -0.0024,  0.0196, -0.0037],\n",
      "        [ 0.0002, -0.0195,  0.0143,  ..., -0.0047, -0.0104,  0.0023]],\n",
      "       dtype=torch.float16, requires_grad=True) 6553600\n",
      "Parameter containing:\n",
      "tensor([ 0.0032,  0.0005, -0.0264,  ...,  0.0139, -0.0339, -0.0049],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([[-0.0062,  0.0341, -0.0020,  ..., -0.0110, -0.0137, -0.0158],\n",
      "        [-0.0196,  0.0705, -0.0504,  ...,  0.0016, -0.0251, -0.0573],\n",
      "        [-0.0016,  0.0673,  0.0316,  ..., -0.0064, -0.0288, -0.0021],\n",
      "        ...,\n",
      "        [ 0.0069,  0.0278, -0.0008,  ..., -0.0421,  0.0684, -0.0375],\n",
      "        [-0.0017,  0.0251,  0.0377,  ..., -0.0186, -0.0029,  0.0142],\n",
      "        [ 0.0399, -0.0240, -0.0155,  ..., -0.0457, -0.0343, -0.0368]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0171, -0.0107, -0.0218,  ...,  0.0016,  0.0017,  0.0028],\n",
      "       dtype=torch.float16, requires_grad=True) 10240\n",
      "Parameter containing:\n",
      "tensor([[ 0.0005,  0.0091, -0.0022,  ..., -0.0175,  0.0123,  0.0075],\n",
      "        [ 0.0366,  0.0070, -0.0120,  ...,  0.0115,  0.0049,  0.0275],\n",
      "        [-0.0143,  0.0311, -0.0094,  ..., -0.0027, -0.0157, -0.0320],\n",
      "        ...,\n",
      "        [ 0.0946, -0.0398, -0.0820,  ...,  0.0572,  0.0152, -0.0062],\n",
      "        [ 0.0170, -0.0041, -0.0041,  ..., -0.0161,  0.0249, -0.0059],\n",
      "        [ 0.0809, -0.0588, -0.0051,  ...,  0.0285, -0.0617, -0.0073]],\n",
      "       dtype=torch.float16, requires_grad=True) 26214400\n",
      "Parameter containing:\n",
      "tensor([ 0.0009, -0.0168,  0.0076,  ..., -0.0131,  0.0164, -0.0091],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([0.4692, 0.4309, 0.4402,  ..., 0.4702, 0.4026, 0.5635],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.1868, -0.2676, -0.1121,  ..., -0.2856, -0.0979, -0.2125],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([1.3301, 1.4346, 1.2207,  ..., 0.7881, 1.1143, 1.1895],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n",
      "Parameter containing:\n",
      "tensor([-0.0117,  0.0499, -0.0923,  ...,  0.1569, -0.0720, -0.0215],\n",
      "       dtype=torch.float16, requires_grad=True) 2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0457,  0.0233,  0.0140,  ...,  0.0389,  0.0362,  0.0141],\n",
      "        [ 0.0039,  0.0307, -0.0015,  ...,  0.0143,  0.0643,  0.0245],\n",
      "        [ 0.0399,  0.0514, -0.0778,  ...,  0.0217, -0.0256,  0.0157],\n",
      "        ...,\n",
      "        [-0.0001, -0.0006, -0.0073,  ...,  0.0007, -0.0113, -0.0007],\n",
      "        [-0.0001, -0.0006, -0.0073,  ...,  0.0008, -0.0112, -0.0006],\n",
      "        [-0.0001, -0.0006, -0.0073,  ...,  0.0008, -0.0112, -0.0005]],\n",
      "       dtype=torch.float16, requires_grad=True) 131072000\n",
      "Parameter containing:\n",
      "tensor([ 0.0233,  0.1755,  0.0953,  ..., -0.1032, -0.1032, -0.1032],\n",
      "       dtype=torch.float16, requires_grad=True) 51200\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p,p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TheBloke/Llama-2-7B-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.3.self_attn.o_proj.qweight', 'model.layers.13.self_attn.v_proj.scales', 'model.layers.24.self_attn.o_proj.scales', 'model.layers.24.self_attn.k_proj.qzeros', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.8.self_attn.o_proj.scales', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.scales', 'model.layers.22.mlp.up_proj.bias', 'model.layers.26.mlp.gate_proj.scales', 'model.layers.15.self_attn.o_proj.qweight', 'model.layers.17.mlp.gate_proj.g_idx', 'model.layers.24.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.k_proj.scales', 'model.layers.13.mlp.gate_proj.scales', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.g_idx', 'model.layers.0.mlp.down_proj.qzeros', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.qweight', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.29.mlp.up_proj.g_idx', 'model.layers.24.mlp.gate_proj.g_idx', 'model.layers.24.self_attn.k_proj.qweight', 'model.layers.28.self_attn.q_proj.scales', 'model.layers.23.self_attn.k_proj.qweight', 'model.layers.29.self_attn.v_proj.scales', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.qzeros', 'model.layers.1.self_attn.q_proj.scales', 'model.layers.18.mlp.up_proj.g_idx', 'model.layers.17.mlp.gate_proj.qweight', 'model.layers.1.mlp.down_proj.scales', 'model.layers.10.mlp.gate_proj.qweight', 'model.layers.22.mlp.up_proj.qweight', 'model.layers.25.mlp.up_proj.scales', 'model.layers.28.mlp.up_proj.g_idx', 'model.layers.3.self_attn.o_proj.scales', 'model.layers.4.mlp.up_proj.g_idx', 'model.layers.10.mlp.down_proj.g_idx', 'model.layers.4.self_attn.k_proj.qzeros', 'model.layers.9.mlp.down_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.0.mlp.up_proj.qweight', 'model.layers.25.mlp.down_proj.scales', 'model.layers.26.self_attn.o_proj.scales', 'model.layers.9.mlp.gate_proj.scales', 'model.layers.19.self_attn.o_proj.scales', 'model.layers.10.self_attn.v_proj.qzeros', 'model.layers.12.self_attn.q_proj.qzeros', 'model.layers.11.mlp.up_proj.g_idx', 'model.layers.24.self_attn.v_proj.scales', 'model.layers.5.self_attn.k_proj.g_idx', 'model.layers.22.self_attn.o_proj.qweight', 'model.layers.18.self_attn.q_proj.qweight', 'model.layers.11.mlp.up_proj.qzeros', 'model.layers.10.mlp.up_proj.qzeros', 'model.layers.4.self_attn.o_proj.qzeros', 'model.layers.17.self_attn.v_proj.scales', 'model.layers.5.self_attn.q_proj.qzeros', 'model.layers.8.self_attn.k_proj.qzeros', 'model.layers.30.mlp.gate_proj.qweight', 'model.layers.2.mlp.down_proj.bias', 'model.layers.9.self_attn.v_proj.g_idx', 'model.layers.18.self_attn.v_proj.qzeros', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.scales', 'model.layers.31.mlp.down_proj.qweight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.g_idx', 'model.layers.14.mlp.gate_proj.qzeros', 'model.layers.12.mlp.down_proj.g_idx', 'model.layers.27.self_attn.o_proj.qzeros', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.qweight', 'model.layers.30.self_attn.k_proj.scales', 'model.layers.12.self_attn.q_proj.scales', 'model.layers.19.self_attn.v_proj.qzeros', 'model.layers.13.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.scales', 'model.layers.19.mlp.down_proj.qzeros', 'model.layers.16.mlp.down_proj.qweight', 'model.layers.26.self_attn.v_proj.qweight', 'model.layers.27.self_attn.q_proj.g_idx', 'model.layers.17.mlp.down_proj.qzeros', 'model.layers.29.mlp.down_proj.qzeros', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.13.self_attn.v_proj.qweight', 'model.layers.31.mlp.gate_proj.scales', 'model.layers.18.self_attn.o_proj.qweight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.gate_proj.qzeros', 'model.layers.30.mlp.up_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.17.self_attn.o_proj.scales', 'model.layers.23.mlp.gate_proj.g_idx', 'model.layers.27.mlp.down_proj.scales', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.27.mlp.up_proj.scales', 'model.layers.29.mlp.up_proj.bias', 'model.layers.14.self_attn.q_proj.qzeros', 'model.layers.5.self_attn.q_proj.g_idx', 'model.layers.15.self_attn.q_proj.g_idx', 'model.layers.6.self_attn.q_proj.qzeros', 'model.layers.22.self_attn.o_proj.qzeros', 'model.layers.25.self_attn.o_proj.qweight', 'model.layers.11.mlp.down_proj.qweight', 'model.layers.12.self_attn.v_proj.scales', 'model.layers.2.mlp.down_proj.qzeros', 'model.layers.23.mlp.up_proj.qweight', 'model.layers.8.self_attn.k_proj.g_idx', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.g_idx', 'model.layers.23.mlp.down_proj.g_idx', 'model.layers.27.self_attn.v_proj.g_idx', 'model.layers.30.mlp.up_proj.g_idx', 'model.layers.30.self_attn.v_proj.qweight', 'model.layers.23.mlp.gate_proj.qzeros', 'model.layers.21.mlp.down_proj.scales', 'model.layers.8.self_attn.o_proj.qweight', 'model.layers.27.mlp.up_proj.g_idx', 'model.layers.2.self_attn.v_proj.qzeros', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.1.mlp.up_proj.g_idx', 'model.layers.20.self_attn.k_proj.qweight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.mlp.down_proj.scales', 'model.layers.5.mlp.down_proj.qweight', 'model.layers.7.self_attn.k_proj.g_idx', 'model.layers.23.self_attn.o_proj.qweight', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.qweight', 'model.layers.15.self_attn.q_proj.qweight', 'model.layers.2.mlp.up_proj.qzeros', 'model.layers.2.self_attn.o_proj.g_idx', 'model.layers.20.mlp.down_proj.qzeros', 'model.layers.20.self_attn.v_proj.qzeros', 'model.layers.31.mlp.gate_proj.qweight', 'model.layers.7.mlp.down_proj.qweight', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.qweight', 'model.layers.28.mlp.gate_proj.qweight', 'model.layers.6.self_attn.v_proj.scales', 'model.layers.19.mlp.down_proj.bias', 'model.layers.24.mlp.down_proj.qzeros', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.11.mlp.up_proj.scales', 'model.layers.15.mlp.down_proj.qweight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.qzeros', 'model.layers.3.self_attn.q_proj.scales', 'model.layers.20.mlp.up_proj.scales', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.qweight', 'model.layers.11.self_attn.o_proj.scales', 'model.layers.18.self_attn.o_proj.g_idx', 'model.layers.22.self_attn.k_proj.g_idx', 'model.layers.0.mlp.down_proj.scales', 'model.layers.6.mlp.gate_proj.qweight', 'model.layers.17.mlp.down_proj.g_idx', 'model.layers.10.mlp.up_proj.qweight', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.30.mlp.up_proj.scales', 'model.layers.8.self_attn.v_proj.qzeros', 'model.layers.10.self_attn.o_proj.qzeros', 'model.layers.10.self_attn.q_proj.g_idx', 'model.layers.13.mlp.down_proj.qzeros', 'model.layers.17.self_attn.k_proj.qweight', 'model.layers.24.self_attn.q_proj.g_idx', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.29.mlp.down_proj.scales', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.scales', 'model.layers.16.self_attn.o_proj.qzeros', 'model.layers.27.mlp.up_proj.qzeros', 'model.layers.26.mlp.up_proj.g_idx', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.g_idx', 'model.layers.16.self_attn.o_proj.qweight', 'model.layers.20.mlp.up_proj.g_idx', 'model.layers.3.mlp.up_proj.qzeros', 'model.layers.17.mlp.gate_proj.qzeros', 'model.layers.0.self_attn.q_proj.qzeros', 'model.layers.29.mlp.down_proj.g_idx', 'model.layers.2.mlp.down_proj.qweight', 'model.layers.2.self_attn.q_proj.g_idx', 'model.layers.5.self_attn.k_proj.qzeros', 'model.layers.31.self_attn.k_proj.g_idx', 'model.layers.7.self_attn.q_proj.scales', 'model.layers.28.self_attn.o_proj.g_idx', 'model.layers.22.mlp.down_proj.qweight', 'model.layers.9.self_attn.q_proj.scales', 'model.layers.13.mlp.down_proj.g_idx', 'model.layers.5.self_attn.v_proj.g_idx', 'model.layers.20.mlp.down_proj.scales', 'model.layers.0.mlp.gate_proj.scales', 'model.layers.27.self_attn.k_proj.g_idx', 'model.layers.7.self_attn.v_proj.qweight', 'model.layers.28.mlp.gate_proj.scales', 'model.layers.0.self_attn.o_proj.g_idx', 'model.layers.20.mlp.gate_proj.qzeros', 'model.layers.25.mlp.gate_proj.qzeros', 'model.layers.9.mlp.down_proj.qzeros', 'model.layers.26.mlp.up_proj.scales', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.scales', 'model.layers.26.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.qzeros', 'model.layers.8.mlp.down_proj.bias', 'model.layers.22.self_attn.q_proj.qzeros', 'model.layers.4.self_attn.v_proj.qweight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.g_idx', 'model.layers.16.self_attn.k_proj.g_idx', 'model.layers.10.self_attn.q_proj.qweight', 'model.layers.12.mlp.up_proj.qzeros', 'model.layers.15.mlp.up_proj.qweight', 'model.layers.12.self_attn.v_proj.qweight', 'model.layers.19.self_attn.q_proj.g_idx', 'model.layers.23.self_attn.o_proj.scales', 'model.layers.24.self_attn.q_proj.qweight', 'model.layers.26.self_attn.v_proj.qzeros', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.qzeros', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.g_idx', 'model.layers.0.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.qzeros', 'model.layers.23.self_attn.v_proj.scales', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.10.self_attn.o_proj.g_idx', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.21.mlp.gate_proj.scales', 'model.layers.5.self_attn.v_proj.scales', 'model.layers.22.self_attn.k_proj.scales', 'model.layers.29.self_attn.v_proj.g_idx', 'model.layers.4.mlp.gate_proj.qzeros', 'model.layers.8.self_attn.o_proj.g_idx', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.qweight', 'model.layers.29.self_attn.v_proj.qzeros', 'model.layers.3.self_attn.v_proj.scales', 'model.layers.3.self_attn.k_proj.qweight', 'model.layers.6.mlp.down_proj.g_idx', 'model.layers.1.mlp.up_proj.qzeros', 'model.layers.23.self_attn.v_proj.qweight', 'model.layers.18.self_attn.q_proj.scales', 'model.layers.28.self_attn.k_proj.g_idx', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.15.mlp.gate_proj.qweight', 'model.layers.25.mlp.gate_proj.qweight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.5.mlp.gate_proj.scales', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.17.mlp.gate_proj.scales', 'model.layers.31.self_attn.k_proj.qweight', 'model.layers.8.mlp.up_proj.qzeros', 'model.layers.24.self_attn.q_proj.scales', 'model.layers.4.self_attn.q_proj.g_idx', 'model.layers.18.mlp.down_proj.g_idx', 'model.layers.2.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.qweight', 'model.layers.17.self_attn.v_proj.qweight', 'model.layers.18.self_attn.v_proj.qweight', 'model.layers.17.mlp.up_proj.bias', 'model.layers.7.mlp.gate_proj.qweight', 'model.layers.18.mlp.down_proj.scales', 'model.layers.30.mlp.down_proj.qzeros', 'model.layers.11.self_attn.o_proj.g_idx', 'model.layers.8.mlp.gate_proj.scales', 'model.layers.11.mlp.gate_proj.qzeros', 'model.layers.22.mlp.down_proj.g_idx', 'model.layers.15.mlp.up_proj.scales', 'model.layers.9.self_attn.k_proj.qzeros', 'model.layers.11.mlp.down_proj.qzeros', 'model.layers.0.self_attn.v_proj.qzeros', 'model.layers.17.mlp.down_proj.bias', 'model.layers.15.self_attn.k_proj.qzeros', 'model.layers.19.self_attn.v_proj.g_idx', 'model.layers.23.mlp.down_proj.scales', 'model.layers.13.mlp.up_proj.qzeros', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.16.self_attn.k_proj.qzeros', 'model.layers.18.self_attn.q_proj.g_idx', 'model.layers.30.self_attn.q_proj.qweight', 'model.layers.25.self_attn.o_proj.qzeros', 'model.layers.6.mlp.up_proj.g_idx', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.1.mlp.down_proj.qzeros', 'model.layers.3.mlp.up_proj.g_idx', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.17.self_attn.v_proj.g_idx', 'model.layers.29.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.qweight', 'model.layers.31.mlp.gate_proj.qzeros', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.q_proj.qzeros', 'model.layers.25.mlp.down_proj.bias', 'model.layers.12.mlp.up_proj.scales', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.g_idx', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.15.mlp.up_proj.g_idx', 'model.layers.1.mlp.down_proj.bias', 'model.layers.28.self_attn.o_proj.scales', 'model.layers.5.self_attn.v_proj.qzeros', 'model.layers.8.self_attn.k_proj.qweight', 'model.layers.21.mlp.down_proj.qweight', 'model.layers.15.mlp.down_proj.g_idx', 'model.layers.7.self_attn.q_proj.qzeros', 'model.layers.28.self_attn.q_proj.g_idx', 'model.layers.6.mlp.down_proj.scales', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.1.self_attn.v_proj.qzeros', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.g_idx', 'model.layers.12.mlp.down_proj.scales', 'model.layers.6.self_attn.v_proj.qzeros', 'model.layers.6.self_attn.q_proj.g_idx', 'model.layers.15.self_attn.v_proj.scales', 'model.layers.23.self_attn.q_proj.qweight', 'model.layers.9.self_attn.v_proj.scales', 'model.layers.27.mlp.gate_proj.scales', 'model.layers.25.mlp.gate_proj.scales', 'model.layers.9.self_attn.q_proj.qweight', 'model.layers.17.self_attn.k_proj.scales', 'model.layers.25.mlp.up_proj.qweight', 'model.layers.22.self_attn.o_proj.g_idx', 'model.layers.14.mlp.down_proj.scales', 'model.layers.21.self_attn.q_proj.qweight', 'model.layers.31.self_attn.k_proj.scales', 'model.layers.2.self_attn.k_proj.scales', 'model.layers.29.self_attn.k_proj.qweight', 'model.layers.30.mlp.gate_proj.qzeros', 'model.layers.21.self_attn.o_proj.qzeros', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.qzeros', 'model.layers.27.mlp.up_proj.bias', 'model.layers.22.mlp.up_proj.qzeros', 'model.layers.24.self_attn.v_proj.qweight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.15.mlp.gate_proj.scales', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.qweight', 'model.layers.10.self_attn.o_proj.qweight', 'model.layers.8.self_attn.q_proj.qzeros', 'model.layers.20.mlp.gate_proj.qweight', 'model.layers.16.self_attn.v_proj.qzeros', 'model.layers.14.mlp.down_proj.qweight', 'model.layers.7.mlp.up_proj.g_idx', 'model.layers.19.mlp.down_proj.qweight', 'model.layers.3.self_attn.k_proj.qzeros', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.qweight', 'model.layers.26.self_attn.v_proj.scales', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.qweight', 'model.layers.27.mlp.gate_proj.qzeros', 'model.layers.28.self_attn.k_proj.qweight', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.31.self_attn.k_proj.qzeros', 'model.layers.4.mlp.down_proj.qzeros', 'model.layers.10.self_attn.k_proj.g_idx', 'model.layers.5.self_attn.k_proj.scales', 'model.layers.22.self_attn.k_proj.qweight', 'model.layers.16.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.qzeros', 'model.layers.29.self_attn.q_proj.g_idx', 'model.layers.17.self_attn.o_proj.qzeros', 'model.layers.2.self_attn.q_proj.qzeros', 'model.layers.23.mlp.up_proj.g_idx', 'model.layers.22.self_attn.v_proj.scales', 'model.layers.7.self_attn.q_proj.g_idx', 'model.layers.14.mlp.up_proj.qweight', 'model.layers.21.mlp.gate_proj.g_idx', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.25.mlp.down_proj.qweight', 'model.layers.0.mlp.gate_proj.g_idx', 'model.layers.12.self_attn.o_proj.g_idx', 'model.layers.17.self_attn.q_proj.scales', 'model.layers.23.mlp.up_proj.qzeros', 'model.layers.4.self_attn.v_proj.scales', 'model.layers.30.self_attn.o_proj.scales', 'model.layers.25.self_attn.v_proj.qzeros', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.qweight', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.o_proj.qweight', 'model.layers.17.self_attn.o_proj.qweight', 'model.layers.21.self_attn.o_proj.scales', 'model.layers.27.self_attn.k_proj.scales', 'model.layers.6.mlp.down_proj.qzeros', 'model.layers.18.mlp.gate_proj.scales', 'model.layers.25.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.qweight', 'model.layers.7.mlp.down_proj.qzeros', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.scales', 'model.layers.13.mlp.down_proj.scales', 'model.layers.14.self_attn.v_proj.g_idx', 'model.layers.2.mlp.gate_proj.g_idx', 'model.layers.0.self_attn.k_proj.qweight', 'model.layers.0.self_attn.v_proj.qweight', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.2.self_attn.o_proj.qweight', 'model.layers.22.self_attn.q_proj.qweight', 'model.layers.30.self_attn.o_proj.g_idx', 'model.layers.10.mlp.down_proj.scales', 'model.layers.5.self_attn.q_proj.scales', 'model.layers.8.self_attn.q_proj.g_idx', 'model.layers.12.mlp.gate_proj.qzeros', 'model.layers.28.mlp.up_proj.scales', 'model.layers.19.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.v_proj.scales', 'model.layers.9.mlp.up_proj.scales', 'model.layers.1.self_attn.o_proj.qweight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.1.self_attn.o_proj.scales', 'model.layers.29.mlp.up_proj.qzeros', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.27.mlp.gate_proj.qweight', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.25.self_attn.k_proj.qzeros', 'model.layers.16.self_attn.q_proj.g_idx', 'model.layers.28.mlp.up_proj.qweight', 'model.layers.31.self_attn.o_proj.qzeros', 'model.layers.16.self_attn.q_proj.qweight', 'model.layers.5.self_attn.v_proj.qweight', 'model.layers.7.self_attn.o_proj.g_idx', 'model.layers.10.mlp.down_proj.qweight', 'model.layers.8.self_attn.v_proj.g_idx', 'model.layers.12.self_attn.o_proj.qweight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.29.mlp.up_proj.qweight', 'model.layers.25.mlp.up_proj.g_idx', 'model.layers.31.mlp.down_proj.g_idx', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.qzeros', 'model.layers.12.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.scales', 'model.layers.25.self_attn.v_proj.g_idx', 'model.layers.4.mlp.down_proj.g_idx', 'model.layers.3.self_attn.v_proj.qweight', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.g_idx', 'model.layers.9.mlp.down_proj.g_idx', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.14.mlp.gate_proj.g_idx', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.1.self_attn.k_proj.g_idx', 'model.layers.4.self_attn.q_proj.qzeros', 'model.layers.1.self_attn.q_proj.qzeros', 'model.layers.17.self_attn.o_proj.g_idx', 'model.layers.2.self_attn.k_proj.qweight', 'model.layers.1.mlp.up_proj.qweight', 'model.layers.20.self_attn.q_proj.g_idx', 'model.layers.13.self_attn.o_proj.scales', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.scales', 'model.layers.24.mlp.gate_proj.qzeros', 'model.layers.14.self_attn.k_proj.qweight', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.qzeros', 'model.layers.3.mlp.down_proj.bias', 'model.layers.8.mlp.up_proj.g_idx', 'model.layers.9.mlp.gate_proj.g_idx', 'model.layers.7.self_attn.o_proj.scales', 'model.layers.18.self_attn.o_proj.qzeros', 'model.layers.8.self_attn.q_proj.scales', 'model.layers.21.self_attn.q_proj.qzeros', 'model.layers.29.self_attn.q_proj.qweight', 'model.layers.28.self_attn.q_proj.qweight', 'model.layers.29.mlp.gate_proj.g_idx', 'model.layers.12.self_attn.o_proj.scales', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.o_proj.g_idx', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.scales', 'model.layers.20.mlp.gate_proj.scales', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.qweight', 'model.layers.4.mlp.up_proj.scales', 'model.layers.13.self_attn.o_proj.qweight', 'model.layers.10.self_attn.q_proj.qzeros', 'model.layers.18.self_attn.v_proj.g_idx', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.22.mlp.gate_proj.scales', 'model.layers.17.self_attn.v_proj.qzeros', 'model.layers.28.self_attn.v_proj.qweight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.qzeros', 'model.layers.1.mlp.gate_proj.g_idx', 'model.layers.5.self_attn.o_proj.g_idx', 'model.layers.12.self_attn.v_proj.g_idx', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.1.mlp.down_proj.qweight', 'model.layers.6.self_attn.o_proj.qzeros', 'model.layers.8.self_attn.v_proj.qweight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.10.self_attn.k_proj.qweight', 'model.layers.3.mlp.gate_proj.g_idx', 'model.layers.0.mlp.gate_proj.qweight', 'model.layers.0.mlp.gate_proj.qzeros', 'model.layers.18.mlp.up_proj.qzeros', 'model.layers.18.self_attn.o_proj.scales', 'model.layers.8.mlp.up_proj.bias', 'model.layers.16.self_attn.v_proj.qweight', 'model.layers.26.self_attn.o_proj.qzeros', 'model.layers.29.mlp.gate_proj.scales', 'model.layers.17.self_attn.k_proj.g_idx', 'model.layers.16.mlp.gate_proj.qweight', 'model.layers.22.self_attn.v_proj.g_idx', 'model.layers.31.mlp.up_proj.scales', 'model.layers.31.self_attn.q_proj.scales', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.0.mlp.up_proj.scales', 'model.layers.16.mlp.up_proj.qzeros', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.qweight', 'model.layers.9.mlp.down_proj.qweight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.scales', 'model.layers.17.mlp.down_proj.scales', 'model.layers.30.self_attn.o_proj.qzeros', 'model.layers.7.mlp.up_proj.qzeros', 'model.layers.16.mlp.down_proj.bias', 'model.layers.27.self_attn.q_proj.qweight', 'model.layers.3.mlp.gate_proj.scales', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.scales', 'model.layers.21.mlp.gate_proj.qweight', 'model.layers.21.self_attn.o_proj.qweight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.scales', 'model.layers.29.self_attn.k_proj.scales', 'model.layers.11.mlp.gate_proj.scales', 'model.layers.20.self_attn.k_proj.scales', 'model.layers.19.mlp.gate_proj.scales', 'model.layers.10.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.qweight', 'model.layers.7.mlp.gate_proj.g_idx', 'model.layers.12.mlp.up_proj.g_idx', 'model.layers.15.self_attn.k_proj.qweight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.15.mlp.gate_proj.qzeros', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.scales', 'model.layers.18.mlp.down_proj.qweight', 'model.layers.4.self_attn.v_proj.qzeros', 'model.layers.24.mlp.gate_proj.scales', 'model.layers.2.self_attn.k_proj.g_idx', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.28.mlp.down_proj.scales', 'model.layers.0.self_attn.o_proj.qweight', 'model.layers.17.mlp.up_proj.scales', 'model.layers.27.self_attn.v_proj.qweight', 'model.layers.9.self_attn.v_proj.qweight', 'model.layers.4.mlp.gate_proj.qweight', 'model.layers.1.self_attn.q_proj.qweight', 'model.layers.19.mlp.gate_proj.qweight', 'model.layers.6.self_attn.q_proj.scales', 'model.layers.7.mlp.gate_proj.qzeros', 'model.layers.16.self_attn.v_proj.scales', 'model.layers.13.self_attn.k_proj.scales', 'model.layers.26.self_attn.k_proj.g_idx', 'model.layers.18.mlp.up_proj.scales', 'model.layers.18.self_attn.k_proj.qzeros', 'model.layers.16.self_attn.k_proj.scales', 'model.layers.13.self_attn.v_proj.g_idx', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.1.self_attn.k_proj.qzeros', 'model.layers.16.mlp.up_proj.qweight', 'model.layers.20.self_attn.q_proj.qweight', 'model.layers.9.self_attn.o_proj.g_idx', 'model.layers.11.self_attn.o_proj.qzeros', 'model.layers.2.self_attn.o_proj.qzeros', 'model.layers.23.self_attn.o_proj.qzeros', 'model.layers.11.mlp.gate_proj.g_idx', 'model.layers.14.mlp.up_proj.qzeros', 'model.layers.19.self_attn.o_proj.g_idx', 'model.layers.4.mlp.down_proj.scales', 'model.layers.4.self_attn.k_proj.scales', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.g_idx', 'model.layers.19.self_attn.k_proj.qweight', 'model.layers.13.mlp.gate_proj.g_idx', 'model.layers.13.self_attn.k_proj.qzeros', 'model.layers.18.mlp.down_proj.qzeros', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.scales', 'model.layers.28.mlp.gate_proj.qzeros', 'model.layers.9.self_attn.o_proj.scales', 'model.layers.1.mlp.gate_proj.qzeros', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.qweight', 'model.layers.16.mlp.up_proj.g_idx', 'model.layers.0.mlp.down_proj.qweight', 'model.layers.3.self_attn.q_proj.g_idx', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.qzeros', 'model.layers.27.self_attn.q_proj.qzeros', 'model.layers.31.mlp.up_proj.qzeros', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.g_idx', 'model.layers.19.mlp.down_proj.scales', 'model.layers.13.self_attn.q_proj.qzeros', 'model.layers.22.self_attn.o_proj.scales', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.qweight', 'model.layers.18.mlp.gate_proj.qweight', 'model.layers.22.mlp.down_proj.scales', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.8.mlp.gate_proj.qzeros', 'model.layers.12.self_attn.k_proj.qzeros', 'model.layers.19.mlp.up_proj.g_idx', 'model.layers.16.mlp.down_proj.scales', 'model.layers.25.self_attn.k_proj.qweight', 'model.layers.31.mlp.down_proj.scales', 'model.layers.19.mlp.up_proj.scales', 'model.layers.6.self_attn.k_proj.g_idx', 'model.layers.1.self_attn.v_proj.g_idx', 'model.layers.5.mlp.gate_proj.g_idx', 'model.layers.2.mlp.up_proj.scales', 'model.layers.24.mlp.up_proj.qweight', 'model.layers.1.self_attn.o_proj.g_idx', 'model.layers.7.mlp.gate_proj.scales', 'model.layers.9.self_attn.k_proj.scales', 'model.layers.8.mlp.up_proj.scales', 'model.layers.29.self_attn.o_proj.qzeros', 'model.layers.23.self_attn.q_proj.qzeros', 'model.layers.5.self_attn.k_proj.qweight', 'model.layers.18.self_attn.v_proj.scales', 'model.layers.16.mlp.gate_proj.g_idx', 'model.layers.19.self_attn.k_proj.qzeros', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.28.mlp.down_proj.qzeros', 'model.layers.17.mlp.up_proj.qzeros', 'model.layers.21.mlp.up_proj.scales', 'model.layers.25.mlp.gate_proj.g_idx', 'model.layers.26.self_attn.v_proj.g_idx', 'model.layers.19.self_attn.k_proj.scales', 'model.layers.5.self_attn.o_proj.scales', 'model.layers.8.self_attn.o_proj.qzeros', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.qzeros', 'model.layers.17.mlp.up_proj.g_idx', 'model.layers.7.mlp.down_proj.bias', 'model.layers.22.self_attn.v_proj.qweight', 'model.layers.28.self_attn.o_proj.qweight', 'model.layers.1.mlp.up_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.g_idx', 'model.layers.13.self_attn.q_proj.qweight', 'model.layers.18.mlp.down_proj.bias', 'model.layers.15.self_attn.o_proj.qzeros', 'model.layers.21.mlp.down_proj.qzeros', 'model.layers.12.mlp.down_proj.qweight', 'model.layers.6.self_attn.v_proj.qweight', 'model.layers.27.mlp.up_proj.qweight', 'model.layers.25.self_attn.o_proj.scales', 'model.layers.29.mlp.down_proj.qweight', 'model.layers.9.mlp.up_proj.qweight', 'model.layers.30.self_attn.q_proj.scales', 'model.layers.0.mlp.down_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.8.self_attn.v_proj.scales', 'model.layers.2.self_attn.v_proj.scales', 'model.layers.28.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.2.self_attn.o_proj.scales', 'model.layers.21.mlp.up_proj.g_idx', 'model.layers.28.mlp.down_proj.qweight', 'model.layers.26.self_attn.k_proj.qweight', 'model.layers.28.self_attn.o_proj.qzeros', 'model.layers.6.self_attn.v_proj.g_idx', 'model.layers.3.self_attn.o_proj.qzeros', 'model.layers.9.self_attn.k_proj.g_idx', 'model.layers.28.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.qweight', 'model.layers.26.self_attn.q_proj.scales', 'model.layers.12.mlp.gate_proj.qweight', 'model.layers.23.self_attn.o_proj.g_idx', 'model.layers.6.self_attn.o_proj.scales', 'model.layers.28.self_attn.k_proj.qzeros', 'model.layers.24.mlp.up_proj.scales', 'model.layers.25.self_attn.q_proj.qweight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.10.mlp.gate_proj.qzeros', 'model.layers.24.mlp.down_proj.qweight', 'model.layers.14.mlp.up_proj.scales', 'model.layers.0.mlp.up_proj.qzeros', 'model.layers.29.mlp.gate_proj.qzeros', 'model.layers.13.mlp.gate_proj.qzeros', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.20.self_attn.v_proj.g_idx', 'model.layers.7.self_attn.k_proj.scales', 'model.layers.10.self_attn.q_proj.scales', 'model.layers.5.mlp.up_proj.qweight', 'model.layers.20.self_attn.o_proj.scales', 'model.layers.8.mlp.down_proj.scales', 'model.layers.26.mlp.down_proj.bias', 'model.layers.28.mlp.down_proj.g_idx', 'model.layers.13.self_attn.o_proj.qzeros', 'model.layers.10.self_attn.k_proj.scales', 'model.layers.12.mlp.gate_proj.g_idx', 'model.layers.14.mlp.gate_proj.qweight', 'model.layers.10.mlp.down_proj.qzeros', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.scales', 'model.layers.31.mlp.up_proj.qweight', 'model.layers.0.self_attn.k_proj.scales', 'model.layers.15.mlp.gate_proj.g_idx', 'model.layers.18.self_attn.k_proj.scales', 'model.layers.1.mlp.gate_proj.qweight', 'model.layers.19.self_attn.v_proj.qweight', 'model.layers.20.mlp.up_proj.qzeros', 'model.layers.16.self_attn.o_proj.scales', 'model.layers.19.mlp.gate_proj.qzeros', 'model.layers.2.mlp.down_proj.g_idx', 'model.layers.26.mlp.gate_proj.qzeros', 'model.layers.26.self_attn.k_proj.scales', 'model.layers.3.mlp.up_proj.scales', 'model.layers.7.self_attn.o_proj.qzeros', 'model.layers.10.self_attn.k_proj.qzeros', 'model.layers.17.self_attn.q_proj.qweight', 'model.layers.16.mlp.down_proj.g_idx', 'model.layers.20.self_attn.v_proj.scales', 'model.layers.22.mlp.up_proj.g_idx', 'model.layers.24.self_attn.v_proj.qzeros', 'model.layers.23.mlp.down_proj.qweight', 'model.layers.23.mlp.gate_proj.scales', 'model.layers.8.mlp.up_proj.qweight', 'model.layers.5.mlp.down_proj.g_idx', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.24.self_attn.o_proj.qzeros', 'model.layers.2.mlp.gate_proj.scales', 'model.layers.1.self_attn.o_proj.qzeros', 'model.layers.21.self_attn.o_proj.g_idx', 'model.layers.30.self_attn.q_proj.g_idx', 'model.layers.5.mlp.down_proj.scales', 'model.layers.23.mlp.down_proj.bias', 'model.layers.28.self_attn.q_proj.qzeros', 'model.layers.31.self_attn.o_proj.qweight', 'model.layers.5.mlp.up_proj.bias', 'model.layers.6.mlp.gate_proj.g_idx', 'model.layers.15.self_attn.v_proj.qzeros', 'model.layers.0.self_attn.v_proj.scales', 'model.layers.17.mlp.up_proj.qweight', 'model.layers.14.mlp.gate_proj.scales', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.qweight', 'model.layers.4.mlp.down_proj.qweight', 'model.layers.5.mlp.up_proj.qzeros', 'model.layers.4.mlp.up_proj.qweight', 'model.layers.14.self_attn.o_proj.g_idx', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.qweight', 'model.layers.3.mlp.gate_proj.qzeros', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.6.mlp.up_proj.qzeros', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.5.mlp.up_proj.g_idx', 'model.layers.0.self_attn.o_proj.qzeros', 'model.layers.30.self_attn.v_proj.g_idx', 'model.layers.16.self_attn.v_proj.g_idx', 'model.layers.9.self_attn.k_proj.qweight', 'model.layers.22.mlp.down_proj.bias', 'model.layers.24.self_attn.o_proj.qweight', 'model.layers.14.self_attn.q_proj.scales', 'model.layers.21.self_attn.v_proj.g_idx', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.28.self_attn.v_proj.scales', 'model.layers.2.self_attn.v_proj.g_idx', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.g_idx', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.qzeros', 'model.layers.18.self_attn.k_proj.qweight', 'model.layers.7.mlp.up_proj.scales', 'model.layers.28.mlp.gate_proj.g_idx', 'model.layers.23.self_attn.q_proj.scales', 'model.layers.3.mlp.down_proj.scales', 'model.layers.20.mlp.up_proj.qweight', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.v_proj.g_idx', 'model.layers.9.self_attn.v_proj.qzeros', 'model.layers.19.mlp.gate_proj.g_idx', 'model.layers.31.self_attn.v_proj.qzeros', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.qzeros', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.6.self_attn.o_proj.g_idx', 'model.layers.19.mlp.up_proj.qzeros', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.scales', 'model.layers.25.mlp.down_proj.qzeros', 'model.layers.5.mlp.gate_proj.qzeros', 'model.layers.23.self_attn.k_proj.scales', 'model.layers.11.mlp.down_proj.bias', 'model.layers.19.self_attn.v_proj.scales', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.27.self_attn.q_proj.scales', 'model.layers.24.mlp.up_proj.bias', 'model.layers.7.self_attn.v_proj.qzeros', 'model.layers.4.self_attn.o_proj.g_idx', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.qweight', 'model.layers.3.mlp.down_proj.qweight', 'model.layers.15.self_attn.v_proj.qweight', 'model.layers.9.self_attn.q_proj.qzeros', 'model.layers.11.self_attn.q_proj.qweight', 'model.layers.4.mlp.up_proj.qzeros', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.scales', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.qweight', 'model.layers.14.mlp.down_proj.bias', 'model.layers.7.self_attn.q_proj.qweight', 'model.layers.12.self_attn.q_proj.g_idx', 'model.layers.12.self_attn.q_proj.qweight', 'model.layers.21.self_attn.k_proj.qzeros', 'model.layers.13.self_attn.q_proj.scales', 'model.layers.14.self_attn.o_proj.qweight', 'model.layers.25.self_attn.k_proj.g_idx', 'model.layers.26.self_attn.q_proj.g_idx', 'model.layers.31.self_attn.o_proj.g_idx', 'model.layers.2.mlp.gate_proj.qweight', 'model.layers.10.self_attn.v_proj.scales', 'model.layers.25.mlp.down_proj.g_idx', 'model.layers.24.mlp.down_proj.bias', 'model.layers.14.mlp.down_proj.g_idx', 'model.layers.1.mlp.gate_proj.scales', 'model.layers.24.mlp.up_proj.qzeros', 'model.layers.27.self_attn.o_proj.g_idx', 'model.layers.30.mlp.up_proj.qweight', 'model.layers.14.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.qweight', 'model.layers.1.self_attn.k_proj.scales', 'model.layers.26.mlp.down_proj.g_idx', 'model.layers.4.mlp.gate_proj.g_idx', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.19.self_attn.q_proj.scales', 'model.layers.0.self_attn.k_proj.qzeros', 'model.layers.17.mlp.down_proj.qweight', 'model.layers.29.mlp.gate_proj.qweight', 'model.layers.20.mlp.gate_proj.g_idx', 'model.layers.24.mlp.down_proj.g_idx', 'model.layers.30.mlp.up_proj.qzeros', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.18.mlp.gate_proj.qzeros', 'model.layers.28.mlp.up_proj.qzeros', 'model.layers.5.self_attn.o_proj.qweight', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.qzeros', 'model.layers.19.self_attn.q_proj.qweight', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.11.mlp.gate_proj.qweight', 'model.layers.11.self_attn.k_proj.qweight', 'model.layers.18.mlp.gate_proj.g_idx', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.qweight', 'model.layers.9.self_attn.o_proj.qzeros', 'model.layers.10.self_attn.o_proj.scales', 'model.layers.14.self_attn.v_proj.qzeros', 'model.layers.24.self_attn.q_proj.qzeros', 'model.layers.26.mlp.up_proj.qzeros', 'model.layers.3.mlp.gate_proj.qweight', 'model.layers.12.mlp.down_proj.qzeros', 'model.layers.13.mlp.down_proj.bias', 'model.layers.23.mlp.up_proj.scales', 'model.layers.31.self_attn.o_proj.scales', 'model.layers.9.mlp.up_proj.qzeros', 'model.layers.0.self_attn.q_proj.scales', 'model.layers.7.self_attn.v_proj.scales', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.17.self_attn.q_proj.qzeros', 'model.layers.4.self_attn.o_proj.scales', 'model.layers.15.self_attn.k_proj.g_idx', 'model.layers.22.mlp.down_proj.qzeros', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.qweight', 'model.layers.7.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.g_idx', 'model.layers.6.self_attn.k_proj.qzeros', 'model.layers.26.mlp.down_proj.qzeros', 'model.layers.24.self_attn.o_proj.g_idx', 'model.layers.27.mlp.down_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.15.self_attn.o_proj.scales', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.qweight', 'model.layers.24.mlp.gate_proj.qweight', 'model.layers.3.self_attn.k_proj.scales', 'model.layers.1.self_attn.q_proj.g_idx', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.qzeros', 'model.layers.7.mlp.down_proj.g_idx', 'model.layers.20.self_attn.o_proj.qzeros', 'model.layers.20.mlp.down_proj.g_idx', 'model.layers.24.mlp.up_proj.g_idx', 'model.layers.29.self_attn.k_proj.qzeros', 'model.layers.3.self_attn.k_proj.g_idx', 'model.layers.1.mlp.up_proj.scales', 'model.layers.27.mlp.down_proj.qzeros', 'model.layers.29.self_attn.v_proj.qweight', 'model.layers.13.mlp.up_proj.g_idx', 'model.layers.14.self_attn.q_proj.qweight', 'model.layers.25.mlp.up_proj.qzeros', 'model.layers.19.mlp.up_proj.qweight', 'model.layers.27.mlp.gate_proj.g_idx', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.30.mlp.down_proj.qweight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.11.mlp.down_proj.g_idx', 'model.layers.2.mlp.down_proj.scales', 'model.layers.11.self_attn.v_proj.qweight', 'model.layers.28.self_attn.k_proj.scales', 'model.layers.6.mlp.gate_proj.scales', 'model.layers.15.mlp.up_proj.qzeros', 'model.layers.11.mlp.up_proj.bias', 'model.layers.13.mlp.down_proj.qweight', 'model.layers.14.self_attn.v_proj.scales', 'model.layers.8.mlp.down_proj.g_idx', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.scales', 'model.layers.0.mlp.down_proj.g_idx', 'model.layers.29.self_attn.k_proj.g_idx', 'model.layers.12.self_attn.k_proj.scales', 'model.layers.15.self_attn.o_proj.g_idx', 'model.layers.12.self_attn.v_proj.qzeros', 'model.layers.4.self_attn.v_proj.g_idx', 'model.layers.24.self_attn.k_proj.scales', 'model.layers.26.self_attn.k_proj.qzeros', 'model.layers.13.mlp.up_proj.qweight', 'model.layers.15.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.g_idx', 'model.layers.18.self_attn.q_proj.qzeros', 'model.layers.13.self_attn.v_proj.qzeros', 'model.layers.14.mlp.down_proj.qzeros', 'model.layers.14.self_attn.o_proj.scales', 'model.layers.29.mlp.up_proj.scales', 'model.layers.4.self_attn.q_proj.qweight', 'model.layers.11.self_attn.k_proj.g_idx', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.21.self_attn.v_proj.qweight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.20.self_attn.q_proj.qzeros', 'model.layers.3.self_attn.v_proj.g_idx', 'model.layers.31.self_attn.q_proj.g_idx', 'model.layers.20.self_attn.k_proj.qzeros', 'model.layers.23.self_attn.v_proj.qzeros', 'model.layers.15.mlp.down_proj.qzeros', 'model.layers.16.self_attn.q_proj.qzeros', 'model.layers.25.self_attn.q_proj.g_idx', 'model.layers.16.self_attn.q_proj.scales', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.14.self_attn.q_proj.g_idx', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.self_attn.k_proj.g_idx', 'model.layers.29.self_attn.q_proj.qzeros', 'model.layers.20.self_attn.o_proj.qweight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.qweight', 'model.layers.0.self_attn.o_proj.scales', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.30.self_attn.v_proj.qzeros', 'model.layers.26.mlp.up_proj.qweight', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.g_idx', 'model.layers.15.mlp.down_proj.scales', 'model.layers.11.self_attn.q_proj.qzeros', 'model.layers.31.self_attn.v_proj.scales', 'model.layers.8.mlp.down_proj.qweight', 'model.layers.18.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.q_proj.scales', 'model.layers.23.mlp.gate_proj.qweight', 'model.layers.0.self_attn.q_proj.g_idx', 'model.layers.22.self_attn.k_proj.qzeros', 'model.layers.26.mlp.down_proj.qweight', 'model.layers.2.mlp.up_proj.qweight', 'model.layers.21.self_attn.q_proj.scales', 'model.layers.20.self_attn.k_proj.g_idx', 'model.layers.10.mlp.up_proj.scales', 'model.layers.20.self_attn.o_proj.g_idx', 'model.layers.31.mlp.gate_proj.g_idx', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.qweight', 'model.layers.21.mlp.gate_proj.qzeros', 'model.layers.26.self_attn.o_proj.g_idx', 'model.layers.0.self_attn.k_proj.g_idx', 'model.layers.0.mlp.up_proj.g_idx', 'model.layers.0.self_attn.v_proj.g_idx', 'model.layers.21.self_attn.v_proj.qzeros', 'model.layers.11.mlp.up_proj.qweight', 'model.layers.10.self_attn.v_proj.qweight', 'model.layers.22.mlp.gate_proj.qweight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.3.mlp.up_proj.qweight', 'model.layers.30.mlp.down_proj.scales', 'model.layers.14.self_attn.k_proj.g_idx', 'model.layers.7.mlp.down_proj.scales', 'model.layers.21.mlp.down_proj.bias', 'model.layers.16.self_attn.k_proj.qweight', 'model.layers.21.mlp.down_proj.g_idx', 'model.layers.26.mlp.gate_proj.qweight', 'model.layers.13.mlp.up_proj.scales', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.5.mlp.down_proj.qzeros', 'model.layers.5.self_attn.q_proj.qweight', 'model.layers.12.self_attn.k_proj.qweight', 'model.layers.2.self_attn.q_proj.qweight', 'model.layers.20.self_attn.v_proj.qweight', 'model.layers.30.self_attn.k_proj.qzeros', 'model.layers.28.self_attn.v_proj.g_idx', 'model.layers.3.self_attn.v_proj.qzeros', 'model.layers.16.mlp.up_proj.scales', 'model.layers.11.self_attn.q_proj.scales', 'model.layers.30.mlp.gate_proj.g_idx', 'model.layers.5.mlp.up_proj.scales', 'model.layers.22.mlp.gate_proj.qzeros', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.23.self_attn.v_proj.g_idx', 'model.layers.11.self_attn.v_proj.qzeros', 'model.layers.10.mlp.gate_proj.g_idx', 'model.layers.15.self_attn.q_proj.qzeros', 'model.layers.4.mlp.gate_proj.scales', 'model.layers.16.mlp.down_proj.qzeros', 'model.layers.30.self_attn.k_proj.qweight', 'model.layers.20.mlp.up_proj.bias', 'model.layers.2.mlp.up_proj.g_idx', 'model.layers.17.self_attn.q_proj.g_idx', 'model.layers.26.mlp.gate_proj.g_idx', 'model.layers.20.mlp.down_proj.bias', 'model.layers.23.self_attn.q_proj.g_idx', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.scales', 'model.layers.18.mlp.up_proj.qweight', 'model.layers.7.mlp.up_proj.qweight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.11.self_attn.o_proj.qweight', 'model.layers.14.self_attn.o_proj.qzeros', 'model.layers.15.mlp.up_proj.bias', 'model.layers.8.self_attn.q_proj.qweight', 'model.layers.20.self_attn.q_proj.scales', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.g_idx', 'model.layers.12.mlp.gate_proj.scales', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.g_idx', 'model.layers.16.mlp.gate_proj.scales', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.6.mlp.up_proj.qweight', 'model.layers.31.mlp.down_proj.qzeros', 'model.layers.22.mlp.gate_proj.g_idx', 'model.layers.1.mlp.down_proj.g_idx', 'model.layers.11.mlp.down_proj.scales', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.g_idx', 'model.layers.9.mlp.up_proj.g_idx', 'model.layers.1.self_attn.v_proj.qweight', 'model.layers.11.self_attn.k_proj.qzeros', 'model.layers.27.self_attn.v_proj.scales', 'model.layers.30.self_attn.o_proj.qweight', 'model.layers.7.self_attn.v_proj.g_idx', 'model.layers.23.mlp.down_proj.qzeros', 'model.layers.30.mlp.gate_proj.scales', 'model.layers.29.self_attn.q_proj.scales', 'model.layers.23.mlp.up_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.qzeros', 'model.layers.22.self_attn.q_proj.g_idx', 'model.layers.27.mlp.down_proj.g_idx', 'model.layers.8.mlp.down_proj.qzeros', 'model.layers.19.mlp.down_proj.g_idx', 'model.layers.21.self_attn.q_proj.g_idx', 'model.layers.22.mlp.up_proj.scales', 'model.layers.13.mlp.gate_proj.qweight', 'model.layers.19.self_attn.o_proj.qweight', 'model.layers.16.mlp.gate_proj.qzeros', 'model.layers.29.self_attn.o_proj.scales', 'model.layers.5.self_attn.o_proj.qzeros', 'model.layers.30.mlp.down_proj.g_idx', 'model.layers.12.self_attn.o_proj.qzeros', 'model.layers.9.mlp.gate_proj.qzeros', 'model.layers.12.mlp.up_proj.qweight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.3.mlp.down_proj.qzeros', 'model.layers.9.self_attn.q_proj.g_idx']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at TheBloke/Llama-2-7B-GPTQ and are newly initialized: ['model.layers.29.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-GPTQ\", use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GPTQ\",\n",
    "                                             use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10275f844a949a5acf8247831054862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4404d3398ec4df3b532d30363a3781b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGUF\", model_file=\"llama-2-7b.Q4_K_M.gguf\", model_type=\"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ctransformers.utils.Vector at 0x1f1d5523cd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04d282bd42964dd2a23eaa0959415f23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09966bb4a04e45cbad464238e668772e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "483f53c628924172bedbd6ac66aeaa22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e6d370082fb406d888bd109f07890d9",
      "placeholder": "​",
      "style": "IPY_MODEL_04d282bd42964dd2a23eaa0959415f23",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "5cbdb7ab718e490c984080785f60a9a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e6d370082fb406d888bd109f07890d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c6844fbf91648778a1605df5ce9cdd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "98fc99b7fbf14f98a5d8a32c4af0c981": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f554f746df440afae509b9ba7e24c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_483f53c628924172bedbd6ac66aeaa22",
       "IPY_MODEL_db4849525171480ab0f8bcc35a4c545c",
       "IPY_MODEL_eb9114d8abf84ad883f19ec1715563cd"
      ],
      "layout": "IPY_MODEL_09966bb4a04e45cbad464238e668772e"
     }
    },
    "d8080c35ca5f4490b1624d27698f5dfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db4849525171480ab0f8bcc35a4c545c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8080c35ca5f4490b1624d27698f5dfa",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c6844fbf91648778a1605df5ce9cdd2",
      "value": 3
     }
    },
    "eb9114d8abf84ad883f19ec1715563cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cbdb7ab718e490c984080785f60a9a8",
      "placeholder": "​",
      "style": "IPY_MODEL_98fc99b7fbf14f98a5d8a32c4af0c981",
      "value": " 3/3 [01:54&lt;00:00, 34.74s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
